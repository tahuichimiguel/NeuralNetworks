{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "This is a pure numpy implementation of word generation using an RNN\n",
    "\n",
    "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
    "\n",
    "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
    "\n",
    "## What is a Recurrent Network?\n",
    "\n",
    "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
    "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "- temperature & location\n",
    "- height & weight\n",
    "- car speed and brand\n",
    "\n",
    "But what if the ordering of the data matters? \n",
    "\n",
    "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
    "\n",
    "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
    "\n",
    "Enter recurrent networks\n",
    "\n",
    "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
    "\n",
    "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
    "\n",
    "So instead of the data flow operation happening like this\n",
    "\n",
    "## input -> hidden -> output\n",
    "\n",
    "it happens like this\n",
    "\n",
    "## (input + prev_hidden) -> hidden -> output\n",
    "\n",
    "wait. Why not this?\n",
    "\n",
    "## (input + prev_input) -> hidden -> output\n",
    "\n",
    "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
    "\n",
    "RNN Formula\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
    "\n",
    "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
    "\n",
    "Loss function\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
    "\n",
    "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
    "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
    "\n",
    "\n",
    "## Our steps\n",
    "\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## What are some use cases?\n",
    "\n",
    "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
    "- Sequential data generation (music, video, audio, etc.)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
    "\n",
    "## What's next? \n",
    "\n",
    "1. LSTM Networks\n",
    "2. Bidirectional networks\n",
    "3. recursive networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Recurrent Network\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Define a function to create sentences from the model **[CRUCIAL & IMPACTFUL STEP]**\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradient and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input. **[WHAT IS \"BIG\"?]**\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137629 chars, 81 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d chars, %d unique' % (data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "- Neural networks operate on vectors (a vector is an array of float)\n",
    "- So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "- We'll count the number of unique chars (*vocab_size*). **[Compare to bag-of-words model]**\n",
    "- That will be the size of the vector. \n",
    "- The vector contains only zero exept for the position of the char where the value is 1 **(1-hot Encoding)**.\n",
    "\n",
    "#### So First let's calculate the *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'C': 31, '!': 3, ' ': 4, '\"': 5, '%': 6, '$': 7, \"'\": 8, ')': 9, '(': 10, '*': 11, '-': 12, ',': 13, '/': 2, '.': 15, '1': 16, '0': 17, '3': 18, '2': 19, '5': 20, '4': 21, '7': 22, '6': 23, '9': 24, '8': 25, ';': 26, ':': 27, '?': 28, 'A': 29, '@': 30, '\\xc3': 1, 'B': 32, 'E': 33, 'D': 34, 'G': 35, 'F': 36, 'I': 37, 'H': 38, 'K': 39, 'J': 40, 'M': 41, 'L': 42, 'O': 43, 'N': 44, 'Q': 45, 'P': 46, 'S': 47, 'R': 48, 'U': 49, 'T': 50, 'W': 51, 'V': 52, 'Y': 53, 'X': 54, 'd': 59, 'a': 55, 'c': 56, 'b': 57, 'e': 58, '\\xa7': 14, 'g': 60, 'f': 61, 'i': 62, 'h': 63, 'k': 64, 'j': 65, 'm': 66, 'l': 67, 'o': 68, 'n': 69, 'q': 70, 'p': 71, 's': 72, 'r': 73, 'u': 74, 't': 75, 'w': 76, 'v': 77, 'y': 78, 'x': 79, 'z': 80}\n",
      "\n",
      "{0: '\\n', 1: '\\xc3', 2: '/', 3: '!', 4: ' ', 5: '\"', 6: '%', 7: '$', 8: \"'\", 9: ')', 10: '(', 11: '*', 12: '-', 13: ',', 14: '\\xa7', 15: '.', 16: '1', 17: '0', 18: '3', 19: '2', 20: '5', 21: '4', 22: '7', 23: '6', 24: '9', 25: '8', 26: ';', 27: ':', 28: '?', 29: 'A', 30: '@', 31: 'C', 32: 'B', 33: 'E', 34: 'D', 35: 'G', 36: 'F', 37: 'I', 38: 'H', 39: 'K', 40: 'J', 41: 'M', 42: 'L', 43: 'O', 44: 'N', 45: 'Q', 46: 'P', 47: 'S', 48: 'R', 49: 'U', 50: 'T', 51: 'W', 52: 'V', 53: 'Y', 54: 'X', 55: 'a', 56: 'c', 57: 'b', 58: 'e', 59: 'd', 60: 'g', 61: 'f', 62: 'i', 63: 'h', 64: 'k', 65: 'j', 66: 'm', 67: 'l', 68: 'o', 69: 'n', 70: 'q', 71: 'p', 72: 's', 73: 'r', 74: 'u', 75: 't', 76: 'w', 77: 'v', 78: 'y', 79: 'x', 80: 'z'}\n"
     ]
    }
   ],
   "source": [
    "##Mapping Tables\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print char_to_ix\n",
    "print ''\n",
    "print ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "- The dictionary defined above allows us to create a vector of size 61 instead of 256.  \n",
    "- Here and exemple of the char 'a'  \n",
    "- The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print vector_for_char_a.ravel()\n",
    "print ''\n",
    "print vector_for_char_a.ravel()[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "\n",
    "#Hyperparameter\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#Initialization Sampled From Uniform Distribution\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to **connect the hidden layer to itself via the previous hidden state.**\n",
    "    - **This is the Key of the Rnn.** Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "    \n",
    "\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "The __loss__ is a key concept in all neural networks training. \n",
    "It is a value that describe how good is our model.  \n",
    "The smaller the loss, the better our model is.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "- The forward pass uses the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the training set.\n",
    "\n",
    "- xs[t] is the vector that encode the char at position t\n",
    "- ps[t] is the **probabilities** for next char\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars via SOFTMAX transformation\n",
    "```\n",
    "\n",
    "Dirty pseudo code for each char\n",
    "```python\n",
    "hs = Wxh*input + Whh*last_value_of_hidden_state + bh\n",
    "ys = Why*hs + by\n",
    "ps = softmax_normalized(ys)\n",
    "```\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technique to calculates all the gradients for all the parameters at once: the backpropagation.  \n",
    "Gradients are calculated in the opposite order of the forward pass, using a simple technique.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "The loss for one datapoint\n",
    "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
    "\n",
    "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
    "\n",
    "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
    "\n",
    "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
    "\n",
    "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
    "\n",
    "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
    "\n",
    "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
    "\n",
    "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    inputs,targets are both list of integers.                                                                                                                                                   \n",
    "    hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "    returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "    \"\"\"\n",
    "    #store our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    #init loss as 0\n",
    "    loss = 0\n",
    "    # forward pass                                                                                                                                                                              \n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "        xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "    \n",
    "    # backward pass: compute gradients going backwards    \n",
    "    #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "  \n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        #output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        \n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y  \n",
    "        \n",
    "        #Compute output gradient -  output times hidden states transpose\n",
    "        #  When we apply the transpose weight matrix,  \n",
    "        #  we can think intuitively of this as moving the error backward\n",
    "        #  through the network, giving us some sort of measure of the error \n",
    "        #  at the output of the lth layer. \n",
    "        \n",
    "        #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        \n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        \n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "    \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " sA:Hg)q@T�w\n",
      "2�*,pR�s7�B6zBCU'xa1Lm4x8jdW5iyEtc*DtE6a1*AS'kFT!-'LQ:(rmlV:A�l?qX1K8\n",
      "AwA%(aq$h6AYBj\"�4u yf%jGO�/s0(ltIX 2�qRYvRfB9j6El6RG�vhatQWvik$!ISnjVsNqPa0hkvQ7NXG5*(XgnF5nOyq1xB�4kjk%Qcox s8.PNUfL? \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    \n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    \n",
    "    #for as many characters as we want to generate\n",
    "    for t in xrange(n):\n",
    "        #A hidden state at a given time step is a function \n",
    "        #  of (1) the input at the same time step modified by a weight matrix \n",
    "        #  added to (2) the hidden state of the previous time step \n",
    "        #  multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        \n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        \n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        \n",
    "        #pick one with the highest probability (not exactly)\n",
    "        #--This implementation is not strictly mathematically correct\n",
    "        #--it allows for random sampling using the probability distribution\n",
    "        #--specified by p. If stochastic sampling is meant to be used\n",
    "        #--as perturbation to prevent overfitting in training, then it is correct.\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        \n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "    \n",
    "# reset RNN memory \n",
    "hprev = np.zeros((hidden_size,1)) \n",
    "\n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main training loop:\n",
    "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradients\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradient technique **Adagrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [43, 69, 58, 4, 66, 68, 73, 69, 62, 69, 60, 13, 4, 76, 63, 58, 69, 4, 35, 73, 58, 60, 68, 73, 4]\n",
      "targets [69, 58, 4, 66, 68, 73, 69, 62, 69, 60, 13, 4, 76, 63, 58, 69, 4, 35, 73, 58, 60, 68, 73, 4, 47]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print \"inputs\", inputs\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print \"targets\", targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "This is a type of gradient descent strategy\n",
    "\n",
    "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "step size = learning rate\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.785149\n",
      "----\n",
      " h yabkem he spaly het, he coutitive, mm woll, to pusinburedy ores had trowe, worfinbed tena saply shen mlation, taning ir of wnok? Itger, atree tard Gregre Gregor gotad woughs though s, and peandmst f \n",
      "----\n",
      "iter 1000, loss: 80.650069\n",
      "----\n",
      "  khalf actuen jetr at thras tort yo ptin hise theen teron thid, cser or wwarps sout ally serve mas the!, ftut soe hey qukeing, of erte the wingithve plek wheve ke yor'f chat qut wand eve un whird sore \n",
      "----\n",
      "iter 2000, loss: 64.503807\n",
      "----\n",
      " titteryto wastedy fo ilce olpoln k hiuld gry enmeecsely act fhad heather nusseaned pris has shece phe am harcter's foughe mowitnyh han be sead haid isploe he's ay eo as loulgos an nos lalf anhe hes Gr \n",
      "----\n",
      "iter 3000, loss: 56.947660\n",
      "----\n",
      " ld robllr thaby gabe ris bilds he go had btaw to his ldoonk them, dost he ther ibne dos wimis,re had apld.\n",
      "\"y him rit wald fry n be.d im, id, bay, ip sase.ads wuns whis cuf ther -omol tied. ateme hir  \n",
      "----\n",
      "iter 4000, loss: 53.477216\n",
      "----\n",
      " he thit to serwee bos then sawit could am serleq\n",
      "tain he sehad, sime indooot qus, him to, saimithinl bet fith. ile ther'sl quinoning ethe want an tickick\"e tham,s of hoom of stsaint hout?y witm exrmeu \n",
      "----\n",
      "iter 5000, loss: 56.377262\n",
      "----\n",
      " tithrref Gregor, tomy'ne \n",
      "uf oute, withe-t outer entiode whis inger Glloy Gn, cayipinadit coulde \"uten can, porede so shos torw ouss or Fyoveat moute tete mim\n",
      "wack turls hl in thas  lonaeing on in an  \n",
      "----\n",
      "iter 6000, loss: 57.468095\n",
      "----\n",
      " e any tho wame kocuscen'd werppred\n",
      "ha doopt bing he door  om as the vens bet en ef difs the ip of cayo the hilg habed nop go't hanrqas ethonnt earss utipley ortis lork he  nonv-s th thou dome she leep \n",
      "----\n",
      "iter 7000, loss: 53.479469\n",
      "----\n",
      " .\n",
      "Et. ploothers?\"\n",
      "\"Wophe at bullepy. tw dor was and coulubsithes ortund Gretsind mowid s co his in ceayer monobldgave the, taa hicom of rouly thatt fatalicerseen deer at dowavis whimite mof wasraprtal \n",
      "----\n",
      "iter 8000, loss: 50.667529\n",
      "----\n",
      " or and roondsed nijoulf fuldyan hed ts wass his sigaveat hel it cokgy comer ad the be withe evedts bemere whe thas alf welre her waghigher bomeg ho une fnaln his his fo It dilt Gregorsewtoray nwo and  \n",
      "----\n",
      "iter 9000, loss: 49.517908\n",
      "----\n",
      " r anf, t ther tunginsy to shidgo - evenicgming fotinld, to ras and for seop ow heresd he efe sop waircen shout hiss the mirt hid; be the steistro if taghen and his werathere Gast of do shom it os jle  \n",
      "----\n",
      "iter 10000, loss: 49.213279\n",
      "----\n",
      "  iu haned do in do'pt ho the dhow putallcltut Grfoll sime and woul. \"jeded stasne leveut teoo teon at sors calind for epor't with at was int him shis had in se notclely orten het shok; caipod befe, go \n",
      "----\n",
      "iter 11000, loss: 56.120918\n",
      "----\n",
      "  ctrentis toriverligem.\n",
      "\n",
      "ftercacior tithichicjrtthoull GuSesleragumo PHrenmoje gou stearbe olestem (ince\n",
      "P dury famintmerb y of qutelne PrUSe sed tatu ansonicin frigin e\n",
      "SIvey to anf roobut her\n",
      "FTrom  \n",
      "----\n",
      "iter 12000, loss: 52.809689\n",
      "----\n",
      " Ise enecslell the blucly Gregurd, wap that Wentsbet of then with hasc sime to the whistce'sed to coulg. whirg, seat ie harpaturghasr lgoamd ure shies cefulrge  loure fe abrais workn ras, dims arven Ip \n",
      "----\n",
      "iter 13000, loss: 49.787801\n",
      "----\n",
      " , frebpastell ev otheqhenceny he dhat in in hasabrotlon call they arlt alvod wint douly thoust llos; ally pthis for. her 1en eored.y ated cod ave have and thaece could or be ans thouth bener inaeny al \n",
      "----\n",
      "iter 14000, loss: 48.185233\n",
      "----\n",
      " r not Guplning ore as nfe, tre th, wer, bavles thes thrmes on anlus wiree ind bomelly. The fore his t of t whoucser un Gorallent macy methat his werp mowlide, mreat pnied tmat fright wepr bowe thalk a \n",
      "----\n",
      "iter 15000, loss: 47.631153\n",
      "----\n",
      "  it He in at dastring ner ak that dang torealned the wouenting witht warly of his finter arsingy the dade momerret his \"gorese couidide as monter mor toom thereig. Somem; to on hes the pus. But ut of  \n",
      "----\n",
      "iter 16000, loss: 50.574777\n",
      "----\n",
      " oount beingurit and roverting coute, his went mute rand plevert is chemped bist beed to thiin lare mof tot. Serould frut hurt, yoot acne sidingungo thant th thet Gregor nory \"b ngor, conkis shot do th \n",
      "----\n",
      "iter 17000, loss: 52.602613\n",
      "----\n",
      " y f's inbo tre her the a dain of ofly whacte cordy had would and the cemy as he! She hem falpuncom thoot. As by\n",
      "kpeave te Wronet, his atll cofdeisly his instubsigh, him alb then.\n",
      "\n",
      "thenc: samer sid lou \n",
      "----\n",
      "iter 18000, loss: 49.831142\n",
      "----\n",
      " . In the coust hatlore thes. Irly his ngoulg balt iireis shald of of wared have arreat he done wit hais down's forne's Yo lo vent; thouter to and to keting at anderighathing iur had he tthy alf the el \n",
      "----\n",
      "iter 19000, loss: 47.638158\n",
      "----\n",
      "  somssufe as bet sere tos, had rae ledy what of sopucad ate out at cale?d his fing roly pafis whos, the wither of the the vefl of ate, ances ioss cussending at lit ho p eftimger hithod, deen he an wor \n",
      "----\n",
      "iter 20000, loss: 47.004735\n",
      "----\n",
      "  khe far wabkidetend beremed lyake gid, rell Gregor be mitincinksting to to anther the mow, alfe fud nove he nolkilled allily, coble move, at and of her's alre bathiced toos fre of his other doorselle \n",
      "----\n",
      "iter 21000, loss: 46.924812\n",
      "----\n",
      " skethit thou hih in coome an bent dasse bortey. At a sapro at they, sting checredry, tupets.\n",
      "Wherer roommrafe note shimmlisg ist, lor beetsly the mid whet in the ant yor'sen his badflning he ared ubl  \n",
      "----\n",
      "iter 22000, loss: 53.023800\n",
      "----\n",
      " om contace \n",
      "ul Dect no fe flagailly te of nots it on and and ataply not Doron- cockreckaspl SuSn armertwopr so aly cad cogh biout lied sopench\n",
      "laclola fous, burlestutting ienex\n",
      "wakings. IrCevertrort f \n",
      "----\n",
      "iter 23000, loss: 50.407948\n",
      "----\n",
      " e quttale on no daann anDorread thiid work acfut his metfeed on cleat thram aly slonger on thit the kit pequect to the ithor and any goven dasstsgen want the lough, thatcoomse pary, reaces ago plaag s \n",
      "----\n",
      "iter 24000, loss: 47.801083\n",
      "----\n",
      "  stakaths nog hus as tre the framis jot not hems alease farle like heen him to henk deith the inting bode ter, as she acrou on requterad, and he mlavelo of he of Gregor's go her itmedior male Gregor w \n",
      "----\n",
      "iter 25000, loss: 46.357404\n",
      "----\n",
      "  he be lerust.\n",
      "\"Id that ow her ar, becamenther Gregor, otsoute, for not nongeand so cwopss head on sivet baed wartiog Grekam bit Greto poffascarsay he, haw jo nor'w his athing thared bisawlald nest ov \n",
      "----\n",
      "iter 26000, loss: 46.075770\n",
      "----\n",
      " r coll letse, would at ag he pleun undy relpet ole on him the leall anllly rit waner not - foem door youfsinit he olee in paclit up leis of hiw of siCimaltt casn's thther Gregor on her would in the in \n",
      "----\n",
      "iter 27000, loss: 48.716005\n",
      "----\n",
      " emang hefis the cunzen lerk roon woulde roowo stistons mowht saste wouldighe rearsed in h ar seetenow Projut noog, in ic hencunchagroverting, meror Tipping, dacroofe toorigbeen acmoy could st\"rurkeupp \n",
      "----\n",
      "iter 28000, loss: 50.814191\n",
      "----\n",
      "  Irr his thtion batreablatt lith stong mowabase's ifsorentdint wrait could he to msthe thef Ind and uven of the shifad do olling have nodsly a brlact ther to itrove allly Liver worr crover, the dorst  \n",
      "----\n",
      "iter 29000, loss: 48.343072\n",
      "----\n",
      " t daked hast cas as noaly his hime, br seetm on incgo momive monter, in a piwl time the\n",
      "\"YO ot to b as with dowr throughwibret,, he at fo flaflecound he' he had roos quething Gregunen's tray wa�d thoo \n",
      "----\n",
      "iter 30000, loss: 46.297434\n",
      "----\n",
      " leit he th thod at if ero wtilrougus noumsy ipedy anpreed even Grket coplite tre moded onet tpme youn, war ore, be, it mimelf was asy shat his ut wookle the lay that mown and from wrowly sreirring to  \n",
      "----\n",
      "iter 31000, loss: 45.805084\n",
      "----\n",
      " oorce doot thoos headeamry room comsably for's peach, bed th us hans the bedstt Gregor slama in wery, wnimteat kence is resy; the wat muray the enjut onsale looker his mome sake Marnger nome to tee wa \n",
      "----\n",
      "iter 32000, loss: 45.718639\n",
      "----\n",
      " ng hied un at now. Grego doove bey begor she room rome he stirtoubutiog, wenteed there way nomd, of his ngoo whet that she Mredussicent on hishing ous ilcser seas, would mulf otppabped siccke she lais \n",
      "----\n",
      "iter 33000, loss: 51.363345\n",
      "----\n",
      "  of bunventer?\n",
      "\n",
      "The ceast cay of of enettbicertsmaim, Guterastist\n",
      " uted\n",
      "frarngun\n",
      "copUrSa Gute anduned of onlacict to inbper tide the Prage soonabe to rirtrit us stilly frossenally on fearne noos to fa \n",
      "----\n",
      "iter 34000, loss: 49.125860\n",
      "----\n",
      " o'd of was litl, peat had the whe for!Be't was peaded I Ae look dor, shed plikss all bede recad non couln of nitser the was somlide with sais as che ancaifne to phis bad peentas gouch roome and pomet  \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 46.662995\n",
      "----\n",
      " msieftrryims was novty the skiy a diresswer het hemwer - the head thoughtide tre ap ware statld gussentit Rhe! The doy anbeace of wid his motidhall sot sqoowe raftsed was the mongind not bly bregars n \n",
      "----\n",
      "iter 36000, loss: 45.300756\n",
      "----\n",
      " t hhing toon hed room dast jow and hamaid go exturd odis lo nou at had the cliemer he would someh f ge a thed and the ere buting darbe to the sas as door, to there bow had brboen to of now was him af  \n",
      "----\n",
      "iter 37000, loss: 45.143798\n",
      "----\n",
      " \n",
      "\n",
      "Shom not to mow his unte his moanded simienutis at. .S. He moselly out on on wit-s ho vech afte mownneraft he exse fouf woor had from annos bely ingant.'s aly, in aglo herly on a ooter and cond, was \n",
      "----\n",
      "iter 38000, loss: 47.557436\n",
      "----\n",
      " ressing Gregor fatad oute ans fich of thaye, tituleed wouch therewt cussens of cightmat\n",
      "eatlore workioussenmirst the by kech to the mayrent bupsed ordion sooped munts.\n",
      "\n",
      "Bupp, Gregoos of Gregor difde f \n",
      "----\n",
      "iter 39000, loss: 49.675683\n",
      "----\n",
      " dumlot put wot to ascpionp wanibes felcuslely's lethed sn*et: vinkist cas onasing anding noom, of war, even of aly yor's but beensliet him pere dundo whir as the crod, she statring net the courmes his \n",
      "----\n",
      "iter 40000, loss: 47.381582\n",
      "----\n",
      " or longl\" Gregor hes vo monct, no it his parion herenbusc sima An buttis momto ntile ho himert! Andwly rarded 1eas orend yood arlasced, at that in had cork and w id plight aly abe \"ut simha nojeleyo u \n",
      "----\n",
      "iter 41000, loss: 45.416002\n",
      "----\n",
      "  the wiided flat was orteer dafre gith hast the moter, the would was he would thtus lo-tlald ansing anther.  tha quieficactto bave his fis warer in he grom an in it preartiwing an to if ac bround byaf \n",
      "----\n",
      "iter 42000, loss: 45.002235\n",
      "----\n",
      " n elsso his room. Satint, hem shouge withly dow; coved the all enefuss of tithan therswonSer, his routhound ritaraw if to pade the warfit roomy the whee wast would and his lot and aboh one soinere teo \n",
      "----\n",
      "iter 43000, loss: 44.976064\n",
      "----\n",
      " o mighed to his her weshenower tered shough have the's in letilus remand, saread theuly momed ay fre.r\n",
      "uraed the do nong his work yist to he dain onfull. Hat the. It him Gregor to hed and his tore out \n",
      "----\n",
      "iter 44000, loss: 50.221168\n",
      "----\n",
      " ing bect tise and thore and up\n",
      "Livindy Gregor ter and movece at teat dion of the jemabling an'tre.\n",
      "H/ END 107 a olly at, out free an the roslit and pave ecancioh actly ortinht and domonne less, to gea \n",
      "----\n",
      "iter 45000, loss: 48.270763\n",
      "----\n",
      " m. To elencunt he mast enpealec ald calln in and was wpatac\" fiutly yor ore ofe as bo pofinen merinecr wis thy to he Nory rome sther to agh vooker the lould dice you long a caller rardeng that sher Pu \n",
      "----\n",
      "iter 46000, loss: 45.900747\n",
      "----\n",
      " could yary would upse Dothing, with edede whetnely at s'se sas und not wif renit's seir weand chemey shes to in, he ol, hemstremen staptbuto him tha do and waleedion, of his fareftes to - leard in hac \n",
      "----\n",
      "iter 47000, loss: 44.577728\n",
      "----\n",
      " -om he Gregor he foritt andation that thinerd him cowrde whens way foribulmone mothence and now comet oull ge's lect worke's in wook an fad wald sen to lay fro\" fich, he had notelll..\n",
      "\n",
      "Whe eotly frot; \n",
      "----\n",
      "iter 48000, loss: 44.478229\n",
      "----\n",
      " toughI.\n",
      "Ircmone spent now shained.at in baut having sould you sow the they. So fomem eo a thound and ut be ceing of thoasimingis thoung \"Il as the raring, macl was ably non. \"uof lithereed have shom h \n",
      "----\n",
      "iter 49000, loss: 46.742586\n",
      "----\n",
      "  notent cevere asy or; hamhittentt wame mater thet asy in to hing the the cighiceenluding tex a tand motidead ang to thing was sadior she masswicg\n",
      "inst blimwtion them loy would a lofkty a daut bo\n",
      "thou \n",
      "----\n",
      "iter 50000, loss: 48.894335\n",
      "----\n",
      " tatlle boF inco, ontellit! tond doo's hin the undeay ovet. And they way seeds himeras mople.\" rroncted for the ak with us thing en gite, for hime go mor and spsent; to anded workind thenst: liat to co \n",
      "----\n",
      "iter 51000, loss: 46.730441\n",
      "----\n",
      " atims as holral not onom twer to tishing sleep bosk stre clain. If calp, thous concenstidueds the ontsif and not nowed fesar thent of haprating she to ge he a for. I'd the onctreed, his purtenble out  \n",
      "----\n",
      "iter 52000, loss: 44.814136\n",
      "----\n",
      " ad at but he his ot letand forded and of that pongull casirion, but \"Youn an the heq in ar in at a medetseruly he butertso rimeter evend. Od had thes. Wivelly ir awiln quiatene the to had (if the done \n",
      "----\n",
      "iter 53000, loss: 44.456187\n",
      "----\n",
      " e n bouther to't the ore her fceal: a bentinitle as bef that her pasilisast frows bed rewims oact bifather so harr ove manirkion; him maghannt and houcher expyo was inking an awleg\n",
      "\n",
      "alst, azatly inden \n",
      "----\n",
      "iter 54000, loss: 44.435995\n",
      "----\n",
      " heake sisting the the cously forsiont cike, still froub\" ANsiout havis his bighings seole in loyin tooutt rais. Gregor to farin had thin bais mofed esene expreer and verto cas whed on ind armscoker bo \n",
      "----\n",
      "iter 55000, loss: 49.317723\n",
      "----\n",
      " ionely sceved insinf ot doid ag For putse said) a gerr becsuld of stonation hiint arstwer de Do ProFr mlancend brinitinly tishrapes\n",
      "\n",
      "ut the could ingiving ramsuata that a of forior\n",
      "\"axnt had peater th \n",
      "----\n",
      "iter 56000, loss: 47.611836\n",
      "----\n",
      "  the. I doand sefill the rouning thus siblen lot.\", mothey spenotlie's wingort the \"paudod caly chee bething he dor. Weme. H As but, chaff the is. Ighal!  his Yesk atisulls. Anly se thy. Horer arcemli \n",
      "----\n",
      "iter 57000, loss: 45.304776\n",
      "----\n",
      "  allpee it noon, her ofly, a had do tirk she ood want wrould dua sleel, asceen, the lato to was themouthablas hast..\" get nith?\n",
      "THe durs th uter cait and his taste, would spen, of thelfint bnay wat si \n",
      "----\n",
      "iter 58000, loss: 44.047158\n",
      "----\n",
      " utte padetatiout ole the diant bake. Fat kgo at so arCse ullepkury intlo whiliting he on mont hime, even quilg mopure of his pelt had half off room. Thtidwe. Wisc'st, roop towh andnenged to of his it  \n",
      "----\n",
      "iter 59000, loss: 43.961481\n",
      "----\n",
      " s osted his she ald owire toming to Gregor nesesell betho him hellyeniniln kan he sach, wake thest to hible enlearw, hting thaistein, reasten the loer have so sist uten he pomes, hely lever wouked tne \n",
      "----\n",
      "iter 60000, loss: 46.121278\n",
      "----\n",
      " mear decapsiting Grego dirgong nes.\"\n",
      "\n",
      "wornt the Projicening mropky any with utene ma may as tver of trencer anquoter eretron or to the. Hirtiws Un thoofe hill brospitay andothan come, oney. Thish to e \n",
      "----\n",
      "iter 61000, loss: 48.232046\n",
      "----\n",
      "  abrabring unded to his in, by 4egwsutt to in to it coulery but gotly vith that beft tlyarched thoughablely to he that ablunt, toor too prianhenf buttion. Ahronctire of his father to of in he steLer t \n",
      "----\n",
      "iter 62000, loss: 46.202881\n",
      "----\n",
      " silitta foon;, at tht hap that thattret shetming andond byonienselly kepe wam. B9 leet and.  at tralnly ay hravelf havitave he jpces whe door to gerk nother be che him be had eo paplly kiss, and, on t \n",
      "----\n",
      "iter 63000, loss: 44.316860\n",
      "----\n",
      " nd heintisctral to out Gregor thoo pay beceme Glewaple she did fer to him it ey her wasper himinge be that se of more heg his the roon sle that dood roomion his fest mixt his recurlly the wompulformed \n",
      "----\n",
      "iter 64000, loss: 43.980387\n",
      "----\n",
      " ey frens. When the foot to him his malbours Gaelly, I kon alouprehwarlys. They, of proos the out for him to orom as time thay o mare intt thout to hast note andfer anfast whirgy, th cidserick sowerle  \n",
      "----\n",
      "iter 65000, loss: 43.981745\n",
      "----\n",
      " ho's there waitome ated chend. The inmee he canlartout, in stust be for the gind oage helment bong go ontttoop haush. Gregor up afre have - sorp and the was was. He low; shag bee ppupe and here him ha \n",
      "----\n",
      "iter 66000, loss: 48.595532\n",
      "----\n",
      " t on anding famer covire e\n",
      "Lussres'm toppablist tse Dork? he foorprakicaing the with. \n",
      "gurdeed moreid in if tome\n",
      "S\n",
      "UJ\n",
      "d fur reand a roon modins of promom.\n",
      "\n",
      "Gofcle muat in toroy sadit and the lokinbely \n",
      "----\n",
      "iter 67000, loss: 47.063480\n",
      "----\n",
      " thens loop lay prode his hef wirs wotrainys, moveen; the coully, tpye moret and boknwong the ge couding and have siod the espen have ever marserasen\n",
      "\n",
      "ale nothing host foren lorealvest do trisas. Aspre \n",
      "----\n",
      "iter 68000, loss: 44.820313\n",
      "----\n",
      " ded Gregor's lers gons astind mo her inskys, one the ojoned not stack a tid, dast was to onming amyon hid a wan torfuast; a gith roud opperwand bee and the tupcempy by for remely ouct be whin; Gregor  \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 43.614220\n",
      "----\n",
      " oraly saneny out and was way what out ho had spay:: Heled clerttend reast what some in himunsice where then fralst only him stre hI, would that the walked that then time murst hice\n",
      "Gregor, suret and u \n",
      "----\n",
      "iter 70000, loss: 43.498591\n",
      "----\n",
      " atem. Yook room. An seiny could muster noncelbutiln ad not had sotent in the tha tast the they of thing he nogmed to days. On deik sale was the oumefly staed seat the theme\" with cheing anwing himentt \n",
      "----\n",
      "iter 71000, loss: 45.642971\n",
      "----\n",
      " e p\"ay sting t ture this ralout shem someerst bilf in th Arder.\n",
      "\n",
      "Soon in fachening faclost litine anise a ehen\n",
      "cims iving fransing the cotlarter and spite peypey/n1\n",
      "in it Dustay Gregor wicke time usin \n",
      "----\n",
      "iter 72000, loss: 47.692789\n",
      "----\n",
      " es noved workat topidid more that gonengand of tht plistoming for the prow a ber ter douh from wfftriogs dist thought sistion an gond'r, the thought moing quigun Save theis dissent thonge bive his ak  \n",
      "----\n",
      "iter 73000, loss: 45.721605\n",
      "----\n",
      " t pmaidever got\n",
      "ange\", foring with dopes his mor batta and\", compey wantelf hing, issaten warly aln hep, him betser fint to pfect you'ving wesay have to in ir the lackmack farack expreceriran to wamml \n",
      "----\n",
      "iter 74000, loss: 43.895633\n",
      "----\n",
      " wing; for - that Gregor'sly but out ffeation ol\"sulned blen batireter the . \n",
      "usr noemed as demer nolker Me sayore beel of tus cabongs andeser inde en his tsen wiret to himstitue ily.r dore had her uhe \n",
      "----\n",
      "iter 75000, loss: 43.640983\n",
      "----\n",
      " tio8 whion, what to couck, thine; the lr. Weper not had sha hend to en that owkd it weghion beh weme mare forlounds simiwing lige had nonly rojegu, but they beif portible too oune ford thoiny; the alu \n",
      "----\n",
      "iter 76000, loss: 43.580512\n",
      "----\n",
      "  sgought out in ghat sleck, erdent, the farryorp, his listo\". He helly ol\n",
      "wanet the mathis asly uprecpecpliwed oke her, biscleed his sel and room - hregrais frand rlal at that thew rave in thes his ko \n",
      "----\n",
      "iter 77000, loss: 47.977798\n",
      "----\n",
      " forcry of arm!Fop and biated stied tiens.  baasive chis lice ware word of chat indercarting thines.\"\n",
      "1.F 4Se's to uptisterCuft arat at he's, might Prwo\n",
      "worke courded mors edat iny ameibion, - Shere no \n",
      "----\n",
      "iter 78000, loss: 46.635563\n",
      "----\n",
      "  jut onltanter agree ansilin mants weplle amy whist, bodhat the canadlich, it knoty conbeatuer in seesled to rearpoved letsente not seralesh couths ko ches of the beampewaunst wellony thout thapring c \n",
      "----\n",
      "iter 79000, loss: 44.401685\n",
      "----\n",
      " (ble side at hert out wlove bater it could nemss close shandy ruen, that an?\"\n",
      "\n",
      "ustroN as, hel soor andongatses alther if Pryooked hers on there than oupper dist, Gregor the seghers hive his is, - alst \n",
      "----\n",
      "iter 80000, loss: 43.251007\n",
      "----\n",
      " hearm lect usenfed in nove ever suyly doo not's intsundeavish he g't his pelt backed diod room to peroving cawhe, lithated Gregor mo from verire and apleed in shoes to that her he could to to no-in th \n",
      "----\n",
      "iter 81000, loss: 43.161712\n",
      "----\n",
      " to withy the arilip the otheredinjed Gregorred my orlig. \"hermach the prouso ome then; day imm, to, line any an he's hee sage'vied. He whish murmed a ssapp bitunit th the could pbesersutly notaly at h \n",
      "----\n",
      "iter 82000, loss: 45.186561\n",
      "----\n",
      " Gokes's the tork\n",
      ", astarure this nook lokes ank\"ST\" Ang.C\n",
      "\n",
      "\n",
      "gex Fuldly whe'venbe som. Th! Thing a in\n",
      "thatr rowhing the. \"Herbolg If camerived chtultwat thar arly parp) temed not of the psike with work \n",
      "----\n",
      "iter 83000, loss: 47.223779\n",
      "----\n",
      " onhomt fuld tut as the deafer juston the lorelses onereinrlone idever him the.  the couching hime and motible muss neven sssiefted was he was bethes ene off vet morecced bable a Projecter that it not  \n",
      "----\n",
      "iter 84000, loss: 45.374723\n",
      "----\n",
      " long they hed a work doned: I\n",
      "Gule he was an really would thout his patarly couto his mearreded ansoountaint the woplent it he hur ontor the dyoussught the onots abret to charmous cup Gute bectrens al \n",
      "----\n",
      "iter 85000, loss: 43.534804\n",
      "----\n",
      "  couce sanionly wheeed of her\n",
      "gainwas land race anmond decked Gregor the could roonly agrouac staplarrert him iter\n",
      "arste they sonent that had trat.\" and repkened astiens smaly waitend wer not lot look \n",
      "----\n",
      "iter 86000, loss: 43.364064\n",
      "----\n",
      " hy tid raffere sithand; awas sead to yake with striewhe for work. Sand to him erthinging from otho wlad than the stor's that wany; fouh, supling bather opteded was it now sonelom. (tooring in in merow \n",
      "----\n",
      "iter 87000, loss: 43.270519\n",
      "----\n",
      " ntingey chad ir able he caghiot his lock, up. His pramatork stisc decunim of at itilned alu, pade not ind un beghit mowl to had's an sark appus chart and roawsly ned fleally to spely go berore te genl \n",
      "----\n",
      "iter 88000, loss: 47.441967\n",
      "----\n",
      " hy swid to workis. UTh to eroplightibe to ewhers into se't in thes\n",
      "CGreeflropreemer\n",
      "che\n",
      "1, uneas leckareingight cerained cfom the all ppurtus on wit wermsey bungabra ty ad moccsting the ofrong.\n",
      "\n",
      "Letti \n",
      "----\n",
      "iter 89000, loss: 46.244702\n",
      "----\n",
      "  thoot he moree would his recay stougherd. He nets orey, sance's fraid wiblentts'n had bibeidey. It thene on that came to works leement; trady and thread you mistion mpsiven seully; vey inded. Aid wiv \n",
      "----\n",
      "iter 90000, loss: 44.035974\n",
      "----\n",
      "  frose atay ever sivened projuro the though. But and pear, siadone couch incergatd his staying.\"\n",
      "Sow had lakisment bed he foutred uw maden anguled a meepeamous prcrece us Prousder nock, of the nicf th \n",
      "----\n",
      "iter 91000, loss: 43.013663\n",
      "----\n",
      " seven Gregorant, doret fer. \"O0 whervert and Ald, belt you had he dowathrart; beems, mater whing owaring somuss, had was'se to he gus one not Gregor's accent haming food aftuce her werasigherlering of \n",
      "----\n",
      "iter 92000, loss: 42.883168\n",
      "----\n",
      " xpyion. \"Duse wake the.; he of caghtion, he, in Mrtcaint about in the sunt wen. Voifned was on the boy in hree aid the chaserad eltens aid she woor ind.\n",
      "\n",
      "The lould the unsttilation some the bethat ale \n",
      "----\n",
      "iter 93000, loss: 44.820806\n",
      "----\n",
      " eds but thom alled any bed to paim coor whic with sodn nert and with mines with ip, rlow he de. by to onming Mr wiss, fomn't\n",
      "Thing though his wayengon sceved the thall thawr was of he dide, roully eft \n",
      "----\n",
      "iter 94000, loss: 46.882319\n",
      "----\n",
      " doon thom on would wiocerndeet Mrest as jas distion or ro poncery. \"Shen veld outhers withou the rimse slow a havolding a Provert wofkly to her room, hegs, hels all forqayny onto could whe remmol an s \n",
      "----\n",
      "iter 95000, loss: 45.077895\n",
      "----\n",
      " ugh. Therectse the forst thenely urdirinky seing it thouss wouldbout iisele was lfeally hork of sivel, no hap mAchtatier? I'N he she anf doom fit but dioke Greco the exces the resersining brobutes and \n",
      "----\n",
      "iter 96000, loss: 43.256693\n",
      "----\n",
      "  he dist bed, cund as leinzingver sleelumed troinilily slick of of from you dides, Hiss Guten bould net, in it was on theyeeft oundeftre ever, a slos cheps to in gell so at seet an the it mave saw Gre \n",
      "----\n",
      "iter 97000, loss: 43.142713\n",
      "----\n",
      " apthery notinioumely Fouf, the anther, wive ore leer her, any was Gregorm as was exclens grow fre ausid, quilowarwed the tnijesck cory disted and fther note, was wabed dpich, cifsed wifl heds bether r \n",
      "----\n",
      "iter 98000, loss: 43.036357\n",
      "----\n",
      "  can - the youn, bethe lint her hath hresver ii sion as then long.\" alate? as, Gregoreded nat hearl andenimsitally fearns reim. His motais bust chen the room to had out, room and and a her him seas to \n",
      "----\n",
      "iter 99000, loss: 46.995042\n",
      "----\n",
      " y sow purely aplirve that the a prais office illibladed the Futh: Golded, chating vapretss and of the Praw. cesred any ficg to goplictioned wiole the full cUke creikkly geelfry it 10. \n",
      "(Preotiwerwous: \n",
      "----\n",
      "iter 100000, loss: 45.915312\n",
      "----\n",
      " ve wenalf orte yore seg hors and time let t way? Lare dess from goecke and sowend. The shaund, thed pucted mudsion, osts, wifecht legh proxpists war the which he makememes. His quiet whayrewr expead b \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0  \n",
    "\n",
    "track_loss = []\n",
    "track_iter = []\n",
    "\n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net \n",
    "    # and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    #store training progress - MIKEY\n",
    "    track_loss.append(loss)\n",
    "    track_iter.append(n)\n",
    "    \n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # Perform parameter update with Adagrad  -- VERY IMPORTANT                                                                                                                                                   \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10ecf4550>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAGDCAYAAAC4Km19AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQggZYvwfoY3EWYU8Q5UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ecb67d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(track_iter[0:1000],track_loss[0:1000])\n",
    "plt.yscale('log')\n",
    "plt.title('Training Loss (1st 1000 Iterations)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The diagnostic plot plateaus after ~1000 iteration suggesting that the AdaGrad update may have saturated, reducing the effective learning rate to 0. To test this, change the optimizer to AdaDelta and see if the loss gets better compared to the AdaGrad results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
