{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet import nd, autograd, gluon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('processed_Iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = mx.io.CSVIter(data_csv='processed_Iris.csv', data_shape=(5,), batch_size=150,round_batch = False)\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for batch in data_iter:\n",
    "\n",
    "    temp = batch.data[0]\n",
    "    for i in temp:\n",
    "        X.append(i[:4])\n",
    "        Y.append(i[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True\n",
    "#                                                                  , transform=transform),\n",
    "#                                       batch_size, shuffle=True)\n",
    "\n",
    "# test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False\n",
    "#                                                                 , transform=transform),\n",
    "#                                      batch_size, shuffle=False)\n",
    "\n",
    "# Practice using Iris Dataset\n",
    "\n",
    "batch_size = 50\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X, Y),\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 4 \n",
    "num_outputs = 3\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    # 2 Hidden Layers\n",
    "    \n",
    "    \n",
    "    ###########################\n",
    "    # Adding first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    \n",
    "    ###########################\n",
    "    # Adding dropout with rate .5 to the first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dropout(0.1))\n",
    "\n",
    "    ###########################\n",
    "    # Adding first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    ###########################\n",
    "    # Adding dropout with rate .5 to the second hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dropout(0.1))\n",
    "\n",
    "    ###########################\n",
    "    # Adding the output layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude = 2.24),ctx = data_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax CrossEntropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'ADAM', {'learning_rate': .005})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(data_ctx)\n",
    "        label = label.as_in_context(data_ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.1935239762067793, Train_acc 0.333333333333\n",
      "Epoch 1. Loss: 1.191799668072429, Train_acc 0.333333333333\n",
      "Epoch 2. Loss: 1.1897135245845076, Train_acc 0.333333333333\n",
      "Epoch 3. Loss: 1.187486742722545, Train_acc 0.333333333333\n",
      "Epoch 4. Loss: 1.1853057692334141, Train_acc 0.286666666667\n",
      "Epoch 5. Loss: 1.1829687742653436, Train_acc 0.246666666667\n",
      "Epoch 6. Loss: 1.1806289227245599, Train_acc 0.333333333333\n",
      "Epoch 7. Loss: 1.1783153561915518, Train_acc 0.333333333333\n",
      "Epoch 8. Loss: 1.176047686513374, Train_acc 0.333333333333\n",
      "Epoch 9. Loss: 1.173813372545476, Train_acc 0.333333333333\n",
      "Epoch 10. Loss: 1.1716221734820231, Train_acc 0.333333333333\n",
      "Epoch 11. Loss: 1.1694973809537081, Train_acc 0.333333333333\n",
      "Epoch 12. Loss: 1.1674315910474744, Train_acc 0.333333333333\n",
      "Epoch 13. Loss: 1.165422452197005, Train_acc 0.333333333333\n",
      "Epoch 14. Loss: 1.163467764184438, Train_acc 0.333333333333\n",
      "Epoch 15. Loss: 1.1615633650635027, Train_acc 0.333333333333\n",
      "Epoch 16. Loss: 1.1597066242528589, Train_acc 0.333333333333\n",
      "Epoch 17. Loss: 1.1579041639764547, Train_acc 0.333333333333\n",
      "Epoch 18. Loss: 1.1561472745189703, Train_acc 0.333333333333\n",
      "Epoch 19. Loss: 1.1544408214173763, Train_acc 0.333333333333\n",
      "Epoch 20. Loss: 1.1527812731257476, Train_acc 0.333333333333\n",
      "Epoch 21. Loss: 1.151160266415338, Train_acc 0.333333333333\n",
      "Epoch 22. Loss: 1.149587386906104, Train_acc 0.333333333333\n",
      "Epoch 23. Loss: 1.1480549387163934, Train_acc 0.34\n",
      "Epoch 24. Loss: 1.1465625091033382, Train_acc 0.346666666667\n",
      "Epoch 25. Loss: 1.145114512103662, Train_acc 0.366666666667\n",
      "Epoch 26. Loss: 1.1436926491265056, Train_acc 0.393333333333\n",
      "Epoch 27. Loss: 1.1423016563445545, Train_acc 0.44\n",
      "Epoch 28. Loss: 1.1409564748460603, Train_acc 0.473333333333\n",
      "Epoch 29. Loss: 1.1396336405024823, Train_acc 0.613333333333\n",
      "Epoch 30. Loss: 1.1383218202894763, Train_acc 0.64\n",
      "Epoch 31. Loss: 1.1370207909202772, Train_acc 0.633333333333\n",
      "Epoch 32. Loss: 1.135752399867287, Train_acc 0.6\n",
      "Epoch 33. Loss: 1.1344902817102838, Train_acc 0.566666666667\n",
      "Epoch 34. Loss: 1.1332051900017852, Train_acc 0.553333333333\n",
      "Epoch 35. Loss: 1.131911476422835, Train_acc 0.533333333333\n",
      "Epoch 36. Loss: 1.130537919097458, Train_acc 0.5\n",
      "Epoch 37. Loss: 1.129136120272787, Train_acc 0.52\n",
      "Epoch 38. Loss: 1.1275832253991576, Train_acc 0.466666666667\n",
      "Epoch 39. Loss: 1.125890585396124, Train_acc 0.48\n",
      "Epoch 40. Loss: 1.1238915923964992, Train_acc 0.526666666667\n",
      "Epoch 41. Loss: 1.1215587229842712, Train_acc 0.54\n",
      "Epoch 42. Loss: 1.1188440031387954, Train_acc 0.573333333333\n",
      "Epoch 43. Loss: 1.1158497274745165, Train_acc 0.586666666667\n",
      "Epoch 44. Loss: 1.112244940942596, Train_acc 0.606666666667\n",
      "Epoch 45. Loss: 1.1081151187483222, Train_acc 0.62\n",
      "Epoch 46. Loss: 1.1031586223088141, Train_acc 0.68\n",
      "Epoch 47. Loss: 1.097669461745048, Train_acc 0.673333333333\n",
      "Epoch 48. Loss: 1.0912012722620743, Train_acc 0.666666666667\n",
      "Epoch 49. Loss: 1.0850456416882266, Train_acc 0.673333333333\n",
      "Epoch 50. Loss: 1.0780117634638047, Train_acc 0.726666666667\n",
      "Epoch 51. Loss: 1.069844757139302, Train_acc 0.7\n",
      "Epoch 52. Loss: 1.0620196983221961, Train_acc 0.693333333333\n",
      "Epoch 53. Loss: 1.0532725798400497, Train_acc 0.686666666667\n",
      "Epoch 54. Loss: 1.0442719832375182, Train_acc 0.706666666667\n",
      "Epoch 55. Loss: 1.0353499617674669, Train_acc 0.713333333333\n",
      "Epoch 56. Loss: 1.0271572458738032, Train_acc 0.72\n",
      "Epoch 57. Loss: 1.0197428101966513, Train_acc 0.733333333333\n",
      "Epoch 58. Loss: 1.011350443643789, Train_acc 0.726666666667\n",
      "Epoch 59. Loss: 1.0030596269290453, Train_acc 0.726666666667\n",
      "Epoch 60. Loss: 0.993993689352483, Train_acc 0.733333333333\n",
      "Epoch 61. Loss: 0.9845916293919231, Train_acc 0.74\n",
      "Epoch 62. Loss: 0.9760641946731942, Train_acc 0.733333333333\n",
      "Epoch 63. Loss: 0.967429819350578, Train_acc 0.733333333333\n",
      "Epoch 64. Loss: 0.9596407974589611, Train_acc 0.733333333333\n",
      "Epoch 65. Loss: 0.9522172542565034, Train_acc 0.74\n",
      "Epoch 66. Loss: 0.944388699828276, Train_acc 0.753333333333\n",
      "Epoch 67. Loss: 0.935595929141646, Train_acc 0.74\n",
      "Epoch 68. Loss: 0.9292411602864726, Train_acc 0.746666666667\n",
      "Epoch 69. Loss: 0.9221301081640881, Train_acc 0.766666666667\n",
      "Epoch 70. Loss: 0.9141308543787915, Train_acc 0.806666666667\n",
      "Epoch 71. Loss: 0.9063767401035583, Train_acc 0.806666666667\n",
      "Epoch 72. Loss: 0.8988464942288364, Train_acc 0.82\n",
      "Epoch 73. Loss: 0.8921745398705552, Train_acc 0.82\n",
      "Epoch 74. Loss: 0.8834917085704337, Train_acc 0.833333333333\n",
      "Epoch 75. Loss: 0.8769485350118755, Train_acc 0.833333333333\n",
      "Epoch 76. Loss: 0.8704453108560624, Train_acc 0.86\n",
      "Epoch 77. Loss: 0.8629683039629253, Train_acc 0.886666666667\n",
      "Epoch 78. Loss: 0.8574525073772579, Train_acc 0.893333333333\n",
      "Epoch 79. Loss: 0.8522533186759013, Train_acc 0.886666666667\n",
      "Epoch 80. Loss: 0.8441442029115787, Train_acc 0.886666666667\n",
      "Epoch 81. Loss: 0.8391247803915807, Train_acc 0.92\n",
      "Epoch 82. Loss: 0.8344300472275276, Train_acc 0.926666666667\n",
      "Epoch 83. Loss: 0.8282419394312063, Train_acc 0.926666666667\n",
      "Epoch 84. Loss: 0.8202930056638055, Train_acc 0.926666666667\n",
      "Epoch 85. Loss: 0.814683125091339, Train_acc 0.926666666667\n",
      "Epoch 86. Loss: 0.8107399018974091, Train_acc 0.946666666667\n",
      "Epoch 87. Loss: 0.8041852893197432, Train_acc 0.96\n",
      "Epoch 88. Loss: 0.799039915115965, Train_acc 0.946666666667\n",
      "Epoch 89. Loss: 0.794154412208131, Train_acc 0.946666666667\n",
      "Epoch 90. Loss: 0.789074203483162, Train_acc 0.96\n",
      "Epoch 91. Loss: 0.783800803086592, Train_acc 0.96\n",
      "Epoch 92. Loss: 0.776580083626475, Train_acc 0.96\n",
      "Epoch 93. Loss: 0.7706090003153645, Train_acc 0.953333333333\n",
      "Epoch 94. Loss: 0.7668607656691784, Train_acc 0.96\n",
      "Epoch 95. Loss: 0.7611633991225978, Train_acc 0.96\n",
      "Epoch 96. Loss: 0.7556737990895251, Train_acc 0.96\n",
      "Epoch 97. Loss: 0.7501008632357065, Train_acc 0.953333333333\n",
      "Epoch 98. Loss: 0.7439505181099315, Train_acc 0.933333333333\n",
      "Epoch 99. Loss: 0.7391299948178602, Train_acc 0.966666666667\n",
      "Epoch 100. Loss: 0.7339401850380268, Train_acc 0.96\n",
      "Epoch 101. Loss: 0.7300691166229056, Train_acc 0.94\n",
      "Epoch 102. Loss: 0.7236482747822355, Train_acc 0.933333333333\n",
      "Epoch 103. Loss: 0.7175952735093085, Train_acc 0.946666666667\n",
      "Epoch 104. Loss: 0.7131870568911001, Train_acc 0.966666666667\n",
      "Epoch 105. Loss: 0.7066132269727317, Train_acc 0.953333333333\n",
      "Epoch 106. Loss: 0.7007029264963666, Train_acc 0.94\n",
      "Epoch 107. Loss: 0.6953509414138116, Train_acc 0.946666666667\n",
      "Epoch 108. Loss: 0.6897141059970144, Train_acc 0.946666666667\n",
      "Epoch 109. Loss: 0.6860527324995322, Train_acc 0.96\n",
      "Epoch 110. Loss: 0.6819210497267308, Train_acc 0.946666666667\n",
      "Epoch 111. Loss: 0.6781336264761005, Train_acc 0.96\n",
      "Epoch 112. Loss: 0.6727801032586944, Train_acc 0.96\n",
      "Epoch 113. Loss: 0.6670969069799463, Train_acc 0.946666666667\n",
      "Epoch 114. Loss: 0.6634669846563895, Train_acc 0.946666666667\n",
      "Epoch 115. Loss: 0.658993451083105, Train_acc 0.96\n",
      "Epoch 116. Loss: 0.6566401132521891, Train_acc 0.966666666667\n",
      "Epoch 117. Loss: 0.6517519942087124, Train_acc 0.946666666667\n",
      "Epoch 118. Loss: 0.6485975551255825, Train_acc 0.946666666667\n",
      "Epoch 119. Loss: 0.6448957711065372, Train_acc 0.973333333333\n",
      "Epoch 120. Loss: 0.6403666386682902, Train_acc 0.96\n",
      "Epoch 121. Loss: 0.6343754846228257, Train_acc 0.966666666667\n",
      "Epoch 122. Loss: 0.630062783464481, Train_acc 0.953333333333\n",
      "Epoch 123. Loss: 0.6265129702648873, Train_acc 0.953333333333\n",
      "Epoch 124. Loss: 0.6221569977568849, Train_acc 0.973333333333\n",
      "Epoch 125. Loss: 0.6184142103350018, Train_acc 0.966666666667\n",
      "Epoch 126. Loss: 0.6164891551111017, Train_acc 0.953333333333\n",
      "Epoch 127. Loss: 0.6115676560161193, Train_acc 0.96\n",
      "Epoch 128. Loss: 0.6057161996556618, Train_acc 0.966666666667\n",
      "Epoch 129. Loss: 0.6016078611296344, Train_acc 0.966666666667\n",
      "Epoch 130. Loss: 0.5995640242547269, Train_acc 0.966666666667\n",
      "Epoch 131. Loss: 0.5954256315750094, Train_acc 0.966666666667\n",
      "Epoch 132. Loss: 0.5903309126893681, Train_acc 0.966666666667\n",
      "Epoch 133. Loss: 0.5860835998067578, Train_acc 0.96\n",
      "Epoch 134. Loss: 0.5829751084010197, Train_acc 0.966666666667\n",
      "Epoch 135. Loss: 0.5813853521143715, Train_acc 0.966666666667\n",
      "Epoch 136. Loss: 0.5797861124123911, Train_acc 0.973333333333\n",
      "Epoch 137. Loss: 0.5779433554223691, Train_acc 0.966666666667\n",
      "Epoch 138. Loss: 0.573695627747576, Train_acc 0.973333333333\n",
      "Epoch 139. Loss: 0.5706345187785569, Train_acc 0.973333333333\n",
      "Epoch 140. Loss: 0.5663643968913727, Train_acc 0.966666666667\n",
      "Epoch 141. Loss: 0.5601634330730847, Train_acc 0.966666666667\n",
      "Epoch 142. Loss: 0.5554136389385047, Train_acc 0.973333333333\n",
      "Epoch 143. Loss: 0.5510282181003476, Train_acc 0.973333333333\n",
      "Epoch 144. Loss: 0.5477559651065119, Train_acc 0.973333333333\n",
      "Epoch 145. Loss: 0.5438968672008171, Train_acc 0.966666666667\n",
      "Epoch 146. Loss: 0.5403376298937489, Train_acc 0.973333333333\n",
      "Epoch 147. Loss: 0.5402060003643384, Train_acc 0.96\n",
      "Epoch 148. Loss: 0.5389448728295778, Train_acc 0.973333333333\n",
      "Epoch 149. Loss: 0.5336858858802765, Train_acc 0.966666666667\n",
      "Epoch 150. Loss: 0.5332072853222187, Train_acc 0.966666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151. Loss: 0.5321192084109513, Train_acc 0.973333333333\n",
      "Epoch 152. Loss: 0.5280314509506547, Train_acc 0.973333333333\n",
      "Epoch 153. Loss: 0.5264410537366135, Train_acc 0.973333333333\n",
      "Epoch 154. Loss: 0.5241138968050624, Train_acc 0.966666666667\n",
      "Epoch 155. Loss: 0.5192151997641188, Train_acc 0.966666666667\n",
      "Epoch 156. Loss: 0.5157603628097185, Train_acc 0.966666666667\n",
      "Epoch 157. Loss: 0.5117693566256715, Train_acc 0.966666666667\n",
      "Epoch 158. Loss: 0.5101870271974904, Train_acc 0.98\n",
      "Epoch 159. Loss: 0.5092105148915479, Train_acc 0.98\n",
      "Epoch 160. Loss: 0.5089489743809139, Train_acc 0.973333333333\n",
      "Epoch 161. Loss: 0.5095574855694187, Train_acc 0.973333333333\n",
      "Epoch 162. Loss: 0.5086543314813982, Train_acc 0.973333333333\n",
      "Epoch 163. Loss: 0.5047650758958779, Train_acc 0.973333333333\n",
      "Epoch 164. Loss: 0.5043006921894105, Train_acc 0.966666666667\n",
      "Epoch 165. Loss: 0.5033911069660375, Train_acc 0.966666666667\n",
      "Epoch 166. Loss: 0.5031714207777825, Train_acc 0.973333333333\n",
      "Epoch 167. Loss: 0.5026715262960844, Train_acc 0.98\n",
      "Epoch 168. Loss: 0.5022280585796152, Train_acc 0.98\n",
      "Epoch 169. Loss: 0.5016776743287281, Train_acc 0.966666666667\n",
      "Epoch 170. Loss: 0.4998361876020836, Train_acc 0.966666666667\n",
      "Epoch 171. Loss: 0.49866708875722876, Train_acc 0.966666666667\n",
      "Epoch 172. Loss: 0.4974917127712759, Train_acc 0.98\n",
      "Epoch 173. Loss: 0.494801597179385, Train_acc 0.98\n",
      "Epoch 174. Loss: 0.4916365265638805, Train_acc 0.98\n",
      "Epoch 175. Loss: 0.4903526771424173, Train_acc 0.98\n",
      "Epoch 176. Loss: 0.4875144441401719, Train_acc 0.966666666667\n",
      "Epoch 177. Loss: 0.48369040930115675, Train_acc 0.966666666667\n",
      "Epoch 178. Loss: 0.48295832280352213, Train_acc 0.98\n",
      "Epoch 179. Loss: 0.4824292946893183, Train_acc 0.98\n",
      "Epoch 180. Loss: 0.48062679374334055, Train_acc 0.98\n",
      "Epoch 181. Loss: 0.48042963400474553, Train_acc 0.98\n",
      "Epoch 182. Loss: 0.47812741817492904, Train_acc 0.966666666667\n",
      "Epoch 183. Loss: 0.47637569950937747, Train_acc 0.966666666667\n",
      "Epoch 184. Loss: 0.47493245442683457, Train_acc 0.966666666667\n",
      "Epoch 185. Loss: 0.47223999187364635, Train_acc 0.98\n",
      "Epoch 186. Loss: 0.47248706073595526, Train_acc 0.98\n",
      "Epoch 187. Loss: 0.4710658836126416, Train_acc 0.98\n",
      "Epoch 188. Loss: 0.4719568696799245, Train_acc 0.966666666667\n",
      "Epoch 189. Loss: 0.4704864064040525, Train_acc 0.96\n",
      "Epoch 190. Loss: 0.46887197904130584, Train_acc 0.966666666667\n",
      "Epoch 191. Loss: 0.46586632948982143, Train_acc 0.98\n",
      "Epoch 192. Loss: 0.4664936989663615, Train_acc 0.98\n",
      "Epoch 193. Loss: 0.4635588032918992, Train_acc 0.966666666667\n",
      "Epoch 194. Loss: 0.4628150825504721, Train_acc 0.966666666667\n",
      "Epoch 195. Loss: 0.4602331371808459, Train_acc 0.98\n",
      "Epoch 196. Loss: 0.45653259906995663, Train_acc 0.98\n",
      "Epoch 197. Loss: 0.4557833718015304, Train_acc 0.98\n",
      "Epoch 198. Loss: 0.4554632569505099, Train_acc 0.98\n",
      "Epoch 199. Loss: 0.4537304881927773, Train_acc 0.98\n",
      "Epoch 200. Loss: 0.4529281921999607, Train_acc 0.966666666667\n",
      "Epoch 201. Loss: 0.45148598748840124, Train_acc 0.966666666667\n",
      "Epoch 202. Loss: 0.44997195944027335, Train_acc 0.98\n",
      "Epoch 203. Loss: 0.4493046051315372, Train_acc 0.98\n",
      "Epoch 204. Loss: 0.4506772825304584, Train_acc 0.98\n",
      "Epoch 205. Loss: 0.45190220941512627, Train_acc 0.966666666667\n",
      "Epoch 206. Loss: 0.4523022905948016, Train_acc 0.98\n",
      "Epoch 207. Loss: 0.452379856835687, Train_acc 0.98\n",
      "Epoch 208. Loss: 0.45061987158268907, Train_acc 0.98\n",
      "Epoch 209. Loss: 0.4505540638413069, Train_acc 0.966666666667\n",
      "Epoch 210. Loss: 0.4473231619112115, Train_acc 0.966666666667\n",
      "Epoch 211. Loss: 0.4467482850127336, Train_acc 0.98\n",
      "Epoch 212. Loss: 0.4474844014886117, Train_acc 0.98\n",
      "Epoch 213. Loss: 0.4488752453816334, Train_acc 0.98\n",
      "Epoch 214. Loss: 0.4475587987327423, Train_acc 0.973333333333\n",
      "Epoch 215. Loss: 0.44634941541258233, Train_acc 0.973333333333\n",
      "Epoch 216. Loss: 0.444572827996743, Train_acc 0.973333333333\n",
      "Epoch 217. Loss: 0.44180224778028043, Train_acc 0.98\n",
      "Epoch 218. Loss: 0.4386977813038989, Train_acc 0.973333333333\n",
      "Epoch 219. Loss: 0.4411259194931296, Train_acc 0.98\n",
      "Epoch 220. Loss: 0.43871773708717976, Train_acc 0.98\n",
      "Epoch 221. Loss: 0.43671742612032566, Train_acc 0.973333333333\n",
      "Epoch 222. Loss: 0.4343520484662836, Train_acc 0.98\n",
      "Epoch 223. Loss: 0.43409578691995027, Train_acc 0.98\n",
      "Epoch 224. Loss: 0.4345948519313259, Train_acc 0.98\n",
      "Epoch 225. Loss: 0.43154742431497417, Train_acc 0.973333333333\n",
      "Epoch 226. Loss: 0.4321611081517896, Train_acc 0.973333333333\n",
      "Epoch 227. Loss: 0.4308485477695941, Train_acc 0.98\n",
      "Epoch 228. Loss: 0.43074673707025707, Train_acc 0.98\n",
      "Epoch 229. Loss: 0.42957329770153224, Train_acc 0.973333333333\n",
      "Epoch 230. Loss: 0.42721520860971757, Train_acc 0.98\n",
      "Epoch 231. Loss: 0.42289966550767716, Train_acc 0.98\n",
      "Epoch 232. Loss: 0.4202800603580104, Train_acc 0.98\n",
      "Epoch 233. Loss: 0.4195817495904493, Train_acc 0.973333333333\n",
      "Epoch 234. Loss: 0.4200789050568575, Train_acc 0.973333333333\n",
      "Epoch 235. Loss: 0.4199079314492969, Train_acc 0.98\n",
      "Epoch 236. Loss: 0.41813310268823206, Train_acc 0.98\n",
      "Epoch 237. Loss: 0.4188152457537898, Train_acc 0.98\n",
      "Epoch 238. Loss: 0.4186555634469731, Train_acc 0.973333333333\n",
      "Epoch 239. Loss: 0.4198124946366234, Train_acc 0.966666666667\n",
      "Epoch 240. Loss: 0.4188032999571354, Train_acc 0.98\n",
      "Epoch 241. Loss: 0.41896635679828426, Train_acc 0.98\n",
      "Epoch 242. Loss: 0.41637875248101974, Train_acc 0.973333333333\n",
      "Epoch 243. Loss: 0.4161569540603907, Train_acc 0.973333333333\n",
      "Epoch 244. Loss: 0.41477514088093625, Train_acc 0.98\n",
      "Epoch 245. Loss: 0.41389173119694406, Train_acc 0.98\n",
      "Epoch 246. Loss: 0.4170428585218139, Train_acc 0.973333333333\n",
      "Epoch 247. Loss: 0.4198298189603722, Train_acc 0.973333333333\n",
      "Epoch 248. Loss: 0.4203847538825044, Train_acc 0.973333333333\n",
      "Epoch 249. Loss: 0.4180114122751764, Train_acc 0.973333333333\n",
      "Epoch 250. Loss: 0.41832115900439165, Train_acc 0.98\n",
      "Epoch 251. Loss: 0.4202469577889184, Train_acc 0.98\n",
      "Epoch 252. Loss: 0.420178152686942, Train_acc 0.973333333333\n",
      "Epoch 253. Loss: 0.4191391450286339, Train_acc 0.98\n",
      "Epoch 254. Loss: 0.4199513898681573, Train_acc 0.98\n",
      "Epoch 255. Loss: 0.42451668562464445, Train_acc 0.973333333333\n",
      "Epoch 256. Loss: 0.423008148346982, Train_acc 0.98\n",
      "Epoch 257. Loss: 0.42294822532390586, Train_acc 0.98\n",
      "Epoch 258. Loss: 0.4206599417798968, Train_acc 0.98\n",
      "Epoch 259. Loss: 0.4205744108141137, Train_acc 0.98\n",
      "Epoch 260. Loss: 0.42105151343617697, Train_acc 0.973333333333\n",
      "Epoch 261. Loss: 0.4186433488765656, Train_acc 0.973333333333\n",
      "Epoch 262. Loss: 0.42259577380633123, Train_acc 0.98\n",
      "Epoch 263. Loss: 0.42136714493436545, Train_acc 0.98\n",
      "Epoch 264. Loss: 0.4208447385978278, Train_acc 0.98\n",
      "Epoch 265. Loss: 0.4192070275322621, Train_acc 0.98\n",
      "Epoch 266. Loss: 0.41997925443872475, Train_acc 0.973333333333\n",
      "Epoch 267. Loss: 0.4197468270892396, Train_acc 0.98\n",
      "Epoch 268. Loss: 0.41648423181877625, Train_acc 0.98\n",
      "Epoch 269. Loss: 0.41502124259019374, Train_acc 0.973333333333\n",
      "Epoch 270. Loss: 0.41597471830778016, Train_acc 0.98\n",
      "Epoch 271. Loss: 0.4145054317002175, Train_acc 0.98\n",
      "Epoch 272. Loss: 0.41611477101317906, Train_acc 0.98\n",
      "Epoch 273. Loss: 0.417458630883462, Train_acc 0.98\n",
      "Epoch 274. Loss: 0.4153833680079297, Train_acc 0.98\n",
      "Epoch 275. Loss: 0.4156051343864861, Train_acc 0.98\n",
      "Epoch 276. Loss: 0.41523239235843745, Train_acc 0.973333333333\n",
      "Epoch 277. Loss: 0.41228115111121105, Train_acc 0.98\n",
      "Epoch 278. Loss: 0.41431866685535523, Train_acc 0.98\n",
      "Epoch 279. Loss: 0.41347310683322774, Train_acc 0.98\n",
      "Epoch 280. Loss: 0.4152208175042039, Train_acc 0.973333333333\n",
      "Epoch 281. Loss: 0.41637562164045683, Train_acc 0.973333333333\n",
      "Epoch 282. Loss: 0.4159930762801772, Train_acc 0.973333333333\n",
      "Epoch 283. Loss: 0.4191634013224351, Train_acc 0.98\n",
      "Epoch 284. Loss: 0.4188883183011892, Train_acc 0.98\n",
      "Epoch 285. Loss: 0.4204520457762384, Train_acc 0.973333333333\n",
      "Epoch 286. Loss: 0.4170671103808608, Train_acc 0.973333333333\n",
      "Epoch 287. Loss: 0.4192188644136242, Train_acc 0.98\n",
      "Epoch 288. Loss: 0.41882632149265686, Train_acc 0.98\n",
      "Epoch 289. Loss: 0.41954591610882885, Train_acc 0.98\n",
      "Epoch 290. Loss: 0.416657236035342, Train_acc 0.973333333333\n",
      "Epoch 291. Loss: 0.4182787694473689, Train_acc 0.973333333333\n",
      "Epoch 292. Loss: 0.4179576847197077, Train_acc 0.98\n",
      "Epoch 293. Loss: 0.418070079028912, Train_acc 0.98\n",
      "Epoch 294. Loss: 0.41950687708971834, Train_acc 0.98\n",
      "Epoch 295. Loss: 0.4193314675251068, Train_acc 0.98\n",
      "Epoch 296. Loss: 0.4184431683975163, Train_acc 0.98\n",
      "Epoch 297. Loss: 0.42016724859816895, Train_acc 0.98\n",
      "Epoch 298. Loss: 0.4174546147728731, Train_acc 0.98\n",
      "Epoch 299. Loss: 0.4159979882834538, Train_acc 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300. Loss: 0.4158459536881124, Train_acc 0.98\n",
      "Epoch 301. Loss: 0.4132993635671646, Train_acc 0.98\n",
      "Epoch 302. Loss: 0.4129755874328827, Train_acc 0.98\n",
      "Epoch 303. Loss: 0.4135318166506291, Train_acc 0.98\n",
      "Epoch 304. Loss: 0.41350998245370596, Train_acc 0.98\n",
      "Epoch 305. Loss: 0.41221182021519026, Train_acc 0.98\n",
      "Epoch 306. Loss: 0.41381882936467035, Train_acc 0.98\n",
      "Epoch 307. Loss: 0.4123509947661353, Train_acc 0.98\n",
      "Epoch 308. Loss: 0.41311879735675616, Train_acc 0.98\n",
      "Epoch 309. Loss: 0.4130342513666373, Train_acc 0.98\n",
      "Epoch 310. Loss: 0.41311588959514495, Train_acc 0.98\n",
      "Epoch 311. Loss: 0.4136631236486209, Train_acc 0.973333333333\n",
      "Epoch 312. Loss: 0.411340495581056, Train_acc 0.98\n",
      "Epoch 313. Loss: 0.41022543514194715, Train_acc 0.98\n",
      "Epoch 314. Loss: 0.40735454094995927, Train_acc 0.98\n",
      "Epoch 315. Loss: 0.40612649958681285, Train_acc 0.98\n",
      "Epoch 316. Loss: 0.4047671833867833, Train_acc 0.973333333333\n",
      "Epoch 317. Loss: 0.40237915630779125, Train_acc 0.973333333333\n",
      "Epoch 318. Loss: 0.40346380761558603, Train_acc 0.98\n",
      "Epoch 319. Loss: 0.4038626895176038, Train_acc 0.98\n",
      "Epoch 320. Loss: 0.4048710541132258, Train_acc 0.98\n",
      "Epoch 321. Loss: 0.40375080406827296, Train_acc 0.98\n",
      "Epoch 322. Loss: 0.4035896451657914, Train_acc 0.98\n",
      "Epoch 323. Loss: 0.4051549760760416, Train_acc 0.98\n",
      "Epoch 324. Loss: 0.4049096207390183, Train_acc 0.98\n",
      "Epoch 325. Loss: 0.4064553615780598, Train_acc 0.98\n",
      "Epoch 326. Loss: 0.40513669969259997, Train_acc 0.98\n",
      "Epoch 327. Loss: 0.4109584629017679, Train_acc 0.973333333333\n",
      "Epoch 328. Loss: 0.41049907671313635, Train_acc 0.98\n",
      "Epoch 329. Loss: 0.4121271798174542, Train_acc 0.98\n",
      "Epoch 330. Loss: 0.4131231552588693, Train_acc 0.98\n",
      "Epoch 331. Loss: 0.4114145158394516, Train_acc 0.98\n",
      "Epoch 332. Loss: 0.40798884987964146, Train_acc 0.98\n",
      "Epoch 333. Loss: 0.40918655766371465, Train_acc 0.98\n",
      "Epoch 334. Loss: 0.40844107505428084, Train_acc 0.98\n",
      "Epoch 335. Loss: 0.4066734088561143, Train_acc 0.98\n",
      "Epoch 336. Loss: 0.40694424207199875, Train_acc 0.98\n",
      "Epoch 337. Loss: 0.4065366720322117, Train_acc 0.98\n",
      "Epoch 338. Loss: 0.40633776540806477, Train_acc 0.98\n",
      "Epoch 339. Loss: 0.40962997297599796, Train_acc 0.98\n",
      "Epoch 340. Loss: 0.4110591881926761, Train_acc 0.98\n",
      "Epoch 341. Loss: 0.40892030846740723, Train_acc 0.98\n",
      "Epoch 342. Loss: 0.4083813726660747, Train_acc 0.98\n",
      "Epoch 343. Loss: 0.41340132963334053, Train_acc 0.98\n",
      "Epoch 344. Loss: 0.41040606137803576, Train_acc 0.98\n",
      "Epoch 345. Loss: 0.41066335947893867, Train_acc 0.98\n",
      "Epoch 346. Loss: 0.40780605354297633, Train_acc 0.98\n",
      "Epoch 347. Loss: 0.41218223297255097, Train_acc 0.98\n",
      "Epoch 348. Loss: 0.4123942707555803, Train_acc 0.973333333333\n",
      "Epoch 349. Loss: 0.4102246440350723, Train_acc 0.973333333333\n",
      "Epoch 350. Loss: 0.411242185311289, Train_acc 0.98\n",
      "Epoch 351. Loss: 0.40954999534064823, Train_acc 0.98\n",
      "Epoch 352. Loss: 0.40909536483790465, Train_acc 0.98\n",
      "Epoch 353. Loss: 0.40728827495656283, Train_acc 0.98\n",
      "Epoch 354. Loss: 0.41135942093281963, Train_acc 0.98\n",
      "Epoch 355. Loss: 0.41059289562480566, Train_acc 0.98\n",
      "Epoch 356. Loss: 0.41059763821423806, Train_acc 0.98\n",
      "Epoch 357. Loss: 0.41129955750573755, Train_acc 0.98\n",
      "Epoch 358. Loss: 0.4138110445846564, Train_acc 0.973333333333\n",
      "Epoch 359. Loss: 0.4091142650184998, Train_acc 0.966666666667\n",
      "Epoch 360. Loss: 0.4064059203672524, Train_acc 0.973333333333\n",
      "Epoch 361. Loss: 0.4092533681712616, Train_acc 0.98\n",
      "Epoch 362. Loss: 0.41041733291402344, Train_acc 0.98\n",
      "Epoch 363. Loss: 0.4085880232169505, Train_acc 0.98\n",
      "Epoch 364. Loss: 0.40642731450678204, Train_acc 0.98\n",
      "Epoch 365. Loss: 0.4035252310136914, Train_acc 0.973333333333\n",
      "Epoch 366. Loss: 0.40373708745654163, Train_acc 0.98\n",
      "Epoch 367. Loss: 0.4027905360660213, Train_acc 0.98\n",
      "Epoch 368. Loss: 0.40342714987706785, Train_acc 0.98\n",
      "Epoch 369. Loss: 0.40246148781952706, Train_acc 0.973333333333\n",
      "Epoch 370. Loss: 0.40615694764230986, Train_acc 0.98\n",
      "Epoch 371. Loss: 0.40810805167985914, Train_acc 0.98\n",
      "Epoch 372. Loss: 0.40591289541511966, Train_acc 0.98\n",
      "Epoch 373. Loss: 0.40436631609640555, Train_acc 0.98\n",
      "Epoch 374. Loss: 0.4081019352900634, Train_acc 0.98\n",
      "Epoch 375. Loss: 0.41062377240262804, Train_acc 0.973333333333\n",
      "Epoch 376. Loss: 0.4110306957553204, Train_acc 0.98\n",
      "Epoch 377. Loss: 0.40893072628856325, Train_acc 0.98\n",
      "Epoch 378. Loss: 0.4099955745453102, Train_acc 0.98\n",
      "Epoch 379. Loss: 0.4091887981175, Train_acc 0.98\n",
      "Epoch 380. Loss: 0.4100487083162158, Train_acc 0.98\n",
      "Epoch 381. Loss: 0.41143236539093353, Train_acc 0.98\n",
      "Epoch 382. Loss: 0.4110816076023209, Train_acc 0.973333333333\n",
      "Epoch 383. Loss: 0.4133876134660915, Train_acc 0.973333333333\n",
      "Epoch 384. Loss: 0.41433319028457055, Train_acc 0.973333333333\n",
      "Epoch 385. Loss: 0.41396149459350245, Train_acc 0.98\n",
      "Epoch 386. Loss: 0.4127157934358069, Train_acc 0.98\n",
      "Epoch 387. Loss: 0.4138174380123203, Train_acc 0.98\n",
      "Epoch 388. Loss: 0.4154498111406813, Train_acc 0.966666666667\n",
      "Epoch 389. Loss: 0.4157954937590543, Train_acc 0.966666666667\n",
      "Epoch 390. Loss: 0.41457096061143356, Train_acc 0.98\n",
      "Epoch 391. Loss: 0.4146562092189995, Train_acc 0.98\n",
      "Epoch 392. Loss: 0.4133689359321982, Train_acc 0.98\n",
      "Epoch 393. Loss: 0.41201449128274514, Train_acc 0.98\n",
      "Epoch 394. Loss: 0.40975874535464096, Train_acc 0.98\n",
      "Epoch 395. Loss: 0.4090668000561193, Train_acc 0.98\n",
      "Epoch 396. Loss: 0.406760903208914, Train_acc 0.98\n",
      "Epoch 397. Loss: 0.4073445005889282, Train_acc 0.98\n",
      "Epoch 398. Loss: 0.4085673065672494, Train_acc 0.98\n",
      "Epoch 399. Loss: 0.4059306313269046, Train_acc 0.98\n",
      "Epoch 400. Loss: 0.40249009750998993, Train_acc 0.98\n",
      "Epoch 401. Loss: 0.40271622373836696, Train_acc 0.98\n",
      "Epoch 402. Loss: 0.4023435204780495, Train_acc 0.98\n",
      "Epoch 403. Loss: 0.40492446478548455, Train_acc 0.98\n",
      "Epoch 404. Loss: 0.4072508014838017, Train_acc 0.98\n",
      "Epoch 405. Loss: 0.40846859295323457, Train_acc 0.973333333333\n",
      "Epoch 406. Loss: 0.4097393759899058, Train_acc 0.98\n",
      "Epoch 407. Loss: 0.41128723194027655, Train_acc 0.98\n",
      "Epoch 408. Loss: 0.4097608468621825, Train_acc 0.98\n",
      "Epoch 409. Loss: 0.41133687098526206, Train_acc 0.98\n",
      "Epoch 410. Loss: 0.4095116974851674, Train_acc 0.98\n",
      "Epoch 411. Loss: 0.41033251720959063, Train_acc 0.98\n",
      "Epoch 412. Loss: 0.41152218554059805, Train_acc 0.98\n",
      "Epoch 413. Loss: 0.41192105625617964, Train_acc 0.98\n",
      "Epoch 414. Loss: 0.4098739671249703, Train_acc 0.98\n",
      "Epoch 415. Loss: 0.40843674331785684, Train_acc 0.98\n",
      "Epoch 416. Loss: 0.4056920199140526, Train_acc 0.98\n",
      "Epoch 417. Loss: 0.406452668259159, Train_acc 0.98\n",
      "Epoch 418. Loss: 0.40833852942240145, Train_acc 0.98\n",
      "Epoch 419. Loss: 0.4105921556293838, Train_acc 0.98\n",
      "Epoch 420. Loss: 0.4101925439217317, Train_acc 0.98\n",
      "Epoch 421. Loss: 0.4097303600346939, Train_acc 0.973333333333\n",
      "Epoch 422. Loss: 0.4122146460298239, Train_acc 0.973333333333\n",
      "Epoch 423. Loss: 0.41052223273947874, Train_acc 0.98\n",
      "Epoch 424. Loss: 0.4112258655307055, Train_acc 0.98\n",
      "Epoch 425. Loss: 0.41066916055174474, Train_acc 0.98\n",
      "Epoch 426. Loss: 0.4090833440314808, Train_acc 0.973333333333\n",
      "Epoch 427. Loss: 0.4066087399100202, Train_acc 0.973333333333\n",
      "Epoch 428. Loss: 0.40694539851897443, Train_acc 0.98\n",
      "Epoch 429. Loss: 0.40726265728674194, Train_acc 0.98\n",
      "Epoch 430. Loss: 0.4067523685836836, Train_acc 0.98\n",
      "Epoch 431. Loss: 0.40797919385268666, Train_acc 0.98\n",
      "Epoch 432. Loss: 0.4077810480952142, Train_acc 0.98\n",
      "Epoch 433. Loss: 0.4101271315451836, Train_acc 0.98\n",
      "Epoch 434. Loss: 0.40912038828305514, Train_acc 0.98\n",
      "Epoch 435. Loss: 0.40710198348989896, Train_acc 0.98\n",
      "Epoch 436. Loss: 0.4060954946814605, Train_acc 0.98\n",
      "Epoch 437. Loss: 0.4072683622350824, Train_acc 0.98\n",
      "Epoch 438. Loss: 0.40571814773564735, Train_acc 0.98\n",
      "Epoch 439. Loss: 0.4046917122648862, Train_acc 0.98\n",
      "Epoch 440. Loss: 0.4052015775329031, Train_acc 0.98\n",
      "Epoch 441. Loss: 0.4023526850511138, Train_acc 0.98\n",
      "Epoch 442. Loss: 0.4048448773768373, Train_acc 0.98\n",
      "Epoch 443. Loss: 0.40222904626832545, Train_acc 0.98\n",
      "Epoch 444. Loss: 0.40321750682343743, Train_acc 0.98\n",
      "Epoch 445. Loss: 0.40423013644694783, Train_acc 0.98\n",
      "Epoch 446. Loss: 0.40567482054740606, Train_acc 0.98\n",
      "Epoch 447. Loss: 0.405639799988118, Train_acc 0.98\n",
      "Epoch 448. Loss: 0.40502565660910406, Train_acc 0.98\n",
      "Epoch 449. Loss: 0.40581129754254813, Train_acc 0.98\n",
      "Epoch 450. Loss: 0.40493343322960684, Train_acc 0.98\n",
      "Epoch 451. Loss: 0.4045249361403018, Train_acc 0.98\n",
      "Epoch 452. Loss: 0.40526299856339454, Train_acc 0.98\n",
      "Epoch 453. Loss: 0.4060747082607648, Train_acc 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 454. Loss: 0.40686205214856236, Train_acc 0.98\n",
      "Epoch 455. Loss: 0.40663279398834723, Train_acc 0.98\n",
      "Epoch 456. Loss: 0.4072260615821024, Train_acc 0.98\n",
      "Epoch 457. Loss: 0.4090266904922526, Train_acc 0.98\n",
      "Epoch 458. Loss: 0.40784606914953997, Train_acc 0.973333333333\n",
      "Epoch 459. Loss: 0.40879670423823894, Train_acc 0.98\n",
      "Epoch 460. Loss: 0.409185246738361, Train_acc 0.98\n",
      "Epoch 461. Loss: 0.40561950900177884, Train_acc 0.98\n",
      "Epoch 462. Loss: 0.40819565908983163, Train_acc 0.98\n",
      "Epoch 463. Loss: 0.411031275867891, Train_acc 0.973333333333\n",
      "Epoch 464. Loss: 0.4151748613160466, Train_acc 0.973333333333\n",
      "Epoch 465. Loss: 0.4152789217871291, Train_acc 0.98\n",
      "Epoch 466. Loss: 0.41476828749351663, Train_acc 0.98\n",
      "Epoch 467. Loss: 0.4154103018944014, Train_acc 0.98\n",
      "Epoch 468. Loss: 0.4156329055525686, Train_acc 0.98\n",
      "Epoch 469. Loss: 0.4186476831989924, Train_acc 0.973333333333\n",
      "Epoch 470. Loss: 0.4180383866755822, Train_acc 0.973333333333\n",
      "Epoch 471. Loss: 0.41744542600696594, Train_acc 0.98\n",
      "Epoch 472. Loss: 0.42138379325133424, Train_acc 0.98\n",
      "Epoch 473. Loss: 0.4225794165665348, Train_acc 0.98\n",
      "Epoch 474. Loss: 0.4211851580991762, Train_acc 0.98\n",
      "Epoch 475. Loss: 0.41955280676254403, Train_acc 0.98\n",
      "Epoch 476. Loss: 0.4200803610122379, Train_acc 0.98\n",
      "Epoch 477. Loss: 0.42073533445431277, Train_acc 0.98\n",
      "Epoch 478. Loss: 0.42202123790264046, Train_acc 0.98\n",
      "Epoch 479. Loss: 0.4196500168390921, Train_acc 0.98\n",
      "Epoch 480. Loss: 0.4192300170653933, Train_acc 0.98\n",
      "Epoch 481. Loss: 0.4187932715063717, Train_acc 0.98\n",
      "Epoch 482. Loss: 0.41746437398879044, Train_acc 0.98\n",
      "Epoch 483. Loss: 0.41600042323155506, Train_acc 0.98\n",
      "Epoch 484. Loss: 0.414963744757732, Train_acc 0.98\n",
      "Epoch 485. Loss: 0.414980633802132, Train_acc 0.98\n",
      "Epoch 486. Loss: 0.4154458181522976, Train_acc 0.98\n",
      "Epoch 487. Loss: 0.4143860371317975, Train_acc 0.98\n",
      "Epoch 488. Loss: 0.4145304240405865, Train_acc 0.98\n",
      "Epoch 489. Loss: 0.4163006488805916, Train_acc 0.98\n",
      "Epoch 490. Loss: 0.41513038212539577, Train_acc 0.98\n",
      "Epoch 491. Loss: 0.411220259689154, Train_acc 0.98\n",
      "Epoch 492. Loss: 0.41000641360462264, Train_acc 0.98\n",
      "Epoch 493. Loss: 0.412141819208369, Train_acc 0.98\n",
      "Epoch 494. Loss: 0.41287277033458514, Train_acc 0.98\n",
      "Epoch 495. Loss: 0.40918125076400946, Train_acc 0.98\n",
      "Epoch 496. Loss: 0.41024114091458025, Train_acc 0.98\n",
      "Epoch 497. Loss: 0.40651457472595687, Train_acc 0.98\n",
      "Epoch 498. Loss: 0.40641348544228817, Train_acc 0.98\n",
      "Epoch 499. Loss: 0.4053609755412116, Train_acc 0.98\n",
      "Epoch 500. Loss: 0.4057560174478052, Train_acc 0.98\n",
      "Epoch 501. Loss: 0.40537549895301506, Train_acc 0.98\n",
      "Epoch 502. Loss: 0.4052927643703841, Train_acc 0.98\n",
      "Epoch 503. Loss: 0.40530447336871867, Train_acc 0.98\n",
      "Epoch 504. Loss: 0.40451878016000925, Train_acc 0.98\n",
      "Epoch 505. Loss: 0.404813007025987, Train_acc 0.98\n",
      "Epoch 506. Loss: 0.4055767751891168, Train_acc 0.98\n",
      "Epoch 507. Loss: 0.4059815112705668, Train_acc 0.98\n",
      "Epoch 508. Loss: 0.4058159871567442, Train_acc 0.98\n",
      "Epoch 509. Loss: 0.40321683504538697, Train_acc 0.98\n",
      "Epoch 510. Loss: 0.40483478518051047, Train_acc 0.98\n",
      "Epoch 511. Loss: 0.4027325423921563, Train_acc 0.98\n",
      "Epoch 512. Loss: 0.40396212072786214, Train_acc 0.98\n",
      "Epoch 513. Loss: 0.40155595880980277, Train_acc 0.98\n",
      "Epoch 514. Loss: 0.40218239964783453, Train_acc 0.98\n",
      "Epoch 515. Loss: 0.40286456618222233, Train_acc 0.98\n",
      "Epoch 516. Loss: 0.4035809018586699, Train_acc 0.98\n",
      "Epoch 517. Loss: 0.40191278048173773, Train_acc 0.98\n",
      "Epoch 518. Loss: 0.39828453877711156, Train_acc 0.98\n",
      "Epoch 519. Loss: 0.3970370268334435, Train_acc 0.98\n",
      "Epoch 520. Loss: 0.3987190459922433, Train_acc 0.98\n",
      "Epoch 521. Loss: 0.39574786437480725, Train_acc 0.98\n",
      "Epoch 522. Loss: 0.39322410523947743, Train_acc 0.98\n",
      "Epoch 523. Loss: 0.39085243674574277, Train_acc 0.98\n",
      "Epoch 524. Loss: 0.3922464814514959, Train_acc 0.98\n",
      "Epoch 525. Loss: 0.3916082813043659, Train_acc 0.98\n",
      "Epoch 526. Loss: 0.3937981996687813, Train_acc 0.98\n",
      "Epoch 527. Loss: 0.39274110035023474, Train_acc 0.98\n",
      "Epoch 528. Loss: 0.3936880214661646, Train_acc 0.98\n",
      "Epoch 529. Loss: 0.39372400085921994, Train_acc 0.98\n",
      "Epoch 530. Loss: 0.3942913061079246, Train_acc 0.98\n",
      "Epoch 531. Loss: 0.3932553998145978, Train_acc 0.973333333333\n",
      "Epoch 532. Loss: 0.3938394230881132, Train_acc 0.98\n",
      "Epoch 533. Loss: 0.39500681317921227, Train_acc 0.98\n",
      "Epoch 534. Loss: 0.3946319672351861, Train_acc 0.98\n",
      "Epoch 535. Loss: 0.3950169114243114, Train_acc 0.98\n",
      "Epoch 536. Loss: 0.39692970406874556, Train_acc 0.973333333333\n",
      "Epoch 537. Loss: 0.39630550450797747, Train_acc 0.98\n",
      "Epoch 538. Loss: 0.39879124780601366, Train_acc 0.98\n",
      "Epoch 539. Loss: 0.4064089201376193, Train_acc 0.98\n",
      "Epoch 540. Loss: 0.4076322107758717, Train_acc 0.966666666667\n",
      "Epoch 541. Loss: 0.40772214994042943, Train_acc 0.96\n",
      "Epoch 542. Loss: 0.4118299644736413, Train_acc 0.973333333333\n",
      "Epoch 543. Loss: 0.41097673768301085, Train_acc 0.98\n",
      "Epoch 544. Loss: 0.4067280925631787, Train_acc 0.98\n",
      "Epoch 545. Loss: 0.4106198963705764, Train_acc 0.98\n",
      "Epoch 546. Loss: 0.40994725861689263, Train_acc 0.973333333333\n",
      "Epoch 547. Loss: 0.41035247179382955, Train_acc 0.98\n",
      "Epoch 548. Loss: 0.40735550552725713, Train_acc 0.98\n",
      "Epoch 549. Loss: 0.4068108349462204, Train_acc 0.98\n",
      "Epoch 550. Loss: 0.40929202418753835, Train_acc 0.98\n",
      "Epoch 551. Loss: 0.4130726883571608, Train_acc 0.973333333333\n",
      "Epoch 552. Loss: 0.4104360672514959, Train_acc 0.966666666667\n",
      "Epoch 553. Loss: 0.4084016401441749, Train_acc 0.973333333333\n",
      "Epoch 554. Loss: 0.4076697622013035, Train_acc 0.98\n",
      "Epoch 555. Loss: 0.40412268127906537, Train_acc 0.98\n",
      "Epoch 556. Loss: 0.4082135449077055, Train_acc 0.98\n",
      "Epoch 557. Loss: 0.40931601845236437, Train_acc 0.973333333333\n",
      "Epoch 558. Loss: 0.4095429136034632, Train_acc 0.973333333333\n",
      "Epoch 559. Loss: 0.409013296975715, Train_acc 0.98\n",
      "Epoch 560. Loss: 0.4081819734533933, Train_acc 0.98\n",
      "Epoch 561. Loss: 0.40533053785222617, Train_acc 0.98\n",
      "Epoch 562. Loss: 0.40773672553886076, Train_acc 0.98\n",
      "Epoch 563. Loss: 0.40968473337863515, Train_acc 0.98\n",
      "Epoch 564. Loss: 0.40851819787560545, Train_acc 0.98\n",
      "Epoch 565. Loss: 0.4093255733906472, Train_acc 0.98\n",
      "Epoch 566. Loss: 0.4063105149810283, Train_acc 0.98\n",
      "Epoch 567. Loss: 0.4038905432965046, Train_acc 0.98\n",
      "Epoch 568. Loss: 0.4027247347506114, Train_acc 0.98\n",
      "Epoch 569. Loss: 0.40393256969793295, Train_acc 0.98\n",
      "Epoch 570. Loss: 0.4047530480169036, Train_acc 0.98\n",
      "Epoch 571. Loss: 0.40802809961375486, Train_acc 0.98\n",
      "Epoch 572. Loss: 0.40748228341559306, Train_acc 0.98\n",
      "Epoch 573. Loss: 0.40721946154324706, Train_acc 0.98\n",
      "Epoch 574. Loss: 0.40647340966037837, Train_acc 0.98\n",
      "Epoch 575. Loss: 0.40650956944538597, Train_acc 0.98\n",
      "Epoch 576. Loss: 0.4066280265134345, Train_acc 0.98\n",
      "Epoch 577. Loss: 0.4079712987217329, Train_acc 0.98\n",
      "Epoch 578. Loss: 0.40888666457110173, Train_acc 0.98\n",
      "Epoch 579. Loss: 0.4078996910683197, Train_acc 0.98\n",
      "Epoch 580. Loss: 0.40998198381019646, Train_acc 0.98\n",
      "Epoch 581. Loss: 0.41033480502958014, Train_acc 0.98\n",
      "Epoch 582. Loss: 0.40762863049077447, Train_acc 0.98\n",
      "Epoch 583. Loss: 0.40776831351804427, Train_acc 0.98\n",
      "Epoch 584. Loss: 0.40990956866238065, Train_acc 0.98\n",
      "Epoch 585. Loss: 0.40681525060981055, Train_acc 0.98\n",
      "Epoch 586. Loss: 0.4067474103200624, Train_acc 0.98\n",
      "Epoch 587. Loss: 0.40631866977300407, Train_acc 0.98\n",
      "Epoch 588. Loss: 0.4045605817523972, Train_acc 0.98\n",
      "Epoch 589. Loss: 0.4018688307440898, Train_acc 0.98\n",
      "Epoch 590. Loss: 0.3998814716266183, Train_acc 0.98\n",
      "Epoch 591. Loss: 0.4005149595497046, Train_acc 0.98\n",
      "Epoch 592. Loss: 0.39872843170227407, Train_acc 0.98\n",
      "Epoch 593. Loss: 0.3979065442334217, Train_acc 0.98\n",
      "Epoch 594. Loss: 0.39720439467711605, Train_acc 0.98\n",
      "Epoch 595. Loss: 0.3964958488061176, Train_acc 0.98\n",
      "Epoch 596. Loss: 0.40041363957494075, Train_acc 0.98\n",
      "Epoch 597. Loss: 0.3990526841590469, Train_acc 0.98\n",
      "Epoch 598. Loss: 0.39917483914996993, Train_acc 0.98\n",
      "Epoch 599. Loss: 0.39975299239109335, Train_acc 0.98\n",
      "Epoch 600. Loss: 0.39712300165770503, Train_acc 0.98\n",
      "Epoch 601. Loss: 0.39903943520993723, Train_acc 0.98\n",
      "Epoch 602. Loss: 0.403843500478201, Train_acc 0.98\n",
      "Epoch 603. Loss: 0.40121891856609243, Train_acc 0.98\n",
      "Epoch 604. Loss: 0.4030491816793806, Train_acc 0.98\n",
      "Epoch 605. Loss: 0.4017951829817404, Train_acc 0.98\n",
      "Epoch 606. Loss: 0.40047113461591766, Train_acc 0.98\n",
      "Epoch 607. Loss: 0.39700439102883783, Train_acc 0.98\n",
      "Epoch 608. Loss: 0.39816816907978336, Train_acc 0.98\n",
      "Epoch 609. Loss: 0.39626418783247364, Train_acc 0.98\n",
      "Epoch 610. Loss: 0.39785994296916777, Train_acc 0.98\n",
      "Epoch 611. Loss: 0.39477109461100385, Train_acc 0.98\n",
      "Epoch 612. Loss: 0.3969997887150001, Train_acc 0.98\n",
      "Epoch 613. Loss: 0.3961094555663527, Train_acc 0.98\n",
      "Epoch 614. Loss: 0.3961594363797627, Train_acc 0.98\n",
      "Epoch 615. Loss: 0.3944086130525041, Train_acc 0.98\n",
      "Epoch 616. Loss: 0.39336690630430293, Train_acc 0.98\n",
      "Epoch 617. Loss: 0.39384249801598314, Train_acc 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 618. Loss: 0.3948517085658316, Train_acc 0.98\n",
      "Epoch 619. Loss: 0.3927247018944793, Train_acc 0.98\n",
      "Epoch 620. Loss: 0.391788024283482, Train_acc 0.98\n",
      "Epoch 621. Loss: 0.38978554114549047, Train_acc 0.98\n",
      "Epoch 622. Loss: 0.389650408951614, Train_acc 0.98\n",
      "Epoch 623. Loss: 0.38994773796892407, Train_acc 0.98\n",
      "Epoch 624. Loss: 0.390981608433468, Train_acc 0.98\n",
      "Epoch 625. Loss: 0.39371764465211995, Train_acc 0.98\n",
      "Epoch 626. Loss: 0.3942389863925218, Train_acc 0.98\n",
      "Epoch 627. Loss: 0.39455724617323373, Train_acc 0.98\n",
      "Epoch 628. Loss: 0.3920162886430169, Train_acc 0.98\n",
      "Epoch 629. Loss: 0.393611502226433, Train_acc 0.98\n",
      "Epoch 630. Loss: 0.39292001029669726, Train_acc 0.98\n",
      "Epoch 631. Loss: 0.39186843950578415, Train_acc 0.98\n",
      "Epoch 632. Loss: 0.39121867711531083, Train_acc 0.98\n",
      "Epoch 633. Loss: 0.3900854013091267, Train_acc 0.98\n",
      "Epoch 634. Loss: 0.38896648674282025, Train_acc 0.98\n",
      "Epoch 635. Loss: 0.3868928485843097, Train_acc 0.98\n",
      "Epoch 636. Loss: 0.3885898426891331, Train_acc 0.98\n",
      "Epoch 637. Loss: 0.39249412563864544, Train_acc 0.98\n",
      "Epoch 638. Loss: 0.3958533142148386, Train_acc 0.98\n",
      "Epoch 639. Loss: 0.39561514836584233, Train_acc 0.98\n",
      "Epoch 640. Loss: 0.39848083667966216, Train_acc 0.98\n",
      "Epoch 641. Loss: 0.39971500739049526, Train_acc 0.98\n",
      "Epoch 642. Loss: 0.39969452810985984, Train_acc 0.98\n",
      "Epoch 643. Loss: 0.39776764498018485, Train_acc 0.98\n",
      "Epoch 644. Loss: 0.3969901209454033, Train_acc 0.98\n",
      "Epoch 645. Loss: 0.3991276893191231, Train_acc 0.98\n",
      "Epoch 646. Loss: 0.39967022229434956, Train_acc 0.98\n",
      "Epoch 647. Loss: 0.40183342424885155, Train_acc 0.98\n",
      "Epoch 648. Loss: 0.4008761004650938, Train_acc 0.98\n",
      "Epoch 649. Loss: 0.39947479353096826, Train_acc 0.98\n",
      "Epoch 650. Loss: 0.3975743840736218, Train_acc 0.98\n",
      "Epoch 651. Loss: 0.3982385760875935, Train_acc 0.98\n",
      "Epoch 652. Loss: 0.3967748791517423, Train_acc 0.98\n",
      "Epoch 653. Loss: 0.3961607535997044, Train_acc 0.98\n",
      "Epoch 654. Loss: 0.3971440851759727, Train_acc 0.98\n",
      "Epoch 655. Loss: 0.3971002488581561, Train_acc 0.98\n",
      "Epoch 656. Loss: 0.3946704306462468, Train_acc 0.98\n",
      "Epoch 657. Loss: 0.3984499495749899, Train_acc 0.98\n",
      "Epoch 658. Loss: 0.3987432002619711, Train_acc 0.98\n",
      "Epoch 659. Loss: 0.39510994288015683, Train_acc 0.98\n",
      "Epoch 660. Loss: 0.39440349432098215, Train_acc 0.98\n",
      "Epoch 661. Loss: 0.394100686266023, Train_acc 0.98\n",
      "Epoch 662. Loss: 0.3935709002764065, Train_acc 0.98\n",
      "Epoch 663. Loss: 0.39190629098583757, Train_acc 0.98\n",
      "Epoch 664. Loss: 0.39192238730759, Train_acc 0.98\n",
      "Epoch 665. Loss: 0.39146858963589815, Train_acc 0.98\n",
      "Epoch 666. Loss: 0.3924896957978484, Train_acc 0.98\n",
      "Epoch 667. Loss: 0.39311946880593296, Train_acc 0.98\n",
      "Epoch 668. Loss: 0.39283064903844134, Train_acc 0.98\n",
      "Epoch 669. Loss: 0.3939401689838726, Train_acc 0.98\n",
      "Epoch 670. Loss: 0.39770169535052136, Train_acc 0.98\n",
      "Epoch 671. Loss: 0.40070806006650495, Train_acc 0.98\n",
      "Epoch 672. Loss: 0.40353280917941037, Train_acc 0.98\n",
      "Epoch 673. Loss: 0.4021196450176395, Train_acc 0.98\n",
      "Epoch 674. Loss: 0.4011024856826474, Train_acc 0.98\n",
      "Epoch 675. Loss: 0.40007252184061604, Train_acc 0.98\n",
      "Epoch 676. Loss: 0.400745942053643, Train_acc 0.98\n",
      "Epoch 677. Loss: 0.40044661096342116, Train_acc 0.98\n",
      "Epoch 678. Loss: 0.4001093059200369, Train_acc 0.98\n",
      "Epoch 679. Loss: 0.3988297208102097, Train_acc 0.98\n",
      "Epoch 680. Loss: 0.4017253775549893, Train_acc 0.98\n",
      "Epoch 681. Loss: 0.40066598288292316, Train_acc 0.98\n",
      "Epoch 682. Loss: 0.40122997961223694, Train_acc 0.98\n",
      "Epoch 683. Loss: 0.39977902504490304, Train_acc 0.98\n",
      "Epoch 684. Loss: 0.4007425789308783, Train_acc 0.98\n",
      "Epoch 685. Loss: 0.3996173861663705, Train_acc 0.98\n",
      "Epoch 686. Loss: 0.39812260325547255, Train_acc 0.98\n",
      "Epoch 687. Loss: 0.3987595542678395, Train_acc 0.98\n",
      "Epoch 688. Loss: 0.4011599249188803, Train_acc 0.98\n",
      "Epoch 689. Loss: 0.40424372607890746, Train_acc 0.98\n",
      "Epoch 690. Loss: 0.40505209352662896, Train_acc 0.98\n",
      "Epoch 691. Loss: 0.40709940074752815, Train_acc 0.98\n",
      "Epoch 692. Loss: 0.40922826948641833, Train_acc 0.98\n",
      "Epoch 693. Loss: 0.4099907487645464, Train_acc 0.98\n",
      "Epoch 694. Loss: 0.4107414127010076, Train_acc 0.98\n",
      "Epoch 695. Loss: 0.4089537353408312, Train_acc 0.98\n",
      "Epoch 696. Loss: 0.4072849106030317, Train_acc 0.98\n",
      "Epoch 697. Loss: 0.4075592713107538, Train_acc 0.98\n",
      "Epoch 698. Loss: 0.4070510660771116, Train_acc 0.98\n",
      "Epoch 699. Loss: 0.40374546655790317, Train_acc 0.98\n"
     ]
    }
   ],
   "source": [
    "epochs = 700\n",
    "smoothing_constant = .01\n",
    "\n",
    "loss_list=[]\n",
    "train_acc = []\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(data_ctx).reshape((-1, 4))\n",
    "        label = label.as_in_context(data_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "#    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    loss_list.append(curr_loss)\n",
    "    train_acc.append(train_accuracy)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111068050>]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXe8FdW1x3/rnHML99I7UryIgGIBEVCsWLCRaPISY0li\nTdSo70VNI/piNHlRo8YY04yxRZNoEmMSEwt2sSOI0nuRCwKXDhduO2e/P2b2nD1z9rRz5tS7vp8P\nH86dmTOzzpTfXrP22muTEAIMwzBMZRErtgEMwzBM9LC4MwzDVCAs7gzDMBUIizvDMEwFwuLOMAxT\ngbC4MwzDVCAs7gzDMBUIizvDMEwF4ivuRPQwEW0mogUu679MRPOIaD4RvUNEY6M3k2EYhgkD+Y1Q\nJaITAOwB8JgQ4lDN+mMALBZCbCeiMwHcIoQ4yu/Affv2FQ0NDdlZzTAM00mZM2fOFiFEP7/tEn4b\nCCFmElGDx/p3lD/fAzAkiIENDQ2YPXt2kE0ZhmEYEyJaG2S7qGPulwN43m0lEV1BRLOJaHZTU1PE\nh2YYhmEkkYk7EZ0EQ9y/57aNEOIBIcQEIcSEfv183yoYhmGYLPENywSBiA4H8CCAM4UQW6PYJ8Mw\nDJM9OXvuRDQMwNMAviqEWJa7SQzDMEyu+HruRPQEgCkA+hJRI4AfAqgCACHE/QBuBtAHwG+ICAA6\nhBAT8mUwwzAM40+QbJkLfNZ/DcDXIrOIYRiGyRkeocowDFOBlKW4v7V8C1ZvaS62GQzDMCVLJNky\nheYrD70PAFhzx7QiW8IwDFOalKXnzjAMw3jD4s4wDFOBlJ24dyRTxTaBYRim5Ck7cW/tYHFnGIbx\no+zEvaU9WWwTGIZhSp7yE3fFc/erRc8wDNNZKT9xVzz3nfvai2gJwzBM6VLW4r6yaU8RLWEYhild\nylDc02GZeY07i2gJwzBM6VJ24t6qeO53vrAUz8//FACwrbkN33xyLj7Zuhd3vrAEn//N2xyTZxim\n01J25QdaOgxxf+CrR+Lel5fjG3/6EH/++lF4Y1kT/vXRBizduBtLNu4GAMxfvxO96qoxtHddMU1m\nGIYpOOUn7mZYZlifOnzl6P1x4z/m48Lfv2+tl8IOAGf/6m0AwIH9u2JEv3pcfEwDJjX0RiJedi8s\nDMMwoSg7lRvZvyu+NXUU+nerxX+NH6zd5rgD++K+C45APEYAgBWb92DGwk248Pfv48Cbnkfj9r2F\nNJlhGKbgULHi0hMmTBCzZ8/OeT9z1m7HT19Ygq41CVxz0gis3NyMLx45BLEYobUjiapYDAs27ERH\nSuCH/1qI+et34vyJQ/G/nxmDrjVl9+LCMEwnh4jmBJntruzFPQwdyRSm3P06Grfvw9QxA/D7i3g2\nQIZhyoug4l52YZlcSMRj+OFnDwEAvLNiS5GtYRiGyR+dStwBYOqYAfjcuP3Qp2tNsU1hGIbJG51O\n3AGgZ101tje3FdsMhmGYvNEpxb13fTV2t3agjcsHMwxToXRKce9VXw0A2LGPvXeGYSqTTinu/cx4\n+6INu4psCcMwTH7olOI+vG89AOCSRz5AO0/bxzBMBdIpxX3/PulaM9v3cmiGYZjKo1OKe21VHAmz\nNMH2Zp7wg2GYyqNTijsA/OGySQDYc2cYpjLptOLeq87MmGFxZximAum84l5fBQDYyoOZGIapQDqt\nuPepr0H32gSenfdpsU1hGIaJHF9xJ6KHiWgzES1wWU9EdB8RrSCieUQ0Pnozo6c6EcNZhw2yTe7B\nMAxTKQTx3B8FcIbH+jMBjDT/XQHgt7mbVRi6d6nC3raOYpvBMAwTOb7iLoSYCWCbxybnAHhMGLwH\noCcRDYrKwHzSpSqOlvYUkimeSJthmMoiipj7YADrlL8bzWUZENEVRDSbiGY3NTVFcOjcqKuOAwD2\ntSeLbAnDMEy0FLRDVQjxgBBighBiQr9+/Qp5aC1S3LfuaS2yJQzDMNEShbivBzBU+XuIuazkqas2\n5lA98a7Xi2sIwzBMxEQh7s8AuMjMmjkawE4hRFnkF0rPnWEYptJI+G1ARE8AmAKgLxE1AvghgCoA\nEELcD+A5AGcBWAFgL4BL82Vs1HRhcWcYpkLxFXchxAU+6wWAayKzqIAQUbFNYBiGyQuddoQqALTz\nNHsMw1QonVrcp4w2Mnaq4uzBMwxTWXRqcU/EY7jkmAZ0qeLYO8MwlUWnFncAqEnE0MrhGYZhKgwW\n90QMbckUjH5hhmGYyqDTi3t1IgYhgPYkizvDMJVDpxf3moQRb29LcmiGYZjKodOLe3XCOAWtXDyM\nYZgKgsXdFHf23BmGqSQ6vbjXWJ47izvDMJVDpxd36bnPWu01HwnDMEx50enFvWeXagDAw2+vLrIl\nDMMw0dHpxf2YEX1QWxVDv241xTaFYRgmMjq9uMdihIkNvbG7hSfKZhimcuj04g4APbpUYVdLe7HN\nYBiGiQwWdwDdu1Rh1z4Wd4ZhKgcWdwDda6uwa18H15dhGKZiYHGHEZZpS6awrz2JFxZsRCrFIs8w\nTHnD4g6gvsaoL/P7matx1R/n4Kk5jUW2iGEYJjdY3JEepbqyaQ8AYEtzazHNYZiKZVdLO2Ys3Fhs\nMzoFLO5IV4ZsbjXSIatifFoYJh/c8JePceXjc7B2a3OxTal4WMWQ9tyb2wxxj8d4TlWGyQdS1Fu4\nllPeYXEHUFNlinurUfaXJ8xmmPxC/IjlHRZ3KGEZ03NPxPm0MEw+4Dy0wsEqBiUs08phGYbJJ3Is\nCT9h+YfFHWnPfa8ZlkmwuDNMXpCeO4dl8g+LO9Ix9z1mWIbHMDFMnrCeLVb3fMPiDqDajLHL6gPJ\nFPfkM0w+YM+9cLC4I+25SzrYdWeYvMLann9Y3JGOuUuSLO4Mkxe4OF/hCCTuRHQGES0lohVENF2z\nvgcR/ZuIPiaihUR0afSm5g+ZLSPpSPINyDD5QD5ZMY7L5B1fcSeiOIBfAzgTwBgAFxDRGMdm1wBY\nJIQYC2AKgJ8RUXXEtuYNp7iz584w+SElUyFZ2/NOEM99EoAVQohVQog2AE8COMexjQDQjYgIQFcA\n2wCUzbx1iXgMFx41DA989UgAHHNnmHwhozLEUfe8kwiwzWAA65S/GwEc5djmVwCeAbABQDcA5wkh\nyirl5LbPH4a2DsNkzpZhmPwieKxq3omqQ/V0AB8B2A/AOAC/IqLuzo2I6Aoimk1Es5uamiI6dHTI\nwUt3v7gMtzyzsMjWMEzlIT137lfNP0HEfT2AocrfQ8xlKpcCeFoYrACwGsBBzh0JIR4QQkwQQkzo\n169ftjbnjZgyMvXRd9YUzxCGqXBY2/NPEHH/AMBIIhpudpKeDyMEo/IJgFMAgIgGABgNYFWUhjIM\nU/7IVEhOicw/vjF3IUQHEV0LYAaAOICHhRALiegqc/39AH4M4FEimg9jfML3hBBb8mg3wzBliHD8\nz+SPIB2qEEI8B+A5x7L7lc8bAJwWrWkMw1QaHHMvHDxCNQtSKcG58BXIhh37cOwdr2Ldtr3FNqUT\nwM9PvmFxD8CyTbttk/pO++VbGHHjc2iY/iy2NbcV0TImSp7+sBHrd+zDE7M+KbYpFYtMgWTPPf+w\nuAfgtJ/PxJWPz7E8usWf7rLWLdqwy+1rTJlB5rBJ1p3oeWDmSqxq2pMOyxTXnE4Bi3sIjr/ztYxl\nPBijcpBD4lPsVkZKc2sHbntuCb70u3fTHap8ivMOi3uO8E1aOchiVnxN80Nza5LPbQFhcfdA12nq\nzM/le7VykGPYOAc7n5gxd35y8g6LuwftycwaM7ta7PXQWAgqB1nMii9p/uBUyMLB4u7gyP17WZ91\n4t60u9X2N9+jlYOMufM1zQ9C8ddZ3PMPi7uDxy6bhBvPMsri6Cbt2NXSbl/AN2nZ0J5M4dJHZuGj\ndTs8t+MO1WhRz6ZVfoAfnLzD4u6gviaB+hpj4K7Oc3cKPt+k5cOaLc14bWkTvvXXj7TruUM1P6Tr\nyYA99wLC4q6hKmacljatuNuX8U1aORB3qOYFtZ4Mn9rCweKuoSphPOXtmrCMU/DfXL4Fu52hGqYs\niZXAIKbvPvUxGqY/W0QLokcn6Czy+YfFXUNV3DgtTi/dWGa/Kx99Zw2u+fNc27Lm1g68tnRz/gxk\ncoJcJvBMe+4FNMbBX2c3Fu/g+UJx3TnmXjhY3DUkzLDM1J/PxJotzbZ1HZop+OY32jvopj89H5c+\n8gFWO77LFBc/OZGSz8ITLULJbeeYe+FgcddQnUh7dpf94QPbOl2oZl970vb3is17ABgePFN6uE3N\nLD16LvgZLTYh59oyBYPFXYMMywDAqiZ/z72lXT+htsvbf6dCl3FULPy8Re5QzQ+qt57+zOc437C4\na1DF3YnOc3fCN67BvMYdGHnT83i9xPof3BpdToXMD7rngU9x/mFx11AVd3e5vTzRN5Y1Yfj3n8WS\njbsBpIezO9m5r13bWVtpzF6zHQDw+tKmIlti4BdLl1eLBzFFiz0Vkuu5FwoWdw1enrtu1Krk4odn\n2W5anUgIITD21hfx3b/Py8nGcqBUwxxujW4pZMtUIul6MtxVXUhY3DV4h2WCe9y6qpJy2dMfrg9v\nWJmRzj4pD6zCYT7bNbd24Nt/+xg79vIsXEFQJT3t8JTLXVG+sLhr8BL3vW1J13VOmts6cPtzi21Z\nM0nz5u4Mna1UYjHs4B2q3ts9MesTPDWnEb98dUU0hlU6SoYMV4UsHCzuGrxi7ve8tCzwfv743lr8\nbuYq/Ob1tAjIZBsvbW/cvhefbA0/SfPGnS2hv5NPyq0BSzdGPrF5K2WSFSoI2myZYhnTiWBx1+Dl\nuYdBevlqhk3ac3dXvuN++hpOuCtzSj8v3l6xBUff/gqen/9pFpbmFwGBzbtbbHPPliLWZB1Bt2OF\nCgSXHygOLO4aohJ3XeerLg4fBfPX7wQA33K2hcSKuQvgxDtfx5m/eLOo9qR8QmKxgB653C5f17KS\n2NvWgcfeXZNeoHSuVgp720pzsCKLu4Yu1fFI9qMb8JQyBSHqiIX1rJRSKEQpxOUcxVsMooq5S889\nyrDMntaOihzR/JNnF+M3r6+0/k6XIoiGnXvbsaeI521l0x6MuXkG/jZ7XdFscCNRbANKkfqoxF3n\nueepQ1U+NG5pfsWgdCwx8BNjq7PPZz/kqB65aVcLhAAG9qjN2rZDfzgj6++WMjv22iumRt2hOvZH\nL6JbbQLzbzk9mh2GZJk5puXlxZtw7oShRbHBDfbcNXjFwwHg9v86LGOZ7jWzXeOlp/L0Ki8PX4qd\nmKXyBu536mUDGTQsI6/lUbe9gqNvfyV3AzsB6Q7V6G6K3S25e+6vLtmEG1wmcSlXWNx9ePiSCRnL\n+tRXZyx7/L21Gct0o1Atzz1iv1aI/IR7ciEWMPukUKR8OrODpmDnIyyTaUtpnLOccbshS+znXfbo\n7Iobe8Li7sP4Yb0yliU0qZL3vbI8Y5kMywgAtz+3GMs27U53wkUdljF3+5vXV+LOF5ZEu/MsKbW3\nCD/BlKudop1MCdz+3GJrcvRYLP/VIyu1rzZdz70yKLV7XIXF3YfutVX47Nj98LerJlvL4rHM0xaP\nZV5l2aG6ZXcrfjdzFS56aBY0fayRoD4sagdWKVAqTqhfu+pWa/ztFVvwu5mrcNM/5tu+z567P85z\nXan13Evx93CHqg+xGOGXFxwBAPjLFUejT9dqbNzZmrFdQiP40kuXoZjWjqQSlomWUry5Sm3yi5/7\nDEBzmyVIXjM5xWIhqkdWiufuDIGlO60r5AeWVCDUTiDPnYjOIKKlRLSCiKa7bDOFiD4iooVE9Ea0\nZhaeOk3GzFEH9MGB/btpvXRdqEYOXmo1670nU8IS/Hxly3ixdmsz/vVR4eKKpVaI652VWwG4n3u3\nTA7n5nErLJNHz71ixM+dTbtKa0R1LpTi1fIVdyKKA/g1gDMBjAFwARGNcWzTE8BvAJwthDgEwLl5\nsLWgvP29k/HW907SrqupCheWaekwcryFUDr1Iu9Q9d/mM/e9hW8+GT4j4No/f5jVpM263zj+xy9h\nc5Efardzlc6W8f4+WR2qERrltKUU1SIL3O7yt5ZvwVG3vYIZCzcW1B4/wobDyj3mPgnACiHEKiFE\nG4AnAZzj2OZCAE8LIT4BACFEac3OkAW96qsxpFeddl23msxolnPGJiDtubeYA3iSQuRtVKNzr7qb\ndHeWgz3+My+3kgaqJdua2/DS4k057S9fqKVpvdaTIxUyn7ZUKgs2GCOq56zdnrGumCM+3c57S3sS\n9768DK0dxR+MF5Qg4j4YgDr8qtFcpjIKQC8iep2I5hDRRVEZWIp0rQ3WVdFqinpLAcIyzruyJIbG\nu4Rlij3QyjUs4/g/vb0zbhwsHz4Xyqko2ZvLm1wnn/Er9eC8T9fv2IcxN8/An97PTC0uBG5n/cE3\nV+Hel5fjsXcMu1Y27Sn5Tu+osmUSAI4EMA3A6QB+QESjnBsR0RVENJuIZjc1lcbsPNnQVeO562jp\nMMMy7bqwTLQ4bzMvbS/UTenWoVqMV9kgv9ktFdKJXJ/P05jrrtfv2Id128JXFg3Le6u24qsPzcK9\nL2emAgP+k5E7xf3THfsAAE/NaYzMxjC4Xft9lqOWxMxlTTjlZ2/gH3PT/VelqPNBxH09AHVc7RBz\nmUojgBlCiGYhxBYAMwGMde5ICPGAEGKCEGJCv379srW56NRXBxN3eePK6pApIdARwqN+f9VW3PPi\n0kDbOm8uL4Eq1I1oebw+HZT55OxfvYV/f7zBJiLunnsw0ZbprEkh8OvXMmu63/rvhXjwzVVZ2WvZ\nkuNFOvaOV3H8na/ltI8gyNz/1Vsyw5JexK1+C/vvTJhF+3bta3d+pSC4nXb1bVMW51vZtKeEc2WC\nifsHAEYS0XAiqgZwPoBnHNv8C8BxRJQgojoARwFYHK2ppUNM03nqhSxslBQiXTgsgPt63gPv4b5X\nVwTzOh0K6vUVAeDel5ehYfqzoWaWCksuN/6yTbtxw18/yim8lEwJzGvcif9+Yq6VzuiFPJTblsLa\nzvi0aVcL7pqR2fg+8vYa/N+zud3+pRBVC4Lfbex2n8uwjNPZkW+5UZQUyIYg4TDZ8PToUqUsLb0L\n5ivuQogOANcCmAFDsP8qhFhIRFcR0VXmNosBvABgHoBZAB4UQizIn9nlhRR3IdLefBjh01VUvGvG\nEhz301etv51i4HWTpoTAQ2+udt131DgtiQVo2K7504d4+sP1WNm0J+vjymyleIxsg8dcY/5WuCVY\nWEamuOaFLLTi2XmfomH6swUJx+SKW6e0vB93tRTHc/dDIG1b99qqQE5asQgUcxdCPCeEGCWEGCGE\n+Im57H4hxP3KNncJIcYIIQ4VQtybL4PLhYsm7299bjNj70TpATFSX/a0dmDU/z6Plxe5Z5Ds2pfp\nxfz6tZVo3L4PU+8xhhQ49eiJWZ+47k8IJQc9S32as3Y7Fm3wnnwjPRoxfFxGfiOXR0eWf4ip5z3A\nMd3MTXekGn+352u4MbLrUJUx4CVmpcJC4paX73b95Muv881MJiG0e0xEn09cwzLKD9lpeu71St9b\nucbcGQ33maNW3ehSlTkIKk6UMc3evMYdaOtI4XczM0sGyBvKy4tZvtnwbJ0Pl1dYICVEVlPFqSL9\nhd++g7Pu8558w+p4DHyEzGPl4hlJcY/HKFjMPeDoSfm78hnSyuacSbtCRg3zi1+2jOP+k5llNYni\nSJPueehIprBPmTtZOlsl7LQDYHHPmrPH7ue5vlYj7qrnLkXr0x3GgJ5BPbpkbC9v8ECdSyHVwPKc\nQol7uGNYw/mz6FCNooSxFZYhCpSTbnnmPpptee4dpZUKaYl7AdU927RWa5RvSuAfcxtx678XAkiH\nZaqLJO66s37VH+fgwbeMMKYQSh9aSpR0OiSLewTcMDWd9fm/0w4GoJ+4IZlSO1SNZZ/uNFK/Bmm2\nr0kYDUSQ+GOYWywlREZN8iCEaQiM4xj/++WN64gkLJOSnizZbPctHOaTuilcPPcoH/RsdiXPd5A+\njWJDlnMBXP+Xj/HI22sApDtUo5rqUuW+V5ajYfqzntdJ16i+vNg+JlO+BQoR/g3rx/9ZhMffXRPy\nW9nB4h4B509KZ4pefEwD7vzC4ThPMytLSqSLT0m2mzPVdLf1vBtIz31nAM89jLCklJh7mNTMsJkr\nbiM+g0hPLiEdiRTfeDyY5241Rj6byvMg+1IkX3no/VC2PTBzZcY+JNnUlklZjVnor+ZM2MZINkCv\nOEYry7BMdQTi3tzaYTUWAHCPWTjOewyI9z6FcmVSIu25B/35D721Gj/418KAW+cGV4WMADXvvSoe\nw5cmGsI+saEXPlhjH17dogyG2N3SbomYl/jsa/OP7YZ5uIQScw8j2GFDBW7bB3EsrQFFOaZCAkZY\nJlCHqsvIU2foQZrU6mio316xVbvfVErg5mcW4LwJw3DYkB4AgD++txa3PbcEyRTwjSkjNLb4mptB\nOhOrgGEZv1RIH1v2ttmztaIMyxzywxk4oF89vn78AVi/fZ+1PJkS2lpQAAKptNqxXsopq+y558AP\nPmPUT9PF1wHgr1dOzljWopQkOOyWF62H+GcvLcuYZEM+rEE67sKFZdLeXRjPPeyN7ObVBBJ381th\nQ0EqMuOCiEI2Yj62yRLALl63k093teCP732Cq/44x1omRW3HvjaXYwTatQ3ZKOkmZi8WbtfareGX\n2TJBHIl/zvWvcLqqqRnff3o+fqUMNvNLEw5KKiWU0cqlp/Is7jlw+XHDseaOaa5eABFh4a2n44RR\n/XDYYMNjW77JnretCrdzkg25Loi4hwqZCLW2R3AhCB2WyfhgEMSzlM+KbpLxoKTz3B2dpD7T7Dkb\nPGeIJOwbzBZzFGfPunToLRHz7vPIpUM1m3M25uYX8Ls3CjfJi9vvawko7nM/2Y7r/pLdnKcZ11c5\nlt+ZE8JepsLvkXhtyWY0TH8Wa7eGG8EbBSzueaa+JoHHLpuErxw9DACsXneJU7jvVx4wK7YbQNxD\neRxKh2oozz2kuMvtM0bPBnjPCFrnpSOZcn1wrFTIoGEZWfI34+G3/x82A3LLHkPc+3atsZZJh8Dt\n/GfTpFlZPCENFEJgb1sStz+f/fSM7sP29bg5CvI6qY3xVY/PwfeemmfbbkcO5Qmcx1arpQZ5jtTS\n0H4e+9Pm24UsWVBIWNwLhFsM0TnM+g7lAWuXYZkOp9hk3lBhRFpA6VAN4eVlmy2TsTyE9vi9Ldw5\nYylOvOt1bNixL2OdlS0TCxaWcWtQ0p27+pi8H7L+iiru0nN3syubvga5r7CjjnMp8ZBtdN+t/ZHL\n1XP8wsKN+Mvsdbbt2gOGxFSsOvyO36vOLxDk0lqNvPAv4S3PzxtLm7B1T+YMbvmEO1QjZNzQnq7r\n3FK75IOvQ8Z025L2h1XnySc1Iq12nKoYg5jM7+XTc3fJcw9W5yWYkL69YgsAo078fj3tYwVkGdp4\njGz7cRMk+fOc58RpQtj46ra9RlxdDctYNkbYIyftcnZS+pHP0aBhYu4pJW/c777M5rwRDMfGef9t\n2pV+Bv3eKgXsI6+DTuzy9Nz1WLqpsCOHWdwjYv4tp3n28Luldm32EHeJ8+HTiY1OMFMiXX3PvjzL\nsIwAJt/+Cq484YDA39HZKxuJXS3teH7+pzhv4jDX7/hFGLx0Vv62eMAOVasT1zfm7rsrG0mlDIJE\nNtApZeyD/RjZxNyN/8NOdpGeGzb0IS3cyw/od6q7Hh3KfAd+5ziX0cHO870ptOdubLR1Txve37zN\nWOayrfrrF/qU64gaDstERLfaKmvQkY4qF+H38twlzqwM54OvPhQqF/7+Pf0ORbpDNYyIJIXApztb\ncMu/FwXa3hnOUPcDANP/Pg/f+/t8zGvMjEe6CW0YrNoyIcsPOBtKZxjJ65w9+vbqjGW6BlQWHetQ\nMi50tugYceNzuOrxORnLrbCMkjorB8wkUwIrNus9RymUiZC55Zt2tYR+S5DofnMylfaE/e7LoJlK\nOpzXt1mJuQcSd/P/X7yyHM/O956lrJiFxVjcC0Tc5SL7xUd711dneClOwetI6uvEv7/a8Cq+9deP\nccEDaaG3DWIK8Uoetka5NOnN5Vu0yzebr8MtmuqKQR9yuVZ3em3lB0KmuNltsYeXvMJTuoZPXi/1\nMsp5dVvak9q3Li9rkymBFzRzj0o797anxUoOmLn44Vk49Z6ZVufu7pZ2a8o4eX+1daTw/qrMXP19\nbUl87Q8fZNRsP+q2V/Ctv33sYal7Q6prtNtTqcDhuFxCSc5jtyn7ksd9/L21+O8n5mZ8t7UjiU07\nM+cALsFMSBb3QpGNp3Fg/66ojscyxN35XDz27hr8++MNrvv5+4eNeFd5aAWE6zRnkv95Yi7G//gl\n2zI5RDwoltg4vLsf/3sRhBCewpwOyxhhC7cytlaBMc3rv6vn7mKvFe/N6FCF599+dKQyBUt67m+v\n2IK9rZkN/ENvhZ/sQ+5+n8abfsvsm5DHOuyWF/Gl+9817FPE7X+ezBS0N5c34eXFm/GTHGvUq+ju\nu2Qy/Rbj17/TlsNcps43MXWKQHnUH/xzgfaZ+t0bq9Ds8rayp7UDaxwNYDELQbC4F4jWLMS9f7ca\nVCdi2NbcboujOm/8Jz9Y5/yqJ/ZBTHq7nvl4A7Y16wfYBMXNm2lLprB2615FmNPs3NeO4+981fIw\nkymB37+5Csff+RqWeXRIaav5qXnugUaomvtKOZfrPfmgyLEEqqDJ+2H73nas12T6/PG9dMnmhRt2\nYsVm/7r20i6duFu2KLZ/3GhMUq120Kudiy3tSTz+3lor7bB7l3QXXWZdHf3xwnSo7m7psCbM9o+5\n2zdwDUFqcDbe6m/JdjCSAHD+A+9iyt2v21cUUd1Z3AvE4F6ZVR/9qKuOoypOeHnxJpygTJnmfDD2\ntSVx7IF9Au9XCH/PPQq8HhS1uJV8I3hzeROenPUJ1m2zDxWfZ4rQUk2dcnmI37+5Cpt321+X7R2q\n/va6Zsu4bOeH/P3SDlVU1JonasxXx7T73sKpZt1+L+QpbfFwJHSi6tY5efeMpfjBPxfgb2YaYvfa\ndLaPn80xnERzAAAgAElEQVSKVdqluvvu2ic+RKNZJsA35u6w+Z2VWwMLs/PYakORS3hlwfrMDtNi\nTgbP4l4gxg3tiZeuPyHUd7pUJ6wUyi170l6087nYuKsFw/vWB96vEOmOnihT8Zx47Vr16J6d/ymE\nEPjqQ7MyBtIkhbCKqukGrsjgzr8+2oArH5+D15akK/i5hWXccCt54Iy5hxURKwPE/H/LnlZs35u+\nnnsjmg1LNpitHvvThTt0pYv/PqfRGnC3wSxLrRa32xNY3PXoQuayEQfS58wtN1zXIPldY7d+HPXt\n1Xlpd+4tzRmhgsDiXkBGDugWavsuVTFteqXOq+mhqSrphpEKaXx+5O3VOU/mLBFC4OG3VltVLL28\nLyK7R+z2XF75+BysNEMSfnXt536yA5c++oH1am9VhXR0qOoyGNqTqfTDrxjzsxeX4tl5RkZE2Awe\n2XB2OER+wv+9jJcXb7ZGqepi7oAx29Xdmjla/fD23DOX6cZNqB2l8rx3r02HZZodNod1Efxi6vJy\nTbrtFe16nbgHdVS8PHfnPXv1nzOzksJQzOrLLO4F5pSD+gfetk7x3FV0ohmPBb+UQsmWeW/VtoxZ\nm+as3RZ4X+pD+v7qbfjRfxbhf/+5wDqO+/fsf3sJ5qw1hj079gbrA9hu9hXIfQbx3Efe9Dzue2U5\nAGBrcxvOvf8dCCHwy1dX4PkF9syUoC87HQ6PXZ0gHUjPA9Dskpf+7b99bCt45Yf8jS0enrtuggm/\nnHE5PL9GKZC3pzWYR+v2luMXdpFvT27XTZctk724Z3aoSoL0dQD236leY+5Q7UQ8dMlEnDS6X6Bt\nu1THrWHqKrobXrcdkJ4MREUdxKTjC799N5B9gP2Bkp2EUoS9whcdqZRN/IN0Uurq2uu+Zk2Dp8bc\nA4xQVflgzXb38gkBwzIyC0MVefV8nTjKuA/2uoQ4BnbPnMDFyY69bemZgUy7vDrvUyIzbTbogKAf\n/HOBVeZhj8vbhu54OvwG9KSEwGE/nKFd19zagbcc6bWAfpR2EJu8OlSz6ZPKZ6gzDCzuRSDoxa+r\nigce4JLQDUUFcNo9MzO/j+gGV3ilGHr9TN1ALD90A75035LL1PIDzho+Nltcju32YAeNuTvDMUmR\n/nzukUOsDkq31Dq3gW8q4370Eo776asA0r/DM+YuRMbYhjCjPf/50XqsbNqDmcuaXLdZvmk3rnhs\nNrY3t+GvsxsD71tFCHtBL5Xr//IRFn2a2TgELXWsGycicV7ybIRavbeLOSsWlx8oAvJhqquOe47w\n61Idtw2wAIwHeK6mwpyb5657QIQQkb0uGg+U8bqemVXi/mB8+MkOW6W8IF6XLmVQJ7TC4cHOWr3N\nyvPW4VbrxhmLdhvB6oYUDdVzbzcFaPTAbqitMsTbrVyArjNR/b1/eGcNAGCH2enn5rmrKa0pkfm7\n2kLMBSsEcMrPMjN31FPy8Ntr8OKiTWjcHnxmqjAs0WRNAXYh9mqAf/fGKtxz3ljUmZPs2EM8Ds89\ni8FSqh0cc+9kSC/yu6eP9tzu7LH7ocoh2g+/vRr/oxk5FzbmHtVUbNILevTt1bj44Vn243h878WF\n9unVgpQ1btyuEXfNdvK5lrFn576dD5ybh64LlyzcsNOWg+7FL15Zhk937rPluUuxiMfIimG7NfBb\n92T2Maim3vJv+3Rt0nF1xtzVwWjJlLAN2ulIpkJ57m6i2dzaYXnzB/bvCgBa7zoK3ARTFVUvh/uF\nhRtx5wvpjmr19zu/1x7wbUA9LWqDUExxZ8+9CMjY8SHmBB46vn3aKPTvXosBysTZ7ckUVjbpO3jc\nPHcdxiCmaO466fX8VjPRg5fn/rJj7szWACMO97YlkUoJxNTfqjmEjLXryhrocLOzSeM5T7vvrUD7\nBIAnZq3Dog27MMCMnSdF2nNPxGPoYoq7W8741ubM46u5/E6zg3SoCiFsnuq+9mRIcdcvf3fVVry7\naitm3XhKXmcl+upD72PtVv1o5Q6bSHvboBYLc6ZCqucvmzIHucweFiXsuRcB+Rrdu74aa+6Ypt1G\nxsSHKGVs97Un0bVG3x67xdx1eHWohi3rq6vi9+byLVjZtCfUgJCgI3jfWO4e65XIUg9uIuccWOLm\nuYcRcjd2tXTY8tzl50SMUGVeM7e4tE5YJt/+quuxgnSoJlPCJuYdSRFK3P1uj5TIbwlhZ50iFdVz\n9+sIVe8NNSwlIGw1ZbLrULVN+xX6+1HB4l4EZHZD77pq122k9l590oHWspa2pOXtOQnjud/z0jKr\ncJWKEALvrNRP8uyGvJGdQn77c0vwwMzg+fOtAb3sSx/5wDyewINvrrINBrL2Zf42t7eBjBK+eZxy\nNKlkyCSVzsxEjCKtGPiPuY1Ww+yZCunIlmlPpTLqHnl53n4NQYzS2xQ6JHHGvTNx/gNGppefY6EW\n7FPFOJXKfKsMgnpPqfcTx9w7KV4Dj6Rn3aNLFe4+dywAI8zgVkUyTMz9pUWbbKMBJf+Yux5feShc\nJ5gUK6cghH1AgoRlVFZs3oP/e3YxtmtGEMqGwi0skxHOCPiKkY0/apSxTWfNSGGVb1phRhZ7cf1f\nPlYKlLlvJ4RdoNuTArscmURe3qquMbV9V6Rj+oXOFGlPCry3yhgT4ReWUe8Ne567yKoEgXrOSmWC\nchb3IvDdM0ZjYPdae+wYwJ1fONxq6dVV1pRsQrjmF4fx3N3QxfP94qfWpBM5xhnDFlbz2r7VJyyj\n2iqE/1Rp6Y2D26ceSzaAKSGsztWE2RifP3Fo+J26HSvA73CGZdo7UhnjB7zS//xKVCdTwsrwirpu\n0c9fWhZ4W7/7ccnGXfjVq8sz+iCyvY3VnxqkAmkhYHEvAldPORDv3XhKxvIvTRyKKvOhV72emDXf\nZsq1pkciTjm/AqoPdTcztu/3fMpX+lyf4/Di7i4ySzfuQsP0ZzNGlgJGJodq6sk/ewMTf/JyqGOH\nIanE2Q1hTYdlgPRE2VEQdCJ1Vczak6mM+ile4u53nVKp3GZJ8uIX5gjiIPg5zy3tKdz94jI07Wm1\nl/zN8j5WnaAgE8MUAhb3EkPXMSqF4JNte13rtidihPrq3JKf1MEc0g6/B1UKSq4ZEl4DbzK27Uja\nZhtyMueT7a7rquMxW0PknIAialIi/ZqeSqUffDnrkdvcupJuLh3oOoKKuypmbclMz90rt9uvb8Ro\nPIoblli+aTdmaCYzccOrtkxQ3Dpzi1kVklMhS4C3p59seU9SyFXPXXp3T3+43nUf8VgM9TXxnKr1\nqQ+9FB1fce/Qd6iGJYznPuHHL+Pn541zXe8lTlWJWEGnzVHDPklF+OR19sty2r9vnbaUrP5Y/tuk\nUsAGZSahjqTIEHev3G6/vpFkCYj71J9njsp2g0BoT6ZQFSe0J/2mx3ZHDYmVVSokEZ1BREuJaAUR\nTffYbiIRdRDRF6MzsfIZ3LMLxuzXHUBaVNW3dTlF30frdrhOwp2r5/7Sok0270Pa4RdjTYdlChdz\n393a4WlXu0dYoTpOWYeQFm8MPyhHzU6Zs3Y7rvnThwDSou7XV7J/73SHa0cEojl77XbbILh2neee\nEq6D3PzDMsJ1xKvbJPH5ZMyg7r7bdKSEdb9n+waqPjsdJTKIyfdsE1EcwK8BnAlgDIALiGiMy3Y/\nBfBi1EZ2JuRDr3a2xs1ln+5swZHDerl+rz7EK7yTrz8223ZTykakxSP8ASiee9ZHNgibLeM9UMf9\ne1XxWEYqZFC86tO4ocbcgbTXLN/GEj5ZTr3r0+myFz6Y+3B+56jRNVv3Yqljhqv2ZMq1L8AvfObl\nuYcZixEVftlIAkalTtnIZtvwq9fYVl46u91FQpCmdBKAFUKIVUKINgBPAjhHs91/A/g7gM2adUxA\n5MOu5kBb2TIpgf7da7Tfi8cIddX6HPigqK/j8pjSQz58iH40bVTZMkFHk6a39xIZD889EctrXrsT\nZyVIifQU/QRPvaazVgcvxRyUj9Ztz8hqSaaEa6zYz3N3ZuOouE0Sn0+cGWlOUinj3k1YbxUReO62\nDtXSHsQ0GIA6SWejucyCiAYD+DyA30ZnWudEeszqLaE+FP276cU9EYvh+2cdnNOx1Zi+Myzz5aOG\nab8TNlvmts8fpl0e3nMPNyGFpCoew9qtzbj44VkhporLnua2JHa3ZObiB/Xca1wGrWWLM+ygq5PT\nnhSuLqd3oyqzZfQXIF4Ez90v7JUUwlaOIwrPXf2cz1IMfkQVBLsXwPeEEJ7NOhFdQUSziWh2U5P/\nMPLOiFeHKgD0cxH3eIwwbmhPvPbtKZHYIYfGy8mWu7jE89Mdqu438QvXHQ8AOKBvvetcr0FHqErc\nYu7796nzDB3EyBDcN5Y14dUlmS+ZVXkQIHXSaes4pqj7pUIWw+/zyk/3jbl7hWUiTPsMit9AKlkS\nQtoWRIvHDumRUQZEHbhkD9GEMDZigoj7egDqSIsh5jKVCQCeJKI1AL4I4DdE9DnnjoQQDwghJggh\nJvTrF2zCis5GQtOhqr66u41qlaIkv9e3aw0GuIRw3FBLG0jPXXpqbmUPWpP+nrvcV9Kjpk3YPHdp\nl3MQUG0i7jnNnBpu0ImQW+2eoHStSeDVb53ou50Udb/GJOpRnkHE65evLs8oSSDxDct4iHuUOf1B\n8WtQUsKYmSpuibv/CTrj0EEZ94maoWXz4kvcc/8AwEgiGk5E1QDOB/CMuoEQYrgQokEI0QDgKQBX\nCyH+Gbm1nYC0SCuDmJTPbp2mcY3H37drOHGX9cWBdIMiPWR1nUpbRwqN2/d6xtzTnVXCvaMuZFhG\nFlG74wuH25bXVMU8PU81CqIToVzFVAiBA/p19d1OXmdfzz0Hc645aUTGsiCdyf8x54zV4ReW2bmv\n3bU+kV8IKh/4hYKM8hDp6xDE047H0n1M35gyAocO7m6Ls9/z0jKrhn5Jh2WEEB0ArgUwA8BiAH8V\nQiwkoquI6Kp8G9jZkEKoPtTqQ+Em7umOWGVZyNQzdXsr5m6GZWoSes999pptOO6nr3l6hFY8M+Xu\nSYX13JtbO7QNjp+nZvfcM40mIjx5xdGhbFEJ+hqe9ty9r1Euzm6YekNB8btOMzSjgiXF8Nz9OnEX\nfboLHalUOiwToPGLUTqddlCPWowb2tMm7nPWbsf4H7+Eddv2FrTz3kmgqy+EeE4IMUoIMUII8RNz\n2f1CiPs1214ihHgqakM7C/JhJ5eYu1vYwOm5E/l7Dc4MGHV7acef3l8LABn59fVmFofbrDgqMeWV\n1y17QRdzv/2/9J2vgJGWqBNGvwZNfdZ1njtRbiIU9DVc2u7vuWdvi07YchUbv3oxPT0qnRYj5q47\nv/91RDof5No/z8WWPW1KWCbYPuWzEo8RErGYdgzCuu17Sz4swxQQmfpmG8QUQNx1HbF+6YkDu9fi\nhqmjrL9Vr0zu78NPjKnwahziXlMVt5V39UKanxLuD7g6eYLEq89gV0u7Vtz9BsqoYqmLKxNyE/eg\nr+FBY+65oEuzDFKiIBe87od85bnfeNZBrut01/LiYxpct/tygLEE8RhZ/n0iRojHSNvoESjnFOFc\nYHEvMQb2MCbnUNP0nOKuc+bkg6Ou8/PSnDeleh86J2d2eu6JGKE6EQso7umYu5vnvnxz5huAV4x2\nd0uHZdMb35mS/o5vB2X68+3PL8lY355M5ZSPLc/h548Y7LldetKO9G/UHTaXmLuu/0DXoN35xcMz\nlmWL1/2QjzARYE8yGNa7zrZu/LBemHbYIOvvqjhpBT9Mg26EZaTnHkMiTtqxDDEqaKWLzOMX79CM\njkHmtHobFU9WvfHqaxLah9YphAR/zz1GZPM0hUgPO3fO3er03KviMdQk4tjenJnD7YQsz114eO6Z\n6YJeQj2vcaflue/fJz0K0a/Tzi/Gvae1I9CDXh2P4fyJQ3HEsJ625fI1/O5zx+KyY4dby2UYS/YT\n9KwzBEk9lu66Zlt46rQxA6xzfdqYATjMnNJR13GtzvaVK25ZNkD+wjJqo3H8yL6OdYTblPDei9ef\nqL2vwoh7PEZWLZlEjJBw89yJPXdGYbD5oKn1PhI2cY9rO9m61hrhGvVeUm8sXU0aInsHoEBaHJ0i\nmOG5xwm96qp8X/OnjO5n1byZPKKvZzaKc5VfiGVfW+YgJL8wh1vWj6Q96Z7RozKoZy3u+MLhGefp\n0mMMQY/HyOZRyutz8TENWHPHNHSrrTLtVTx3xzF+f9EE7NezFtlw07SDrYk19uvZBY9dNgmAXnz9\nRnGGwUvcdcd5+upjXLevq47jOz6TyAP2t7GR/Z2ZSnaHYnjfeq0DEErcKR2WiccI8VhM67kTRV/T\nPgws7iXGZ8fuh8uOHY5vnpKOhas3Xk0inuHNnTiqX0YsXhXuG6aOwkvXn5BxrBiRrcNHiLS37AzL\nOLNlEjFyHVCl8psvj0e9mft91xcP9/Teap3H0Ij7oB61VkOj66zy61B1y/pRCfKgWx3XyrIffnYM\nfvCZ9Chh1UOU18f5+90898kH9MHUMQNw9tj9MkINQahJxHGgKXTnTRxqCatOfKPMYml1NPZTRqfH\ns+iufU+P2cgevXRSoNRUdZMedfb9pUTmG6DOjjBvFbFYOrQjPXetXTCeqQMzGpzCwOJeYlQnYrj5\ns2Nswul8+NSb+c4vHI5HL52YsR8h0p77MSP6aEUtRvZSpULxcpxes3MQU7faKl9xP+7AvqgzvfYD\n+nVFbVXc9lvOOmygbfuaKmfoR//Q3GG+Zuv6FHL13IFgue66LeKOeVFVW6RAOz08uU11ImbbqXQu\niQinHjwAgBHKOfqA3r62AUYY7fNHDMaCW0/HwYO6W96tWu7XOlbIwL7X+Aln4zH9zHRnp04Edceu\nTsSw5MdnYNLw3oH6HNR9OAf5GaNP7ddc15iFOQfxmD3bya1xNBws9wqb+YbFvQxwE/cXrz8BX5o4\n1CYoctPaqrgVoqmtikMXio45YoJCKAWtHMd0hmV61lWhn88gKd1NT0SYdtggXHXiCNx73hG2dbq4\nvo5as6HRxTOr/Gq1ROS5S7w0QYpKQ586HDPCiAV3OHLr5bFqHL9VfTuT5lTHYzjuQHtM2Y2aqhiI\nyHpj8PpNYT13t/pGQKa4q/eR/n7Q7ESkr7HkM4cP0myYiVPcdeKqnRAnRCZPjMhyfhJxu+duj/mT\nWT65OOrOk3WUAU6hlQ++rgpkv241uP7UUThn3H64+JFZAIwHRZcBIpAZc7dS9Fzqxkt6dvH33N1E\n49dfHq9d7nyg3cXdDMto4pl+D2kQzz3IK7o8naoIO78ld3PiqH7WuXDaLBuA6kTMVpVTvVwypFIV\nj+EbUw7E3S+6zyV6yTENOPmg/tYbU9oWD3EPKT79utUALoNYnRNoq52dupCZX4exXLufR6cvEeG6\nU0diwfqdWs/dOVZAF3MP57mT9caViMVs9/lgxU5hFiUrVmVIFvcywNkRJf9084y/eepIAGkhqYqT\n9uZtaU/axUbx3J3ZMk561lX7intYj8XpubuJrIzNq6YP71uP1VuafbNhnA2IjiAdjEEeWF2Ixpkq\nKN8+qhMx7G0j7Xflx+qEISTyt+qoq47jhFGZdZu8rkXYDMU+Xd0HKs1r3Gn7W204dNdTZ5ZulKgQ\nAo9cMhHvrtqKB2ausq2LEXDdqUYf1UZH2EnXn5lrzD1OZL3JOmPuauOSTMm6NYF3HSkclikDMjx3\n84nwE08pljEirWC1tCftqZAQ6RmCfO7InnX+nnsy5HBIZ8hEJ9SEdBlc1fbnv3k8Ft56uu9D6mxA\ndITxZINuKs+nMyzT3RSDi49psIUPhvdJd6LK6ywbCK+BUm7lAZyn5d3vn2x9DhuWCTPjl1rb5QRH\nmuJHN0/FoB61nv0I8vwKAZx0UH9cf+qozG0U779XvSMso1F3Xb2ZMDn4sRjZY+7KfdpdEfeUkGGh\n4njuLO5lgFvM3e+meejiififkw/EkF5dtA/wzn3t9rCMUkBJJ6wvXHc8jtzfmAmqR5cq9Ovqnaa3\nba9/DvytZx+Ck8yMisyYu/7NxArLKCJXWxVHfU3CN5wUxHNXz9XZY/fTbhP2cZWNjnN+0q41Cay+\n/SxcecIBVqP9488dihunpbNurLEHcio4j+Ncftxw7XLn9R/UIx0+CBuWqasJXmNebWynHjIQH908\n1fo7FiMk4jE8ecVk23e8UsN1Kb3qT6tJxHGnUkxOm1GlHcTkfsyMbcku7ur+7OIukDTDMgO7Z5fS\nmgss7mWA8+GTf/k5XA1963HDaaNBRNoHuCoew7FKB51AukNS3bfMxjloYHfLaxzcs4s1EMcNr5xn\nycXHNFgNhvPtwu3twepQ1c1w5HJSTj14AK47dWQwz13Zx7dPG61dbsXc1cO5iKRAWpidnrvxNSPL\nRn576sEDbG8xac/d2/YB3WtcY9NeYaSwnntdVQjPXT1nsJfPcGtU/nJlunCbPCtqXrnEmtjGsZuq\nRHqBtl9G46WHqVgZj6XDMs5xEWpY5rUlm7FrXzviZLxZTj5AP5dBvmBxLwOcD58UwTAdNc5798kr\njsY9543DGYcOxN+/YQwkESIdlpGv91NG98OU0f2t78lY74j+XTG4ZxdcckxDxijNb582ytxHsDK+\n1YnMBgXQD2L63BH7WQKtjae6CODPzh1rxWX9UM+3+gqvipEUnaAjSN06VFWsN7KYc7mxQp4PN882\n27IJocU9xHSOtnNG7gXxVI7cPx2m8fpJbl3Z6hutLoSl99zDPEvpbJn2ZMruudemG68H31qNj9bt\nQIwIveqrMXJAYfPdWdzLAKeIy7/C1Ip2PvhHH9DH6tmXkzAbI1SN7WTdbueDIGuVD+tdh1iMcMvZ\nh2D0gG62bcabnnjQ2ZXkw+gMMzkzXxbeejq+NXW0Zyqk22QmUjDdZnBSUXP64y5iFEZHCcCxB/ZF\nr7oqXHHCAe7buZwHKyyTkF5stNPYhY0J60Ijbqg2kaMpDHJY2Xmr699Jhycdx1QWdK3NfMvQ9T+F\n7VBVO8jjLmEZ63iaAW+FqPPO2TJliBSBMLeHl2eiTjEWt8Q9Za6zP8gPXjQB63fss4UInPepTMML\nW6Pd+bA7HzhZy16Ku84LdsvBdmtAdKjipf58bZaFTbzsqIfqXV+NuTef5nlct74U+bdbp983pozA\nb19fWTDP3S88pJJwNIiqiUHs/dy4wYiZYyPccDo/6n7PGetdwE0SpgRDLAYcNrgHXlvahF711bb5\nfHXORTqEp75R5FYULpCd+d09kw+k9xdmSjivEI4qZrIc6tDehlfv9AZ71Vfj0MGOOvCOZka+trcF\nDMu42ea2vNa090RNyl9/lzLBUsCmHTYI3z1jdGBBUxs3ex668X01hp/rw+r2RiZN1c3z+Yvzx1n1\nyYN64HJOW0l4cQ8hhGRv/IKEZVSICOeMG+yZI+/cjTzEaWMGBBbtMA1jnAjfPHUUnrpqMsYP62Vr\n4Ltp3hR0jWEhCoqxuJcRUkiuOnEE1twxLVDmRxBU7+qccYOx5o5pVmepX767DhnWCOq5yyMEFadE\nPIY3vjMF911wRMa6/t30WQly34l4DFdPOTCjnIJbOp760Ksempy7NaprALi/kZHluWeK+znjBqOX\nGVabOmZAoOMcNLC77e98hmVsbzsZIpxda/jUVZPxujIRfOZuwu83TEhLlhyY0GDcM+pv1M01rOv4\nLUQ9MQ7LlAmPXz4Jw/vW+2+YBbr0QTkFXTY1uKXnHljcs3jG1TK/KgO616JbbQK7W+wVI/3aqJMP\n6o9vnzbamvvS+p7m5//1ysmYNNx4sJ3FzlQOHmSIqBQBPy47tgF3v7gs443MaphcfkTfrjWYdeMp\n6ONTDuK6U0dadquoHvStZx+CRJxw0z8WuO4nTFjGni3jf6FlZ7wX8nzqRgmrhNFPr5h7/2412Lw7\nXZLa+Tagng9dY29NnanYyZ47Y3H8yH4Y0it8dcAg6OqxCGvkpP8D6bxP3eZ5dUMeIYph2tWJGOb+\nYCq+eOQQ2/KMsQKO78XI8MROO8RezEwXOhjSK51u6FXOYGJDb7x/4yn4rEuuvJNrTx6pfSNzjkge\noaky2L97rW+Y47pTR1k1blTUt5MzDx2I0x3nwEkQcb/zC4dj/z512pG2Xlx78kj/jeT+Quy3S1Uc\nBw3s5rre69xdPcU+0bgzhNNFyR7SNRLyfB0zglMhmTxxy2fHaJfrYqjnjBuM8yYMxXdOd5/CzI0g\nueQqVmpn6CPpScRjuGHqKFtM3q/hcAtN6B56dUCKKsQ6D3JABINXnJ77ry48AtPPPCiyGZTUtp2I\n0LdrDf5g1n/XESTm/qWJQ/HGd04y9m952PkhI5tMc6BFPzodz3/z+MwVJqpgO6uVxh2NmfOeUN/e\ndPeZ3P7UMQNwzUlGQ8GeOxMpZ7pkHMibT/Vsaqvi+OkXD7fSJL3QxYh7dKnC984I1jCE8cCCsl/P\nLp4C5eSgQXqvTnaonnJQOtdffS2viTDm7oY8L9ID7F5bhatOHIEvTRgayf7tNeWN/3Wd1RK/SVSc\nSMHLVwGtIN1CcqCY5KazDratt6Uz1jqKjznqATkdgS7V3udD9eZ7djGep0LE3FncOxFuzwAR4e/f\nmIwnvn60yxbBkamIH//wNHzD8TrrxslmvfIvH7V/zscPjHIy3vzuSdpwBWA89K99ewp+/eXxOH5k\nX1wwyS6oQapM5oqVmZOnY6liFaRz1a/EQ+b+jf/lns84ZCDu+dLYUPvwwvnGFMQp/sxY99TKuuoE\nFtx6Ok492GjQ9zrGRjg9d78y0gN7pN/e5OkthOfOHaqdCCkSR2k61dRRgWGR9+nkA/rgrnPDhwoG\n9+yCNXdMy6joVwhu/swYDPWZ6Uh2ZD9++VEZ62psr+TR2iaRZRaC1KLPBmcuuh9hUyeN+05Y+77/\nq0eG+r73frM7785GTJVaIiPNuFed4WW3dzhr8Nv31cVjxO53zxhtq/djZUSx585ESb9uNbj/K+Px\n4NX+43IAAApmSURBVMUTIt2vzHP//PjBOXX6ZpGYkzWPXDIRnzl8EC4x8/qdvHj9Cda8o150scXc\n84Ms4xC2LyMo9pG3+l8hY8WA8Tt/feF4jB1iH+/gDHWo2xv/R3uG/MJ5XkfzekORI7dvmnYwLp68\nP6Ydbo/BO7/rlQ570eQGR50g4/9Zq7d5WBcNLO6djDMOHWRNzhw1uT66+SiNesh+3bXLJzT0xq8u\nHO86yGXUgG7auuhOvGYligqZUpovz103k5dK15oEvnP6QTjcFHMiwrTDB1nT/0ncBpBZ1zXiyyuL\npO1rCzZYTsX59qEOHJMNfs+6atx6zqEZ5z2zQ9VdRp3r5KjqN5ZtDm1zWDgsw5QMqrg/ffUxkezz\nL1dOxnZH7nqUnHJwf9QkYqFLLYRBVtfMV8xdRee5v/GdKdptnQ2jruKlsU/7/1HxywuPwHeemofD\nHCOmg+BsxKS2HzSwW8bvcr4xtTt+p9fcB85167btBQA0uIzTiBL23JmcuWHqKBw/si/OONQ7P9oP\nmY6WiBHGDzOKj733/VNy2mfXmoRvTD0XjOHxwfLYsyXtuef/cdV57nJwlFP4nfakhMCDF03ISNHM\n12QVowZ0w7+uORb9s0g3dQr4qIHdcOKofrjri5kdvU7PPWy2kMqarYa4uw3CixIWdyZnhvSqw+OX\nH5VzuIfMu1EVAzXToFQ581Aj80JWw4yadMw9/2mXXkL8y/OPwEWT97c8ZedI2pQQOHXMgIwUTbnH\nQnQiBsU5EKkmEcMfLpuEw4ZkvgWob0zPXHsshvXxdhZ+dq57JtBXjzYywiY25OdeUWFxZ0qGfMVm\n881JB/XHmjumYdQA9xGQuSBLJ4ep6ZIPhvWpw4/OOdSKOTtHIg/s4TKJtaXueTTOhvA9nFvlTR2q\np374kJ6u20nO8qhgeeqYAWbtJv/xI7nCMXemZJBvylnUKqtoChuWCX7ypec+akBX3Pb5w1xr6HSv\nrcqo9ZNPxgwyvG+3KRKBzMwsz4YgwA359NXHoLcp2IXM+vKCxZ0pGeRre7EmFC5V8p0KqRKmYZWe\ne0dKeBZH+/PXj8ILCzaih8+0jFExrE8d1twxzXMb5z2mm7IxDLKPCMh+RqyoCXS3ENEZRLSUiFYQ\n0XTN+i8T0Twimk9E7xBRdMPPmE5DXXUc5x45BI9fHrxsQGfg8uOM+v1HRhzTv/e8cRkF1sKUCKg3\nJ8p2y5KR7N+nHlee6D1a+WfnjvWMVUdNVTyG+79yJI490Cjm5TX9YVjCDvLKF76eOxHFAfwawFQA\njQA+IKJnhBCLlM1WAzhRCLGdiM4E8ACAzOF8DOMBEeGuAj7g5cLkEX18PdFs+NwRg/G5I+wzFam6\ndOT+vTyng5NhmY5k7mmgX3A0MoXgjEMH4sVFGwFEWw4gXzV0whIkLDMJwAohxCoAIKInAZwDwBJ3\nIcQ7yvbvASj8lWIqlm9NHWVNSMHkF1WY5MTpbsiRmW0+nnspEytgOYBCE0TcBwNYp/zdCG+v/HIA\nz+diFMOo/PcpwWt8M4VDztblrHdeTsj4eLIC1T3SDlUiOgmGuB/nsv4KAFcAwLBhw6I8NMMwBaYm\nEc9LuKiQyEyYKGPupUKQDtX1ANRRCUPMZTaI6HAADwI4RwixVbcjIcQDQogJQogJ/fr51+1gGIbJ\nJzKF3atvoVwJIu4fABhJRMOJqBrA+QCeUTcgomEAngbwVSHEsujNZBiGiR4Zc8+H5z46T4PaguIb\nlhFCdBDRtQBmAIgDeFgIsZCIrjLX3w/gZgB9APzG7JDpEEJEW1eWYZi8MWH/Xpi9dnuxzSg4Utz9\ntH1Iry5o3L4v8H7f/O5JRU8CCBRzF0I8B+A5x7L7lc9fA/C1aE1jGKZQPHb5JGzLY/XMUiUt7t7q\n/uq3pljzFgQhn8XqgsIjVBmGQV11AnXVnU8O5PynfqOii13XJxs639VkGIYxuXrKgWhPClx4VOVl\n77G4MwzTaamvSeBGl+kBy53ye9dgGIZhfGFxZxiGqUBY3BmGYSoQFneGYZgKhMWdYRimAmFxZxiG\nqUBY3BmGYSoQFneGYZgKhIpV6pKImgCszfLrfQFsidCcfMP25o9yshUoL3vLyVagvOzNxdb9hRC+\nNdOLJu65QESzy6nqJNubP8rJVqC87C0nW4HysrcQtnJYhmEYpgJhcWcYhqlAylXcHyi2ASFhe/NH\nOdkKlJe95WQrUF725t3Wsoy5MwzDMN6Uq+fOMAzDeFB24k5EZxDRUiJaQUTTi20PABDRw0S0mYgW\nKMt6E9FLRLTc/L+Xsu77pv1Liej0Ats6lIheI6JFRLSQiL5ZqvYSUS0RzSKij01bby1VW5Xjx4lo\nLhH9pwxsXUNE84noIyKaXQb29iSip4hoCREtJqLJpWgvEY02z6n8t4uIriu4rUKIsvkHY4LulQAO\nAFAN4GMAY0rArhMAjAewQFl2J4Dp5ufpAH5qfh5j2l0DYLj5e+IFtHUQgPHm524Alpk2lZy9AAhA\nV/NzFYD3ARxdirYqNt8A4M8A/lPK94FpwxoAfR3LStnePwD4mvm5GkDPUrbXtCMOYCOA/Qtta0F/\naAQnajKAGcrf3wfw/WLbZdrSALu4LwUwyPw8CMBSnc0AZgCYXES7/wVgaqnbC6AOwIcAjipVWwEM\nAfAKgJMVcS9JW81j6sS9JO0F0APAapj9hKVur3Lc0wC8XQxbyy0sMxjAOuXvRnNZKTJACPGp+Xkj\ngAHm55L5DUTUAOAIGB5xSdprhjk+ArAZwEtCiJK1FcC9AL4LIKUsK1VbAUAAeJmI5hDRFeayUrV3\nOIAmAI+YYa8HiagepWuv5HwAT5ifC2pruYl7WSKM5rik0pKIqCuAvwO4TgixS11XSvYKIZJCiHEw\nvOJJRHSoY31J2EpEnwGwWQgxx22bUrFV4Tjz3J4J4BoiOkFdWWL2JmCEPn8rhDgCQDOM0IZFidkL\nIqoGcDaAvznXFcLWchP39QCGKn8PMZeVIpuIaBAAmP9vNpcX/TcQURUMYf+TEOJpc3HJ2gsAQogd\nAF4DcAZK09ZjAZxNRGsAPAngZCL6Y4naCgAQQqw3/98M4B8AJqF07W0E0Gi+uQHAUzDEvlTtBYxG\n80MhxCbz74LaWm7i/gGAkUQ03GwVzwfwTJFtcuMZABebny+GEduWy88nohoiGg5gJIBZhTKKiAjA\nQwAWCyHuKWV7iagfEfU0P3eB0TewpBRtFUJ8XwgxRAjRAOO+fFUI8ZVStBUAiKieiLrJzzBiwwtK\n1V4hxEYA64hotLnoFACLStVekwuQDslImwpna6E7GCLooDgLRobHSgA3Fdse06YnAHwKoB2Gh3E5\ngD4wOteWA3gZQG9l+5tM+5cCOLPAth4H43VwHoCPzH9nlaK9AA4HMNe0dQGAm83lJWerw+4pSHeo\nlqStMDLOPjb/LZTPUqnaax5/HIDZ5v3wTwC9StVeAPUAtgLooSwrqK08QpVhGKYCKbewDMMwDBMA\nFneGYZgKhMWdYRimAmFxZxiGqUBY3BmGYSoQFneGYZgKhMWdYRimAmFxZxiGqUD+H8qIf5JVaonR\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110f8e2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(loss_list)),loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
