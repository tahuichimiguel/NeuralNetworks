{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True\n",
    "                                                                 , transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False\n",
    "                                                                , transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    ###########################\n",
    "    # Adding first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    ###########################\n",
    "    # Adding dropout with rate .5 to the first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dropout(.5))\n",
    "\n",
    "    ###########################\n",
    "    # Adding first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    ###########################\n",
    "    # Adding dropout with rate .5 to the second hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dropout(.5))\n",
    "\n",
    "    ###########################\n",
    "    # Adding the output layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude = 2.24),ctx = ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Predict Mode\n",
    "- Let’s grab some data and pass it through the network. \n",
    "- To see what effect dropout is having on our predictions, it’s instructive to pass the same example through our net multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.36852747  0.17856757 -0.21739244 -0.14330083  0.3186754   0.04040951\n",
      "   0.00358494 -0.03745136  0.04799145 -0.05472468]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "\n",
      "[[-0.36852747  0.17856757 -0.21739244 -0.14330083  0.3186754   0.04040951\n",
      "   0.00358494 -0.03745136  0.04799145 -0.05472468]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for x, _ in train_data:\n",
    "    x = x.as_in_context(ctx)\n",
    "    break\n",
    "print(net(x[0:1]))\n",
    "print(net(x[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.36852747  0.17856757 -0.21739244 -0.14330083  0.3186754   0.04040951\n",
      "   0.00358494 -0.03745136  0.04799145 -0.05472468]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "\n",
      "[[-0.36852747  0.17856757 -0.21739244 -0.14330083  0.3186754   0.04040951\n",
      "   0.00358494 -0.03745136  0.04799145 -0.05472468]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#Note that we got the exact same answer on both forward passes through the net! \n",
    "#That’s because by default, mxnet assumes that we are in predict mode. We can \n",
    "#explicitly invoke this scope by placing code within a with \n",
    "#autograd.predict_mode(): block.\n",
    "\n",
    "with autograd.predict_mode():\n",
    "    print(net(x[0:1]))\n",
    "    print(net(x[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.92596197  0.01081653 -0.53987718  0.00942785  0.08536667 -0.36357176\n",
      "   0.45205432  0.54172647  0.46752959 -0.37828952]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "\n",
      "[[-0.57946366 -0.06495641 -0.43908095 -0.26397094  0.52696717 -0.23973548\n",
      "  -0.06280036 -0.42162403 -0.21467119 -0.06900401]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#We can also run the code in train mode. This tells MXNet to run our Blocks as\n",
    "#they would run during training.\n",
    "\n",
    "with autograd.train_mode():\n",
    "    print(net(x[0:1]))\n",
    "    print(net(x[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing is_training() status\n",
    "\n",
    "- How precisely do the Blocks determine whether they should run in train mode or predict mode? Basically, autograd maintains a Boolean state that can be accessed via autograd.is_training(). \n",
    "\n",
    "- By default this value is False in the global scope. \n",
    "\n",
    "- When we enter a train_mode() block, we create a scope in which is_training() returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with autograd.predict_mode():\n",
    "    print(autograd.is_training())\n",
    "\n",
    "with autograd.train_mode():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax CrossEntropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.336052790441, Train_acc 0.935733333333, Test_acc 0.9377\n",
      "Epoch 1. Loss: 0.230901298299, Train_acc 0.959033333333, Test_acc 0.957\n",
      "Epoch 2. Loss: 0.191177943836, Train_acc 0.966283333333, Test_acc 0.9648\n",
      "Epoch 3. Loss: 0.165003347572, Train_acc 0.9728, Test_acc 0.967\n",
      "Epoch 4. Loss: 0.15555009224, Train_acc 0.975616666667, Test_acc 0.9696\n",
      "Epoch 5. Loss: 0.143066859636, Train_acc 0.980383333333, Test_acc 0.9754\n",
      "Epoch 6. Loss: 0.143860827531, Train_acc 0.981016666667, Test_acc 0.9752\n",
      "Epoch 7. Loss: 0.118295699025, Train_acc 0.982333333333, Test_acc 0.9745\n",
      "Epoch 8. Loss: 0.118070674371, Train_acc 0.9836, Test_acc 0.9757\n",
      "Epoch 9. Loss: 0.11175141926, Train_acc 0.986466666667, Test_acc 0.9774\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10L, 28L, 28L, 1L)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABECAYAAACRbs5KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDl0ikUjcBOnQJRKJxE34f9YML4eno6DOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f051d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model predictions are:', \n",
      "[ 2.  7.  6.  0.  8.  6.  9.  0.  6.  2.]\n",
      "<NDArray 10 @cpu(0)>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_predict(net,data):\n",
    "    output = net(data.as_in_context(ctx))\n",
    "    return nd.argmax(output, axis=1)\n",
    "\n",
    "# let's sample 10 random data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                              10, shuffle=True)\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(ctx)\n",
    "    print(data.shape)\n",
    "    im = nd.transpose(data,(1,0,2,3))\n",
    "    im = nd.reshape(im,(28,10*28,1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "\n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred=model_predict(net,data.reshape((-1,784)))\n",
    "    print('model predictions are:', pred)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
