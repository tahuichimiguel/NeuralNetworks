{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_examples = 60000\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False\n",
    "                                                                , transform=transform)\n",
    "                                     ,batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model with `gluon.Block`\n",
    "In gluon a Block has one main job - **define a forward method** that takes some NDArray input x and generates an NDArray output. Because the output and input are related to each other via NDArray operations, MXNet can take derivatives through the block automatically. A Block can just do something simple like apply an activation function. But it can also combine a bunch of other Blocks together in creative ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense0 = gluon.nn.Dense(64)\n",
    "            self.dense1 = gluon.nn.Dense(64)\n",
    "            self.dense2 = gluon.nn.Dense(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Sequential Stacking\n",
    "        x = nd.relu(self.dense0(x))\n",
    "        x = nd.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "net.collect_params().initialize(mx.init.Normal(sigma = 0.01)\n",
    "                                            , ctx = model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Representation 1: \n",
      "[[ 0.          0.          0.02570282  0.41763657  0.          0.          0.\n",
      "   0.          0.          0.03712554  0.16054311  0.35507885  0.\n",
      "   0.12578693  0.          0.          0.          0.30374652  0.2925671\n",
      "   0.35357705  0.          0.07809133  0.21969813  0.21779835  0.\n",
      "   0.34579116  0.13206208  0.01624629  0.27534342  0.22952282  0.22022063\n",
      "   0.          0.00258672  0.0639514   0.68015647  0.          0.          0.\n",
      "   0.16524595  0.18695298  0.25243062  0.01728733  0.06471731  0.          0.\n",
      "   0.25521508  0.          0.          0.03300405  0.33107036  0.64537466\n",
      "   0.04547647  0.          0.          0.          0.19542478  0.02424751\n",
      "   0.          0.          0.04300816  0.16542055  0.13203511  0.          0.        ]]\n",
      "<NDArray 1x64 @cpu(0)>\n",
      "Hidden Representation 2: \n",
      "[[  0.00000000e+00   0.00000000e+00   4.84574912e-03   0.00000000e+00\n",
      "    2.49755625e-02   0.00000000e+00   9.23847407e-03   1.18465191e-02\n",
      "    0.00000000e+00   1.50872627e-02   0.00000000e+00   1.34272072e-02\n",
      "    1.60157625e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   2.71624140e-02   4.19826247e-05\n",
      "    0.00000000e+00   1.89468060e-02   3.05789663e-03   0.00000000e+00\n",
      "    0.00000000e+00   2.77549531e-02   7.56423804e-04   0.00000000e+00\n",
      "    0.00000000e+00   1.97578315e-02   1.76706966e-02   0.00000000e+00\n",
      "    4.06699441e-03   1.02655776e-02   7.50058331e-03   1.55558866e-02\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   2.81560794e-02\n",
      "    0.00000000e+00   0.00000000e+00   2.08071955e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   5.26529737e-04   0.00000000e+00\n",
      "    0.00000000e+00   3.66710126e-02   1.68865174e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.50891412e-02\n",
      "    1.06385862e-02   9.01553128e-03   1.86272338e-02   1.40412226e-02\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.25551578e-02]]\n",
      "<NDArray 1x64 @cpu(0)>\n",
      "Network output: \n",
      "[[ -1.17857929e-03   1.90144812e-04   8.11181555e-04  -3.82558326e-04\n",
      "    4.79567272e-04  -1.27192441e-04   3.38522368e-05  -2.32845981e-04\n",
      "    7.18050636e-04   1.17539347e-03]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ -1.17857929e-03   1.90144812e-04   8.11181555e-04  -3.82558326e-04\n",
       "    4.79567272e-04  -1.27192441e-04   3.38522368e-05  -2.32845981e-04\n",
       "    7.18050636e-04   1.17539347e-03]]\n",
       "<NDArray 1x10 @cpu(0)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gibberish Test Data\n",
    "data = nd.ones((1,784))\n",
    "net(data.as_in_context(model_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Representation 1: \n",
      "[[ 0.          0.          0.          0.          0.          0.07163201\n",
      "   0.          0.          0.3801617   0.39327362  0.28479201  0.\n",
      "   0.60684663  0.17895162  0.          0.          0.          0.13489616\n",
      "   0.06595086  0.          0.          0.          0.31699872  0.          0.\n",
      "   0.          0.59682322  0.          0.33130279  0.21678263  0.          0.025242\n",
      "   0.18803698  0.04500828  0.39512357  0.81530541  0.          0.\n",
      "   0.22699934  0.          0.14096937  0.09317383  0.32004726  0.19807492\n",
      "   0.          0.37229979  0.1869809   0.19709662  0.          0.          0.\n",
      "   0.          0.          0.          0.47881356  0.00093201  0.23278564\n",
      "   0.2540164   0.15339032  0.12496112  0.59734321  0.12047357  0.01379671\n",
      "   0.        ]]\n",
      "<NDArray 1x64 @cpu(0)>\n",
      "Hidden Representation 2: \n",
      "[[ 0.01177626  0.          0.          0.01597291  0.          0.0166887\n",
      "   0.          0.00757967  0.0024682   0.          0.02058876  0.          0.\n",
      "   0.          0.          0.          0.          0.01764479  0.01568343\n",
      "   0.          0.          0.00555972  0.          0.          0.\n",
      "   0.01307633  0.          0.          0.00439471  0.          0.          0.\n",
      "   0.          0.0346678   0.          0.          0.00540923  0.          0.\n",
      "   0.01646167  0.          0.0190303   0.          0.          0.03373244\n",
      "   0.          0.          0.00377814  0.00924637  0.01417759  0.00416635\n",
      "   0.          0.00745585  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.01851815  0.        ]]\n",
      "<NDArray 1x64 @cpu(0)>\n",
      "Network output: \n",
      "[[ -1.01347046e-04   4.23876540e-04  -6.14300661e-05  -1.39293051e-03\n",
      "    5.35664731e-04  -1.94838783e-03   3.76371667e-04  -3.30635346e-04\n",
      "    7.33450521e-04   1.59116765e-03]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ -1.01347046e-04   4.23876540e-04  -6.14300661e-05  -1.39293051e-03\n",
       "    5.35664731e-04  -1.94838783e-03   3.76371667e-04  -3.30635346e-04\n",
       "    7.33450521e-04   1.59116765e-03]]\n",
       "<NDArray 1x10 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Because we’re working with an imperative framework and not a \n",
    "#symbolic framework, debugging Gluon Blocks is easy. If we want \n",
    "#to see what’s going on at each layer of the neural network, we can \n",
    "#just plug in a bunch of Python print statements.\n",
    "\n",
    "class MLP(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense0 = gluon.nn.Dense(64, activation=\"relu\")\n",
    "            self.dense1 = gluon.nn.Dense(64, activation=\"relu\")\n",
    "            self.dense2 = gluon.nn.Dense(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense0(x)\n",
    "        print(\"Hidden Representation 1: %s\" % x)\n",
    "        x = self.dense1(x)\n",
    "        print(\"Hidden Representation 2: %s\" % x)\n",
    "        x = self.dense2(x)\n",
    "        print(\"Network output: %s\" % x)\n",
    "        return x\n",
    "\n",
    "net = MLP()\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.01), ctx=model_ctx)\n",
    "net(data.as_in_context(model_ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster Modeling via gluon.nn.Sequential\n",
    "\n",
    "- MLPs, like many deep neural networks follow a pretty boring architecture. Just take a list of the layers, chain them together, and return the output. \n",
    "\n",
    "- There’s no reason why we have to actually define a new class every time we want to do this. Gluon’s Sequential class provides a nice way of rapidly implementing this standard network architecture. We just:\n",
    "    - Instantiate a Sequential (let’s call it net)\n",
    "    - Add a bunch of layers to it using net.add(...)\n",
    "   \n",
    "   \n",
    "- Sequential assumes that the layers arrive bottom to top (with input at the very bottom). We could implement the same architecture as shown above using sequential in just 6 lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_hidden,activation='relu'))\n",
    "    net.add(gluon.nn.Dense(num_hidden,activation='relu'))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter  Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma = 0.1), ctx = model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax CrossEntropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),'ADAM',{'learning_rate':0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.252397501802, Train_acc 0.954516666667, Test_acc 0.955\n",
      "Epoch 1. Loss: 0.151714300022, Train_acc 0.9577, Test_acc 0.9478\n",
      "Epoch 2. Loss: 0.13060991219, Train_acc 0.9687, Test_acc 0.9601\n",
      "Epoch 3. Loss: 0.125669416395, Train_acc 0.969933333333, Test_acc 0.9615\n",
      "Epoch 4. Loss: 0.115476436557, Train_acc 0.97085, Test_acc 0.96\n",
      "Epoch 5. Loss: 0.106028216281, Train_acc 0.967766666667, Test_acc 0.9576\n",
      "Epoch 6. Loss: 0.103657322463, Train_acc 0.9751, Test_acc 0.9636\n",
      "Epoch 7. Loss: 0.0959326371819, Train_acc 0.97845, Test_acc 0.9678\n",
      "Epoch 8. Loss: 0.0943484722865, Train_acc 0.977433333333, Test_acc 0.9646\n",
      "Epoch 9. Loss: 0.0913317041772, Train_acc 0.9802, Test_acc 0.9645\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10L, 28L, 28L, 1L)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABECAYAAACRbs5KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFslJREFUeJztnXtQVdf1x78beSgoE/BVfDT4biVRNFZj0kHU+vPR2ig4\nLehU7UiUBCk/NVM1jZV0yhhtYh1NpNHRqbX4fvSnTnzw0ERSNBqiYgwKCoKICaI8Agh4z/f3x+We\ncOVevFzOPReu+zOzhsu5+5y91t7nrLPuOnvvI0hCIpFIJO0fN2crIJFIJBJtkA5dIpFIXATp0CUS\nicRFkA5dIpFIXATp0CUSicRFkA5dIpFIXIRWOXQhxBQhxHUhRK4QYoVWSkkkEomk5Qh7x6ELIToA\nuAFgEoA7AC4AiCR5TTv1JBKJRGIrrYnQRwPIJXmLZB2APQBe00YtiUQikbQU91bs2xtAYaP/7wAY\n09wOQgg5LVUikUhazn2S3Z9WqDUO3SaEEAsBLHR0PRKJROLC3LalUGtSLkUA+jb6v0/DNjNIbiE5\niuSoVtQlkQAAnnvuOaSnp6OgoABRUVGIiopytkoSSZuhNQ79AoBBQoh+QghPABEAjmijlkQikUha\nit0OneRjAIsBnATwDYB9JL/WSjGJxBIxMTF45ZVX0KdPH4wdOxZjx451tkoSSZuhVePQSX5CcjDJ\nASQTtFKqrXL48GF89dVX6NGjh7NVeWYZP348AEAIgZycHOTk5DhZI3OysrKQlZUFkti6datTdNiw\nYQP+9re/OaVuiZMhqZsAYHuW2tpaKorCqKgop+kghKAQgn379uV7773H9957jwcOHKAJRVGYkJDA\njh07Or29tJbevXuztLSUiqKwuLiY3bt3Z/fu3Z2ul0ni4+NZX1/P+vp6KopCg8HAGTNm6K5Hbm4u\nT5065fT2cIbMnz+faWlpTEtL48KFCzl58mQCYFBQEENDQxkaGup0He2Ui7b4WIePcnEG8fHxAIDV\nq1fj3XffVf9vDXFxcfDw8ABgjICuXTPOn/rvf//b6mO3hLlz5wIAtm/fbrY9Pz8fAODj44MVK1ag\nuroaCQkJ6NWrFwBAURTcu3fP7no7d+6Mw4cPAwAmTJgAALh06RLOnj1rulnj2rVr6NOnDz7++GPc\nvXvX7rqsERoaCj8/PwBAZmYmSkpKNK+jNfTs2RMdOnRQ/y8tLcU333yjux79+vXDrVu3dK/X2Ywa\nNQrr1q2Dv78/ACAkJASKouDBgwfw9vZWr9/y8nIMGzYMZWVlqKurc6bK2tPeIvTQ0FCePn262e8b\no9UdOTAwkHV1dVQUhYqi8ODBgzx48KBud+iuXbty+/btrKurY11dHQ0GAw0GA1NSUhgWFkZfX1/6\n+vpy0KBBPHPmDKdOncolS5awuLiYxcXFTElJaVXUHhkZycePHzcRg8HQZNv9+/c5e/Zsenp60tPT\nU7M22L9/v9r+iYmJzoiSrMrcuXPVX3Am+ec//+kUXQwGg1Mj9IEDB3LVqlVmkp2dzZKSEq5atYrB\nwcEMDg7WtE4fHx9mZmbadH6atkdGRureNt7e3nzjjTeYkpJCRVEYExPDmJgYW/a1KUKXi3NJJBKJ\nq9CeInQT1iL0J6Pz5iJ5e+Tu3btq9JWZmcnMzExd7uqjR49mWlqaGpUbDAbW1NRw8eLF7Natm8V9\nXn75ZRYXF5vtM2DAALt1OHr0qM0Rukm0znFXVVWp7f/rX/9al7a3RX7yk5/w9u3bZtH5uXPn2L9/\nf6foo2eEPmzYMC5fvpzLly/n3bt3WVtby/r6evV5jiUxPWd4+PAhR40apYkehw8fbtH5aTAY+PDh\nQ4aEhDisbXr27MlZs2Zx9+7dLCwsZGFhoZkPURSF77//Pt9//31bjmdThN7mHbolJ20tjXL69GmH\npFtMEhcXp7tDj4qKYkVFheqUExMTmZiYyKFDh1rdZ8KECfzuu+/MnHlkZCS9vLzs0mHIkCEsLCw0\nuyC2bNnCqVOnMiAggCtXruTKlSt59+5d9fuqqip269bN6g3HHjE59NLSUvbu3dvhbW+LeHl58ezZ\ns+p5YWrvuLg4p+gzZMgQXR16Xl6eVcdti2jVTo3bvrE8bXtKSoqm7TFy5EiOHDmSa9asYXFxcZOb\nWFVVFffv389bt25RURSuXbuWa9euteXYruHQbXXSjcudPn1a8+gccI5Dj46OVk/CS5cu2eQk//jH\nP6r7bNy4kRs3brQ7l+3h4cETJ06YOfPKykq+9NJLTcpevnxZLfPvf/9b87YwOfS6ujqOHz/e4W3f\nnHTo0IEdOnTg+fPnzRzUjh07uGPHDgKgm5sbly5dys2bN3Pw4MEcPHiww/WKi4vT1aHHxsZadNQH\nDx7kRx99pMrq1avZtWtX7tu3z6zckSNHNNGjuUg8Pz+fV65csRi5FxcXa9YWERERfPDgAR88eGBm\nY1paGkePHs3Ro0fT29ubr7/+uvrd8OHDOXz4cFuO3/4d+pNYc+ZPRvGOvFj0cuh+fn708/PjlStX\nVOfc3E94k4O5desWHz16RIPBwN/97nfqdnv1OHv2bJOLZN++fRbLNnbojkiJtJWUi5eXFw8cOMAD\nBw6YXbh79+6lv78//f39CRhvrKbvMjIymJGR4XDd9Hbo3t7eatpgyZIlDAgIYEBAgMUA4rnnnmNG\nRoZDIvTo6Ogm52lJSQljY2Pp6+vL8PBwhzv0+/fvm9lWWVnJHTt2cODAgWqZgQMHqoMrzpw5Q3d3\nd7q7u9tyfPlQVCKRSJ4p2mKEbilvbimFYpooYEsUr4U0jtDz8vKYl5fHwMBAh9RlunMfPXpUjdBn\nzZplsWzXrl2ZlJTEpKQkGgwG3r9/nwsWLGi1Dj/72c9YU1OjRjWlpaUsLS1lp06dzMoFBgYyMDBQ\nzaEXFBSwb9++mrfJv/71L7X9ly9fTi8vL3p5eTEqKorJycm8cOFCE5k7d66mOnTq1Inr1q1rkmLI\nyMhg165d1XKTJk0yG8Z47tw5njt3zmHnpkk2bNjg9GGLlsTPz48nTpwwa7Oamhp14k9rxdfXlzt3\n7mRqaqr6kNbX15cAGBwczC+++MJihK7l0NKlS5eypKSEJSUlvHDhAn/5y1+q33l4eNDDw4ORkZFU\nFIWPHj3iG2+80ZLjt9+Uy5PO3Fo5k6PXw5kD5g7dJI4ay2oaVx4fH6869H379nHhwoVcuHChmo/1\n9vbml19+afbAJzo6WhMdGqdQSktLGRsby9jY2CblEhISmJCQoJb9/PPPHdIm27dvN7uhWspXWpK6\nujp++OGHapu2RocdO3Y0OX55eTnHjh1LAOzXrx/79evH7OxsszItePjVKlm1alWbc+ghISE8fvy4\n2hbV1dWsrq7WZRx4SEgIs7OzrY5++fOf/6xLG0ycOJETJ05U22Djxo0tPUb7myl6+vRpi9utzfQM\nDQ1VP48fPx5nzpzRXiknUVFRAcA4K3XGjBl48cUXER4ejvDwcABAbW0tli5dipdffhnBwcHqfps3\nb8aWLVs00eHWrVsICgoCAHz00UfYtGlTkzIBAQG6LWHbsWNH9fPzzz+vfi4pKUFqaioA42zVoUOH\nqmXGjh0Ld3d3vPnmmygsNL6PZe3atXbVP336dEyZMsVsW1VVFWbOnImMjAx4enpizZo1AIDBgweb\nlfvVr34FAPjqq69w6NAhh81QFELAzc1NPX+cyauvvgoA+Pvf/44RI0YAMM7wvXz5MgBg9+7ddh97\nyJAhcHc3d18VFRUoLCxE377GVb1ff/11rFq1CoqiNNnfzc0NZWVlFs9prenRowc2btyo/n/79m0s\nWbLEMZW1pQi9NTzt2FrIggUL+PjxY5I/jLGNiYlh586dHVrvCy+8wK1bt/Ly5csWh2AZDAZu2LCB\nGzZs0HRmZpcuXbhu3To1vWGpTFhYWJPo5+bNm4yLi6O3tze9vb010yc+Pt4s6j1x4gRPnDjBH/3o\nRxbLe3p6cvLkybxw4YLZ0LHZs2fbVf+dO3fUur///nt+//33nDhxovr9ypUrbRqqt3XrVoedK7m5\nuTQYDJw+fbpDz8mnyezZs1lRUcGKigrV7lOnTpmlpeyRX/ziF0xLSzNLBZoi7uLiYqalpfHevXu8\nd+9em5kpOmDAALUNamtr7U2Htr+Uiykn/uRQxbbi0AEwPz+/yQX6hz/8QZe6/fz8VEfS2Jnn5+ez\nV69e7NWrl27t8Oqrr/Lhw4ckafUmk56ezvT0dHbp0kWTOgcNGmTW7jdv3uTNmze5e/fuZvd7MlVm\n40QOMxk2bBirq6vVY8ybN4/z5s1Tc6PTp083G4XTnNy+fdth/ZKbm8uysjJbh8JpLt27d+e+fftY\nXl5ulpJKSkrSZE5CVVVViycQWdoeGxtLDw8Ph7dHjx491AXlWjmqR45ykUgkkmeKthShP02exBnL\nYW7bts1pEXpwcDBrampYU1NjFglXV1eri3BNmzZNF12Sk5OfOvW/traWtbW1nDp1qiZ19u/f32zk\nSHR0NKOjo63+jO/YsSP/+te/sqamhoqisKysjGVlZfzNb37T4rqDgoJsjsCbk/Lycs6bN0/z/jCN\n+MnLy2NWVpYu58CTMm/ePF65csXM1vLyckZERGhWR0sjcWvb9+7dy4SEBIeNUjPJO++8Q5JMTU1l\nampqa47V/lIu1kTv4YnNye9//3uLF6kj6+zfvz/j4+NZWlqqOvFjx45x/fr1TdIv9jgre2TChAlM\nT0+3esHcuHGDCxYs0GT4ZGOZP3++6tR37drFXbt2Ncmhm0YUHD582KyfwsPDGR4ebnfdFy9etMuJ\np6amcs+ePdyzZ4/FGbZaiGn00+PHj3VfdsA027FxOvLYsWMcM2YMx4wZo2ldWjl00/Z79+5x2bJl\nXLZsmebt0rNnT65fv56KovDkyZM8efIkfXx87D2eazj0+Ph4M0fe3FouesicOXPMLlaSfPTokUPr\n7NSpE69evaqOMb9//76aIw0ICGBOTo7q0D/44APd2sLX17fJBXP8+HGOGDHC7nVjbJGCggKzPsjM\nzOSyZct49OhR5uTkmH1nMBh49epVRkZGsnPnzq16gP3jH/+Yn332mU1OvLi4mLt372Z4eDiFEA7v\ni+TkZPVXk54O3d/fn0VFRSwqKlJ/Be3fv1+dLau1vPPOO6ysrGx2zZbKykpWVlayqKiIpOVnPI23\nmxbOam59JHskMTGRiqLwP//5jxbHa/8O/Uln7szI3CSTJk1iZWWl2cXraIf+4YcfqiffqFGjzFao\nGzJkCL/99lv1+5iYGF0cCPDDNPPGDn3lypUOr9fb25uffPKJTY5Vo4tJFQ8PDwYFBTE3N5e5ublN\n6luzZg3XrFmj65uU3Nzc1Lf06OnQG48gMokeI0e2bt1qMeI2/RIKCwtjWFgYe/fuzXHjxnH9+vVW\n13JpLAUFBZrp2KtXL2ZnZ7OqqorDhg3T4pjtbxx6Y0JDQ7F69WqzbUIIJ2nzA8nJycjNzcXw4cN1\nq/OnP/2p+rm6ulr9PHToULz99tvo1q2bum38+PFITEw03UB1Z+/evQ6vo7q6GhEREQgLCwMAjB49\nGhEREbh06RKys7PVckeOHEFycrKmddfX1+Prr79GXl4eAKB///5m9f3lL38BANTU1Ghab3MEBQUh\nJCREt/oAYMqUKXj33Xfx0ksvqdvOnz+P48ePO7zuxYsXIycnx2xeQmJiIsrKylBfX29WtqioCJ9+\n+il8fHwwcOBAAMa+efPNN7Fo0SJ4eHio+1y/fr3VupnGxs+ZMweDBw9Gbm6urm/WkqNcJBKJxFWw\nIU3SF8BpANcAfA0grmF7PIAiAJcaZJqWKRfTWHRTztzZqZbGkp6erv7EfPz4Mf/xj384tL7Nmzer\nKZU7d+7wzp07PH/+fJM1zw0Gg67jj3fu3Gn20/XRo0eaTiRqq9KxY0dmZWUxKyvLbGz5lClTnKLP\niy++aJY62L59u8Pr/PTTT1XbTRO8nL2kcUtl3LhxnDVrFseNG8dx48ZpcsyQkBCGhIRQURRWVFTw\nt7/9rVb6apNDBxAAYGTD5y4AbgAYCqNDf8sROfTGufO25MhNEhgYyBs3bvDGjRu6vNvS29ubu3bt\nsjqBx2AwqO9utHEpTk3ENCvR5Eg+/vhjp/eNXjJnzhzOmTOHBQUFTEpK0mWtc2vi6enJTZs2cdOm\nTbx48aJmzsmaTJ48WV0C9tSpU+zSpYtmk8fauxw7dozHjh2joihctGiRlsfWJodOshhAccPnSiHE\nNwB6P20/LThz5kybXJ8lPz+/yVodjqS6uhrz58/H1atX0aVLFwCAj48PYmJiUFRUhGnTpqlvlzcY\nDLrpdejQISxbtkz935RXfhZISkoy++tM6urqEBsbq1t9wcHBcHd3x+eff46ZM2eiqqpKt7rbMj4+\nPnBz+yGLrWfuXKWFEXYggAIAvjBG6LcBXAGwHYCf1hG6reWlOEd8fX25bds2NUJ/4YUXnK6TFMfL\nW2+9RUWxa8VAl5ZXXnnFbLSTm5ublsfXduq/EKIzgIMA/pdkBYBEAP0BBMMYwX9gZb+FQoiLQoiL\nttYlkUgkEjuwMTL3AHASwNJmIverWj4UlSJFStuVkydPNnnRybMuMTEx6tIcDhicYFOELviU8crC\nOPh7B4AHJP+30faAhvw6hBBLAIwhGfGUYzVfmUQikUgs8SXJUU8rZItD/zmAswCyAJhWin8bQCSM\n6RYCyAewyOTgmzlWCYAqAPefppiL0A3Pjq2AtNfVeZbsbWu2Pk+y+9MKPdWha40Q4qItdxpX4Fmy\nFZD2ujrPkr3t1VY5U1QikUhcBOnQJRKJxEVwhkPX5g3G7YNnyVZA2uvqPEv2tktbdc+hSyQSicQx\nyJSLRCKRuAi6OXQhxBQhxHUhRK4QYoVe9eqJECJfCJElhLhkmhkrhPAXQiQLIXIa/vo5W097EUJs\nF0J8J4S42mibVfuEECsb+vu6EGKyc7S2Dyu2xgshihr695IQYlqj79qtrQAghOgrhDgthLgmhPha\nCBHXsN1V+9eave27j1uylou9AqADgJswLhXgCeAygKF61K2nwDgev9sT29YBWNHweQWAtc7WsxX2\nhQAYiUazgq3ZB+OKnJcBeAHo19D/HZxtQyttjYeFFUbbu60NNlhbVdVV+7dFq8i2F3v1itBHA8gl\neYtkHYA9AF7TqW5n8xqMM23R8HeGE3VpFSQ/A/Dgic3W7HsNwB6StSTzAOTCeB60C6zYao12bSsA\nkCwmmdnwuRKAaVVVV+1fa/Zao13Yq5dD7w2gsNH/d6DTErw6QwApQogvhRALG7b15A8zaO8B6Okc\n1RyGNftctc9jhRBXGlIypvSDS9kqhAgEMALAeTwD/fuEvUA77mP5UFRbfk4yGMBUADFCCLMXPdL4\n281lhxW5un2wcYXR9oyFVVVVXLF/7V1Ftq2il0MvgvFVdib6NGxzKUgWNfz9DsBhGH+SfSuECACM\nC5oB+M55GjoEa/a5XJ+T/JakgaQCYCt++MntErYKITxgdG5JJA81bHbZ/rVkb3vvY70c+gUAg4QQ\n/YQQngAiABzRqW5dEEL4CCG6mD4D+B8AV2G0c15DsXkA/s85GjoMa/YdARAhhPASQvQDMAjAF07Q\nTzNMjq2BmTD2L+ACtjasqroNwDck1zf6yiX715q97b6PdXyqPA3GJ8k3AfzJ2U+DHWBffxifgl+G\n8WXaf2rY3hVAKoAcACkA/J2tayts3A3jz9B6GHOIC5qzD8CfGvr7OoCpztZfA1t3wrjq6BUYL/AA\nV7C1Qf+fw5hOuYJGL3534f61Zm+77mM5U1QikUhcBPlQVCKRSFwE6dAlEonERZAOXSKRSFwE6dAl\nEonERZAOXSKRSFwE6dAlEonERZAOXSKRSFwE6dAlEonERfh/+T8t0uDIWKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d41910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model predictions are:', \n",
      "[ 4.  1.  8.  7.  6.  6.  1.  7.  2.  7.]\n",
      "<NDArray 10 @cpu(0)>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_predict(net,data):\n",
    "    output = net(data.as_in_context(model_ctx))\n",
    "    return nd.argmax(output, axis=1)\n",
    "\n",
    "# let's sample 10 random data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False\n",
    "                                                                  , transform=transform)\n",
    "                                       ,10\n",
    "                                       , shuffle=True)\n",
    "\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(model_ctx)\n",
    "    print(data.shape)\n",
    "    im = nd.transpose(data,(1,0,2,3))\n",
    "    im = nd.reshape(im,(28,10*28,1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "\n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred=model_predict(net,data.reshape((-1,784)))\n",
    "    print('model predictions are:', pred)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
