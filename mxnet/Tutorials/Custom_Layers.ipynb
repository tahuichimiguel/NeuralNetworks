{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn, Block\n",
    "mx.random.seed(1)\n",
    "\n",
    "###########################\n",
    "#  Speficy the context we'll be using\n",
    "###########################\n",
    "ctx = mx.cpu()\n",
    "\n",
    "###########################\n",
    "#  Load up our dataset\n",
    "###########################\n",
    "batch_size = 64\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Custom Layer\n",
    "# Returns a Tensor with a Mean of Zero\n",
    "class CenteredLayer(Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CenteredLayer, self).__init__(**kwargs)\n",
    "\n",
    "    #This is the only `custom` operation \n",
    "    def forward(self, x):\n",
    "        return x - nd.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-2. -1.  0.  1.  2.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The output of the `net` call is a mean centered version\n",
    "# of [1,2,3,4,5]\n",
    "net = CenteredLayer()\n",
    "net(nd.array([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Insert the custom Block into an existing net with\n",
    "# more sophisticated, Blocks\n",
    "net2 = nn.Sequential()\n",
    "net2.add(nn.Dense(128))\n",
    "net2.add(nn.Dense(10))\n",
    "net2.add(CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to initialize the parameters of the Dense Blocks\n",
    "# in net2\n",
    "net2.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.01874375 -0.04043868 -0.06232345  0.40486944 -0.58767474  0.84935367\n",
      "   0.30716246 -0.31174922 -0.31321037 -0.26473284]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "\n",
      "[  5.96046457e-09]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for data, _ in train_data:\n",
    "    data = data.as_in_context(ctx)\n",
    "    break\n",
    "output = net2(data[0:1])\n",
    "\n",
    "print(output)\n",
    "print(nd.mean(output))\n",
    "\n",
    "#NOTE: The mean is rather large for rounding/truncation error\n",
    "# because MXNet uses low precision arithmetic operations. This\n",
    "# results in significant speedups and 'doesn't affect' most deep\n",
    "# learning algorithms. \n",
    "# - This may be an issue for NN that output probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter exciting_parameter_yay (shape=(5, 5), dtype=<type 'numpy.float32'>)\n",
      "\n",
      "[[-0.6098488  -0.28859827  0.28575182  0.66764957 -0.03796715]\n",
      " [-0.03556556  0.1872465   0.00136471  0.00830621 -0.36514667]\n",
      " [-0.44752467 -0.57045501 -0.11451089  0.34310347  0.50825721]\n",
      " [-0.40443578  0.48336524 -0.51013076 -0.11458552 -0.14575809]\n",
      " [ 0.22497982 -0.09228575  0.34368902  0.28922212  0.41042238]]\n",
      "<NDArray 5x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "# Toy Example with mxnet.Parameter defined outside of a Block\n",
    "\n",
    "# Assign a name\n",
    "# Specify gradient calculation \n",
    "# Specify shapes of each parameter\n",
    "my_param = gluon.Parameter(\"exciting_parameter_yay\"\n",
    "                           , grad_req='write'\n",
    "                           , shape=(5,5))\n",
    "print(my_param)\n",
    "\n",
    "my_param.initialize(mx.init.Xavier(magnitude=2.24)\n",
    "                    , ctx=ctx)\n",
    "print(my_param.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Dictionaries\n",
    " - In practice, we’ll rarely instantiate our own ParameterDict. That’s because whenever we call the Block constructor it’s generated automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['block1_exciting_parameter_yay']\n",
      "Parameter block1_exciting_parameter_yay (shape=(5, 5), dtype=<type 'numpy.float32'>)\n"
     ]
    }
   ],
   "source": [
    "#Create a Parameter dictionary\n",
    "pd = gluon.ParameterDict(prefix = 'block1_')\n",
    "\n",
    "# Instantiate a new Parameter with pd.get()\n",
    "pd.get(\"exciting_parameter_yay\"\n",
    "       , grad_req='write'\n",
    "       , shape=(5,5))\n",
    "\n",
    "# Check names of parameters in `pd`\n",
    "print(pd.keys())\n",
    "\n",
    "print(pd[pd.keys()[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Fully-Connected gluon Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a Block\n",
    "class MyDense(Block):\n",
    "    ####################\n",
    "    # We add arguments to our constructor (__init__)\n",
    "    # to indicate the number of input units (``in_units``)\n",
    "    # and output units (``units``)\n",
    "    ####################\n",
    "    def __init__(self, units, in_units=0, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.units = units\n",
    "            self._in_units = in_units\n",
    "            #################\n",
    "            # We add the required parameters to the ``Block``'s ParameterDict ,\n",
    "            # indicating the desired shape\n",
    "            #################\n",
    "            self.weight = self.params.get(\n",
    "                'weight', init=mx.init.Xavier(magnitude=2.24),\n",
    "                shape=(in_units, units))\n",
    "            self.bias = self.params.get('bias', shape=(units,))\n",
    "\n",
    "    #################\n",
    "    #  Now we just have to write the forward pass.\n",
    "    #  We could rely upong the FullyConnected primitive in NDArray,\n",
    "    #  but it's better to get our hands dirty and write it out\n",
    "    #  so you'll know how to compose arbitrary functions\n",
    "    #################\n",
    "    def forward(self, x):\n",
    "        with x.context:\n",
    "            linear = nd.dot(x, self.weight.data()) + self.bias.data()\n",
    "            activation = relu(linear)\n",
    "            return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instantiate and initialize custom layer\n",
    "dense = MyDense(20,in_units=10)\n",
    "dense.collect_params().initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mydense1_ (\n",
       "  Parameter mydense1_weight (shape=(10, 20), dtype=<type 'numpy.float32'>)\n",
       "  Parameter mydense1_bias (shape=(20,), dtype=<type 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Parameters of `dense`\n",
    "dense.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.          0.          1.17950749  0.          0.          0.33774683\n",
       "   0.          0.2917698   0.36031649  0.          0.41916567  1.34597456\n",
       "   0.          0.          1.0746069   0.8672781   0.          0.\n",
       "   0.37210393  1.5765295 ]\n",
       " [ 0.          0.          1.17950749  0.          0.          0.33774683\n",
       "   0.          0.2917698   0.36031649  0.          0.41916567  1.34597456\n",
       "   0.          0.          1.0746069   0.8672781   0.          0.\n",
       "   0.37210393  1.5765295 ]]\n",
       "<NDArray 2x20 @cpu(0)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run data through custom Block\n",
    "dense(nd.ones(shape = (2,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an MLP with Custom Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(MyDense(128, in_units=784))\n",
    "    net.add(MyDense(64, in_units=128))\n",
    "    net.add(MyDense(10, in_units=64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(ctx = ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params()\n",
    "                        ,'sgd'\n",
    "                        ,{'learning_rate':0.1} \n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        with autograd.record():\n",
    "            data = data.as_in_context(ctx).reshape((-1,784))\n",
    "            label = label.as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "\n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_acc 0.742, Test_acc 0.7383\n",
      "Epoch 1. Train_acc 0.749342857143, Test_acc 0.7432625\n"
     ]
    }
   ],
   "source": [
    "epochs = 2  # Low number for testing, set higher when you run!\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Train_acc %s, Test_acc %s\" % (e, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
